{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian A/B Testing\n",
    "\n",
    "So now we are going to spend today reviewing concecpts regarding AB testing using a bayesian approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success Criteria**\n",
    "\n",
    "I will feel successful today if I can... \n",
    "\n",
    "- Compare and Contrast Frequentist vs Bayesian Mindsets regarding A/B testing\n",
    "- Give at least one reason why we use the Beta Distribution in baysian A/B testing\n",
    "- Explain what a conjugate prior is\n",
    "\n",
    "\n",
    "**Agenda**\n",
    "1. Review Frequentist Hypothesis Testing\n",
    "3. Introduce Beta Distribution\n",
    "4. How to use Beta Distribution\n",
    "5. Actually do the original A/B test\n",
    "6. Answer the question... What's the probability that site B is better than site A?\n",
    "7. Answer Why do we call this Bayesian A/B Testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Frequentist Hypothesis Testing and P-values\n",
    "\n",
    "#### Steps for frequentist hypothesis testing:\n",
    "\n",
    "1. Define your null hypothesis.\n",
    "2. Define the alpha (significance) cutoff.\n",
    "3. Determine how many samples you'll need for a given effect size and significance level (the power).\n",
    "3. Collect data.\n",
    "4. Compute the appropriate statistic (e.g. the t-statistic).\n",
    "5. Compute the probability of that statistic (or something more rare) under the null hypothesis. (aka, the p-value)\n",
    "6. \"Reject\" or \"fail to reject\" the null hypothesis.\n",
    "\n",
    "#### Concept of P-value\n",
    "\n",
    "\"*The probability of observing data at least as extreme as the observation given the null hypothesis*\"\n",
    "\n",
    "#### Example\n",
    "\n",
    "Say our company is rolling out an A/B test to determine if the new \"Click to Shop\" Button is improving our clickthrough rates? \n",
    "\n",
    "You go through all the steps a frequentist would and find a p_value lower than your original alpha value. You head to the stakeholders meeting and say... \n",
    "\n",
    "    \"Assuming that our new button is not better than our original, I feel confident that would not see this data (or something more rare) if I sampled from this site over-and-over-and-over... forever. so we can claim that we have a higher CTR using our new button. - nothing is ever certain but we are happily confident that our Null Hypothesis can be rejected.\"  \n",
    "\n",
    "Not only is it a mouthful but wouldn't it be nice to come to your stakeholders with a phrase you can get behind like ... \n",
    "\n",
    "    \"It is 98% likely that site B has a higher CTR than site A\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dream of frequentist hypothesis testing\n",
    "\n",
    "Wouldn’t it be nice if, instead, we could give a **probability** of probability, rather than *yes* or *no*, of a ***hypothesis*** given the ***data***?\n",
    "\n",
    "This is where bayesian statistics comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's do Bayesian A/B Testing\n",
    "\n",
    "So, we know how to run hypothesis tests (from the **frequentist** world), and we can do **A/B testing via hypothesis testing**, of course.\n",
    "\n",
    "But now let's do **A/B testing under a Bayesian framework**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick question... \n",
    "\n",
    "What do we use to model uncertainty about female heights? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we use to model the uncertainty of a probability? \n",
    "\n",
    "Which distribution do we know that will allow us to say:\n",
    "\n",
    "    \"It is 98% likely that site B has a higher CTR than site A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Beta Distribution: $\\text{Beta}(\\alpha, \\beta)$\n",
    "\n",
    "The Beta Distribution is a **probability distribution on probabilities**. In other words, it is used to model probabilities.\n",
    "\n",
    "<!-- \n",
    " First, define p as the probability of landing a head. If the coin is fair, then it is most likely that the coin will land head half of the time. In this case, p = 50% is the most likely value for p. But wait, it is also possible to have an unfair coin that behaves accidentally like a fair coin. So we cannot rule out the possibility that the coin is unfair, even if we observe heads half of the time.\n",
    " -->\n",
    "##### For example:\n",
    "\n",
    "Consider determining if you have a fair or unfair coin. The Beta Distribution describes how likely p can take on each value between 0 and 1.\n",
    "\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "The beta distribution has two hyper-parameters (also known as \"shape parameters\"):\n",
    "- $\\alpha > 0$: we will use this to encode the number of \"successes\" of a website (more on that later)\n",
    "- $\\beta > 0$: we will use this to encode the number of \"failures\" of a website (more on that later)\n",
    "\n",
    "You can choose the $\\alpha$ and $\\beta$ parameters however you think they are supposed to be. For example, if you think the probability of success is very high, let’s say 90%, set 90 for $\\alpha$ and 10 for $\\beta$. Again more on this later...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine we get 7 heads and 3 tails... \n",
    "alpha = 7\n",
    "beta = 3\n",
    "\n",
    "dist = stats.beta(alpha, beta)\n",
    "x = np.linspace(0.0, 1.0, 301)\n",
    "\n",
    "# The probability density at each sample support value.\n",
    "y = dist.pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "lines = ax.plot(x, y)\n",
    "ax.fill_between(x, y, alpha=0.2, color='red')\n",
    "    \n",
    "ax.set_title('First View of Beta')\n",
    "# ax.get_yaxis().set_ticks([])\n",
    "ax.set_xlabel('Some probability')\n",
    "ax.set_ylabel('Probability Density');\n",
    "dist.cdf(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**NOTE: knowing the pdf and ${B(\\alpha, \\beta)}$ piece is not in your success criteria, but if you are interested here is some information about the Distribution.**\n",
    "\n",
    "Support: $x \\in [0, 1]$\n",
    "\n",
    "PDF: $f(x) = \\dfrac{x^{\\alpha - 1} (1-x)^{\\beta - 1}}{B(\\alpha, \\beta)}$\n",
    "\n",
    "where $B(\\alpha, \\beta) = \\dfrac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$\n",
    "\n",
    "where $\\Gamma(x)$ is an extension of the factorial function which works for a wider range of input (like any positive real value)\n",
    "\n",
    "**Get a feel for the Beta Distribution:**\n",
    "\n",
    "Let's plot a few Beta Distributions (with varying shape parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note:\n",
    "\n",
    "### A key property of the $\\beta$ distribution is that it is constrained to the range of 0 to 1. This is different than some other distributions that extend from $-\\infty$ to $+\\infty$ or $0$ to $+\\infty$.\n",
    "\n",
    "### This constraint makes it useful for modeling the $p$ parameter of a bernoulli or binomial distribution, which we know can only take values between 0 and 1.\n",
    "\n",
    "### As we will see in a minute, this correspondence between $\\beta$ and $p$ is more than just superficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Similar to our original plot done above\n",
    "def plot_beta(alpha, beta, ax, title=None, label=None, xticks=[0.0, 0.5, 1.0]):\n",
    "\n",
    "    # Build a beta distribtuion scipy object.\n",
    "    dist = stats.beta(alpha, beta)\n",
    "\n",
    "    # The support (always this for the beta dist).\n",
    "    x = np.linspace(0.0, 1.0, 301)\n",
    "\n",
    "    # The probability density at each sample support value.\n",
    "    y = dist.pdf(x)\n",
    "\n",
    "    # Plot it all.\n",
    "    lines = ax.plot(x, y, label=label)\n",
    "    ax.fill_between(x, y, alpha=0.2, color=lines[0].get_c())\n",
    "    if title: \n",
    "        ax.set_title(title)\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    #ax.get_yaxis().set_ticks([np.max(y)])\n",
    "    ax.get_xaxis().set_ticks(xticks)\n",
    "    ax.set_ylim(0.0, np.max(y)*1.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The shape parameters. \n",
    "alpha_values = [1, 2, 4, 8, 16, 32]  \n",
    "beta_values  = [1, 2, 4, 8, 16, 32]\n",
    "n_rows, n_cols = len(alpha_values), len(beta_values)\n",
    "alpha_beta_pairs = ( (i, j) for i in alpha_values for j in beta_values )\n",
    "\n",
    "# Create a large figure - nice way to do it.\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 15))\n",
    "\n",
    "# Plot each beta dist. One plot per pair of shape params.\n",
    "for (alpha, beta), ax in zip(alpha_beta_pairs, axes.flatten()):\n",
    "    plot_beta(alpha, beta, ax, r\"$\\alpha={} ,  \\beta={}$\".format(alpha, beta))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What patterns do you see in the above distributions?**\n",
    "\n",
    "Here are a few:\n",
    "* If $\\alpha$ is larger than $\\beta$, the distribution is shifted to the **right** and skewed to the left.\n",
    "* If $\\alpha$ is smaller than $\\beta$, the distribution shifted to the **left** and skewed to the right.\n",
    "* If $\\alpha = \\beta$, the distribution is **symmetric** and **centered** at $0.5$.\n",
    "* The distribution gets **skinnier** as $\\alpha$ and $\\beta$ increase.\n",
    "\n",
    "Linking $\\alpha$ and $\\beta$ to the moments:\n",
    "\n",
    "The **mean** of the Beta Distribution is: $E(X) = \\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "The **variance** of the Beta Distribution is: $\\text{Var}(X) = \\dfrac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How we can use the Beta Distribution (A BIG IDEA)\n",
    "\n",
    "What if we set:\n",
    "- $\\alpha = 1 + \\text{number of conversions on our website}$\n",
    "- $\\beta = 1 + \\text{number of misses on our website}$\n",
    "- $\\alpha + \\beta = 2 + \\text{total number of visits to website}$\n",
    "\n",
    "Then, as we know, the **mean of the beta distribution** would be $\\dfrac{\\alpha}{\\alpha + \\beta}$ which equals smoothed **conversion rate**!\n",
    "\n",
    "If we think of $\\alpha$ as \"Wins\" or \"Heads\" and $\\beta$ as \"Losses\" or \"Tails\", then \n",
    "\n",
    "$$\\alpha + \\beta = flips$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\frac{\\alpha}{\\alpha + \\beta}=\\frac{Heads}{Flips}$$\n",
    "\n",
    "is the probability of heads, which is the $p$ of a bernoulli or binomial distribution.\n",
    "\n",
    "Also, the more visitors we have (i.e. the larger $(\\alpha + \\beta)$), then the **smaller the variance of our beta distribution will be**.\n",
    "\n",
    "Put all this together, **_the beta distribution models the probability of the conversion rate_** of our website (which we are trying to figure out from our data). The beta distribution is one way to model our **belief** of what the conversion rate might be.\n",
    "\n",
    "A continuous probability distribution (like the beta distribution) puts relative likelihoods to each of the values in the support. In our case, the support is theorized conversion rates. On the y-axis of the PDF we have probability density, and on the x-axis of the PDF we have probabilities (conversion rates). So we have probabilities of probabilities (relative probability of conversion rates.)\n",
    "\n",
    "**Why we like it:** The reason we like it is that this lets us know the **strength of our belief** about the conversion rate of our website. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "QUESTION: Are all of these conversion rates the same? \n",
    "\n",
    "$$ \\frac{2}{10} =\\frac{20}{100}=\\frac{200}{1000} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_beta_dist_params(website_samples):\n",
    "    #array of website conversions... zeros and ones (convert or didnt convert)\n",
    "    website_samples = np.array(website_samples)\n",
    "    \n",
    "    #total number of conversions\n",
    "    num_conversions = website_samples.sum()\n",
    "    #total number of datapoints\n",
    "    total_visitors = len(website_samples)\n",
    "    \n",
    "    #plus one to set a and beta as uniform priors...try other numbers to see the changes\n",
    "    alpha = num_conversions + 1\n",
    "    beta = (total_visitors - num_conversions) + 1\n",
    "    \n",
    "    #mean number of conversions... aka conversion rate\n",
    "    mean = 1 * num_conversions / total_visitors\n",
    "    \n",
    "    # Alpha = number of successes +1\n",
    "    # Beta = Number of failures +1\n",
    "    # Mean = conversion rate\n",
    "    # num_conversions & total_visitors(used to make labels on graphs)\n",
    "    return alpha, beta, mean, num_conversions, total_visitors\n",
    "\n",
    "def plot_beta_website(website_samples, ax, label=None):\n",
    "    alpha, beta, mean, num_conversions, total_visitors = get_beta_dist_params(website_samples)\n",
    "    title = None if label else r\"Converted {}/{}\".format(num_conversions, total_visitors)\n",
    "    plot_beta(alpha, beta, ax, title, label, [0.0, mean, 1.0])\n",
    "    ax.set_xlabel(\"Conversion Rate\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "\n",
    "fig, (left, mid, right) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "plot_beta_website([0, 1, 0, 0, 0]*2, left)\n",
    "plot_beta_website([0, 1, 0, 0, 0]*20, mid)\n",
    "plot_beta_website([0, 1, 0, 0, 0]*200, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Looking at these three graphs we can say:\n",
    "* ### $\\frac{2}{10}$ conversion suggests the conversion rate is about $p=0.2$, but values of $p$ in the range of $0.05$ to $0.45$ would not be unreasonable\n",
    "* ### $\\frac{20}{100}$ also suggests $p=0.2$ and less probability of values of $p$ far from $0.2$\n",
    "* ### $\\frac{200}{1000}$ indicates $p=0.2$ or some number quite close to $0.2$. No value far from $0.2$ is reasonable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### It is clear that we can use this distribution to model both (1) our belief of the conversion rate based on the data we have (i.e. via the mean), and (2) the strength of our belief (i.e. via the variance).\n",
    "\n",
    "**A good Bayesian never claims to know anything exactly. Instead they have some beliefs, and they have various levels of strengths regarding their beliefs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian A/B testing\n",
    "\n",
    "We're finally ready to do the Bayesian A/B testing.\n",
    "\n",
    "Say we have two versions of our website: version A and version B (version C we'll use later).\n",
    "\n",
    "Let's read in the log of our historical visitors for both version A and version B.\n",
    "\n",
    "A .npz file contains saved numpy arrays using [.savez](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.savez.html#numpy.savez)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.load(\"samples.npz\")\n",
    "list(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# take a peek at the data\n",
    "x['site_A_samples'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how many total?\n",
    "print(\"Shape A: \", x['site_A_samples'].shape)\n",
    "print(\"Shape B: \", x['site_B_samples'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load in a subset\n",
    "n = 200\n",
    "site_A_samples = x['site_A_samples'][:n]\n",
    "site_B_samples = x['site_B_samples'][:n]\n",
    "\n",
    "np.mean(site_A_samples), np.mean(site_B_samples)\n",
    "\n",
    "#can we stop there and be content that site_B_samples have a higher average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot our belief about each site's conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "plot_beta_website(site_A_samples, ax, label=\"Site A\")\n",
    "plot_beta_website(site_B_samples, ax, label=\"Site B\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's look at how the distributions evolve as data rolls in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for n in [2**n for n in range(12)]:\n",
    "    ax = plt.subplot()\n",
    "    site_A_samples = x['site_A_samples'][:n]\n",
    "    site_B_samples = x['site_B_samples'][:n]\n",
    "    plot_beta_website(site_A_samples, ax, label=\"Site A\")\n",
    "    plot_beta_website(site_B_samples, ax, label=\"Site B\")\n",
    "    plt.title(f'after {n} visitors')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important question: What's the probability that site B is better than site  A?\n",
    "Let's figure out how much of site B's Beta distribution is to the right of site A's.\n",
    "\n",
    "To do this, all we have to do is to take the integral of this function:\n",
    "\n",
    "$f(x) = \\dfrac{x^{\\alpha - 1} (1-x)^{\\beta - 1}*{\\Gamma(\\alpha)\\Gamma(\\beta)}}{{\\Gamma(\\alpha + \\beta)}}$\n",
    "\n",
    "Where $\\Gamma$ is, of course the well-known gamma function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As it turns out, this calculation is somewhat difficult, but we can take a shortcut using *Monte Carlo* simulation. This shortcut is widely used among data scientists and data engineers. \n",
    "\n",
    "We simply **draw a large number of random samples from each distribution, and count how many times B's samples are greater than A's.\n",
    "\n",
    "The code is short and easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 100_000\n",
    "\n",
    "#Let's just grab our Alpha and betas from site_A\n",
    "alpha, beta = get_beta_dist_params(site_A_samples)[:2]\n",
    "print(f'Site_A alpha and beta {alpha, beta}')\n",
    "#Set up first distribution\n",
    "dist_A = stats.beta(alpha, beta)\n",
    "\n",
    "#Same steps for beta dist\n",
    "alpha, beta = get_beta_dist_params(site_B_samples)[:2]\n",
    "print(f'Site_B alpha and beta {alpha, beta}')\n",
    "dist_B = stats.beta(alpha, beta)\n",
    "\n",
    "#randomly sample 100_000 data points from each distribution\n",
    "simulated_A = dist_A.rvs(num_simulations)\n",
    "simulated_B = dist_B.rvs(num_simulations)\n",
    "\n",
    "\n",
    "print(f'On average, how many times is Bs Conversion Rate greater than As: {(simulated_B > simulated_A).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another way to look at this data is by plotting random samples from B against random samples of A. This creates a 'blob plot'. By measuring how much of the blob is above the *y=x* line, we can determine the probability that B is better than A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#scatter plot our different conversion rates sampled from our distributions\n",
    "plt.scatter(simulated_A, simulated_B, alpha = .2);\n",
    "plt.xlim(plt.ylim())\n",
    "plt.xlabel('A'), plt.ylabel('B')\n",
    "plt.plot(plt.xlim(), plt.xlim(), color = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Translation: In our simulation, 94% of the time, site B is better than site A. We can then interpret this as a probability, meaning we can say:\n",
    "\n",
    "> \"It is 94% likely that site B is better than site A.\"\n",
    "\n",
    "**CAN FREQUENTIST DO THAT?** No, they can NOT!\n",
    "\n",
    "They can only say horribly unintelligible things like \"the p-value of 0.11 does not support the decision to reject the null hypothesis at an $\\alpha$ level of 0.05.\"\n",
    "\n",
    "Who needs that crap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In our model, conversion is a bernoulli process: it either happens or it doesn't and the probability that it is going to happen is a fixed value, *p* that does not change over time.\n",
    "\n",
    "### The beta distribution gives us an estimate of *p*, but as a distribution over a range of values, not a fixed value.\n",
    "\n",
    "### Suppose we want to put some bounds on the range where the **true** value of *p* may be found. \n",
    "\n",
    "### We call this the \"credible interval\" and it's largely analogous to the \"confidence interval\" of conventional stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the 95% credible interval of site B's conversion rate?\n",
    "\n",
    "To calculate it, we measure the area under the curve that contains 95% of the data, or rather the interval under the curve that does not contain the first or last 2.5% of the data. \n",
    "\n",
    "Note that the distribution may be skewed, so these aren't necessarily symmetrical around the distribution mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(*dist_B.ppf([.001, .999]),101)\n",
    "plt.plot(x, dist_B.pdf(x))\n",
    "plt.vlines(dist_B.ppf([.025, .975]), ymin = 0, ymax = dist_B.pdf(x).max(), linestyles='dotted')\n",
    "plt.fill_between(x, 0, dist_B.pdf(x), where = (x< dist_B.ppf(.025)) | (x > dist_B.ppf(.975) ));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interval_size = 0.95\n",
    "tail_area = (1 - interval_size) / 2\n",
    "\n",
    "print(\"{0:0.1f}% credible interval conversion rate lower bound: {1:0.3f}, upper bound {2:0.3f}\".format(\n",
    "       interval_size *100,\n",
    "       dist_B.ppf(tail_area), \n",
    "       dist_B.ppf(1 - tail_area)))\n",
    "\n",
    "# or if you are lazier\n",
    "# dist_B.ppf([0.025, .975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What if the difference between sites A and B is larger?\n",
    "\n",
    "Let's explore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.load(\"samples.npz\")\n",
    "\n",
    "n = 1000\n",
    "\n",
    "site_A_samples = x['site_A_samples'][:n]\n",
    "site_C_samples = x['site_C_samples'][:n]\n",
    "\n",
    "np.mean(site_A_samples), np.mean(site_C_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "plot_beta_website(site_A_samples, ax, label=\"Site A\")\n",
    "plot_beta_website(site_C_samples, ax, label=\"Site C\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 10000000\n",
    "\n",
    "alpha, beta = get_beta_dist_params(site_A_samples)[:2]\n",
    "dist_A = stats.beta(alpha, beta)\n",
    "\n",
    "alpha, beta = get_beta_dist_params(site_C_samples)[:2]\n",
    "dist_C = stats.beta(alpha, beta)\n",
    "\n",
    "simulated_A = dist_A.rvs(num_simulations)\n",
    "simulated_C = dist_C.rvs(num_simulations)\n",
    "C_vs_A = simulated_C > simulated_A\n",
    "print((simulated_C > simulated_A).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "C_vs_A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our simulation, 100% of the time site C is better than site A. We can then interpret this as a probability, meaning we can say:\n",
    "\n",
    "> \"It is 100% likely that site C is better than site A.\"\n",
    "\n",
    "We used 1,000 samples of each site just now. We _probably_ didn't need this much data in this case; we probably spent longer running this test than needed. Re-run the three cells above using less data. (You really only need about 300 samples of each site with this amount of difference between the sites.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Benefits of Bayesian A/B Testing\n",
    "\n",
    "In general, Bayesian A/B testing is better for these reason:\n",
    "\n",
    "1. You can say stuff like \"It is \\_\\_% likely that site \\_\\_ is better than site \\_\\_.\"\n",
    "2. You can stop the test early based on surprising data.\n",
    "3. You can update the test while it is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we call this Bayesian A/B Testing?\n",
    "\n",
    "I haven't yet proved to you that what we did above is actually _correct_. All we've done is show that the Beta Distribution _seems_ to do what we want if we set $\\alpha$ and $\\beta$ to the number of conversions and failures, respectively. Let's work on showing _why_ this works.\n",
    "\n",
    "We can derive it by beginning with Bayes' Theorem. First, let's just recall the theorem:\n",
    "\n",
    "![](images/bayes_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\Theta$ represents the random variable that is our unknown conversion rate, and $y$ is the number of conversions we've observed on the website to-date.\n",
    "\n",
    "Next, we note that the denominator on the right ($P(y)$) is just a normalizing term, so we'll simplify it to:\n",
    "\n",
    "$$P(\\Theta \\, | \\, y) \\propto P(y \\, | \\, \\Theta) P(\\Theta)$$\n",
    "\n",
    "In the past we've only put scalar values into each part of the equation above, but... what if we plugged PDF equations into each part? Let's try it.\n",
    "\n",
    "Our prior ($P(\\Theta)$) will be a uniform distribution initially, meaning we don't have any initial belief about what values $\\Theta$ should be -- we see all values as equally likely (we don't _have_ to do it this way, but this will work fine).  We use the $\\beta$ here, however, because it can give a uniform distribution initially but evolve as data comes in.\n",
    "\n",
    "Our likelihood distribution ($P(y \\, | \\, \\Theta)$) is a Binomial distribution. It will tell us the likelihood of our data under various values of $\\Theta$. \n",
    "\n",
    "Finally, the posterior distribution ($P(\\Theta \\, | \\, y)$) tells us what we actually want to know: the relative probability of each value of $\\Theta$ (i.e. the relative probability of each possible conversion rate).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\Theta \\, | \\, y) &\\propto P(y \\, | \\, \\Theta) * P(\\Theta) \\\\\n",
    "P(\\Theta \\, | \\, y) &\\propto \\text{Binomial}(n, \\Theta) * \\text{Beta}(\\alpha, \\beta) \\\\\n",
    "P(\\Theta \\, | \\, y) &\\propto {n \\choose y} \\Theta^y (1-\\Theta)^{n-y} * \\dfrac{\\Theta^{\\alpha - 1} (1-\\Theta)^{\\beta - 1}}{B(\\alpha, \\beta)} \\\\\n",
    "P(\\Theta \\, | \\, y) &\\propto \\Theta^y (1-\\Theta)^{n-y} * \\Theta^{\\alpha - 1} (1-\\Theta)^{\\beta - 1} \\\\\n",
    "P(\\Theta \\, | \\, y) &\\propto \\Theta^{\\alpha + y - 1} (1-\\Theta)^{\\beta + n - y - 1} \\\\\n",
    "P(\\Theta \\, | \\, y) &= \\text{Beta}(\\alpha' = \\alpha + y, \\, \\, \\beta' = \\beta + n - y) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "All that's to say, if you multiply a Beta distribution by a Binomial distribution, out pops a *new* Beta distribution:\n",
    "\n",
    "$$\\text{Beta} \\propto \\text{Binomial} * \\text{Beta}$$\n",
    "\n",
    "This relationship, where your prior and posterior are of the same distribution family, is called a \"conjugate prior\".  The Beta distribution is a \"conjugate prior\" of the binomial distribution. There are many more [conjugate prior relationships.](https://en.wikipedia.org/wiki/Conjugate_prior)\n",
    "\n",
    "![](images/conj_prior.png)\n",
    "\n",
    "The derivation above shows that our process for modeling conversion rates on websites is sound. What's really going on is that we've found a short-cut way of applying Bayes' theorem to update our prior beliefs. We usually make our first prior just a uniform distribution by using a Beta distribution with $\\alpha=1$ and $\\beta=1$.\n",
    "\n",
    "To recap, we model the conversion rate as a Beta distribution where:\n",
    "- $\\alpha = 1 + \\text{number of conversions on our website}$\n",
    "- $\\beta = 1 + \\text{number of misses on our website}$\n",
    "\n",
    "This final plot shows how we can update our belief with more-and-more data to get stronger-and-stronger beliefs of the underlying conversion rate of our website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.load(\"samples.npz\")\n",
    "\n",
    "site_A_samples = x['site_A_samples']\n",
    "\n",
    "len(site_A_samples), np.mean(site_A_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "num_samples = [10, 100, 1000]  # , 10000\n",
    "\n",
    "for k in num_samples:\n",
    "    samples = site_A_samples[:k]\n",
    "    plot_beta_website(samples, ax, label=\"After {} samples\".format(k))\n",
    "\n",
    "ax.set_title(\"Belief about Conversion Rate (True rate is 0.2)\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Words\n",
    "\n",
    "Frequentist methods were created before modern computers. Those methods lean on limits and integrals which can be done by hand. It's from those limits and integrals that they have to take the \"long run\" point of view.\n",
    "\n",
    "Bayesians still build on top of the laws of probability and the well-known distributions, but Bayesians take a different approach to how they interpret probability, which fits better with our needs (usually). Bayesians love to build layers of distributions one atop the other, using Bayes' theorem to string them all together. Once built, they  visualize the final distribution by repeated sampling, requiring a lot of computation (thank you, computers).\n",
    "\n",
    "All scientists love data. We love data because it helps us understand the world. That's why we like the Bayesian mindset. We believe things, we collect data, and we refine our beliefs. Then we repeat that. I leave you with this final _xkcd_.\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/the_difference.png\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
