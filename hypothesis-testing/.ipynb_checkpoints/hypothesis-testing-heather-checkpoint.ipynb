{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Hypothesis Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick note on Normal Distribution\n",
    "\n",
    "The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If $X \\sim B(n, p)$ and if n is large and/or p is close to $\\frac1 2$, then X is approximately $N(np, \\sqrt {npq})$\n",
    "\n",
    "This concept usually comes with some stipulations like \"continuity corrections\" but for the sake of hypothesis testing, it is something we don't need to discuss at length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick visual\n",
    "n=100\n",
    "p = 0.4\n",
    "\n",
    "\n",
    "sigma = np.sqrt(n*p*(1-p))\n",
    "b_dist = stats.binom(n, p)\n",
    "n_dist = stats.norm((n*p), sigma)\n",
    "x = range(int(n*p-sigma*4), int(n*p+sigma*4))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.bar(x, b_dist.pmf(x), align=\"center\", color=\"k\", label = 'Binomial PMF')\n",
    "ax.plot(x, n_dist.pdf(x), c='r', label = 'Normal PDF', linewidth=4)\n",
    "ax.set_title(r'$B(n, p) \\sim N(np, \\sqrt {npq})$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The One Sample Approximate Test of Population Proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the week we had an example where our friend Matt was testing how frequently he can land kickflips.  \n",
    "\n",
    "Suppose he wants to test his hypothesis more rigorously.  Instead of skating for a day, he spends an entire month collecting data.\n",
    "\n",
    "**Let's say** he attempts 100 kickflips a day, for a total of 3100 kickflips, and he lands 2531 of them. Remember, he does not want to move onto another trick until he can cleanly land a kickflip 80% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our Null Hypothesis would take on this type of distribution:\n",
    "\n",
    "$$ \\text{# Kickflips Landed} \\approx Binomial(3100, 0.8) $$\n",
    "\n",
    "In this case, our $N$ is quite large, so it's possible that we have a computer that cannot handle the exact calculations for the binomial distribution (we don't).\n",
    "\n",
    "Luckily a binomial with large $N$ is well approximated by a Normal distribution with the appropriate mean and variance.\n",
    "\n",
    "$$ Binomial(3100, 0.8) \\approx N(3100 \\times 0.8, \\sqrt{3100 \\times 0.8 \\times 0.2}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial = stats.binom(n=3100, p=0.80)\n",
    "binomial_mean = 0.8 * 3100\n",
    "binomial_var = 3100 * 0.8 * 0.2\n",
    "normal_approx = stats.norm(binomial_mean, np.sqrt(binomial_var))\n",
    "x = np.linspace(0, 3100, num=3000)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 6))\n",
    "\n",
    "bar_sizes = [binomial.pmf(i) for i in range(3101)]\n",
    "bars = ax.bar(range(3101), bar_sizes, color=\"k\", align=\"center\")\n",
    "ax.plot(x, normal_approx.pdf(x), linewidth=5, c = 'r')\n",
    "ax.set_xlim(2400, 2600)\n",
    "\n",
    "ax.set_title(\"# of Kickflips Landed Under The Null Hypothesis\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation here is very good, so we can replace the exact Binomial distribution with the approximate Normal distribution.\n",
    "\n",
    "```python\n",
    "binomial_mean = 0.8 * 3100\n",
    "binomial_var = 3100 * 0.8 * 0.2\n",
    "normal_approx = stats.norm(binomial_mean, np.sqrt(binomial_var))\n",
    "```\n",
    "\n",
    "The p-value for this one-month experiment is:\n",
    "\n",
    "What is the probability that we will see this data (2531 Kickflips) or more extreme under the null hypothesis?\n",
    "\n",
    "$$ P(\\geq \\text{ 2531 Kickflips Landed} \\mid \\text{Null Hypothesis} ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 1 - normal_approx.cdf(2530)\n",
    "print(\"p-value for one month kickflip experiment: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(x, normal_approx.pdf(x), linewidth=3)\n",
    "ax.set_xlim(2400, 2600)\n",
    "ax.fill_between(x, normal_approx.pdf(x), \n",
    "                where=(x >= 2530), color=\"red\", alpha=0.5)\n",
    "ax.set_title(\"p-value Region\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The month of practice has made Matt much more confident in his skill. He should definitely move on to another trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximate test for a [population proportion](https://en.wikipedia.org/wiki/Population_proportion) (the \"approximate\" because the Binomial distribution is approximated with a Normal) is often called the **z test for a population proportion** because the tables of tail probabilities of normal distributions that people would use to look up tail probabilities from the normal distribution back in the day were called \"z-tables\".\n",
    "\n",
    "It's an amusing fact that crappy undergrad statistics textbooks still print the mandatory z-tables on their back covers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise**: Matt has worked at galvanize 312 days, and rides the bus two ways each day.  He is thinking of buying a car, as observed that a bus is late often, 330 times to be exact.  He would like to purchase the car if the bus is *truly* late more than half the time.  Set up a z-test to decide if Matt should purchase the car.\n",
    "<!-- \n",
    "# Your work here!\n",
    "# Ridden the bus 312 times 2\n",
    "# late 330 \n",
    "# more than half late\n",
    "n = 312*2\n",
    "p = 0.5\n",
    "\n",
    "binom = stats.binom(n,p)\n",
    "binomial_var = n*p*(1-p)\n",
    "normal = stats.norm(n*p, np.sqrt(binomial_var))\n",
    "\n",
    "\n",
    "x = np.linspace(0, n, num=3000)\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(x, normal.pdf(x), linewidth=3)\n",
    "ax.set_xlim(270,370)\n",
    "ax.fill_between(x, normal.pdf(x), \n",
    "                where=(x >= 330), color=\"red\", alpha=0.5)\n",
    "ax.set_title(\"p-value Region\"); -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example \n",
    "\n",
    "<img src = \"https://imgix.bustle.com/rehost/2016/9/13/f56aa227-6ea1-4f8d-a995-b5c1b3911b62.jpg\" width=300>\n",
    "\n",
    "Karen claims she has a 5th sense... like ESPN or something. She's thinks she can tell when it's already raining at least 30% of the time.\n",
    "\n",
    "Let's say we have some hypotheses:\n",
    "\n",
    "* $H_0$: Karen can't correctly guess the weather more than 30% of the time.\n",
    "* $H_a$: Karen *can* correctly guess the weather more than 30% of the time.\n",
    "\n",
    "On a 100 days we ask Karen if it's raining and record whether or not she's correct. \n",
    "\n",
    "What distribution would be a good choice to model her success rate? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# karen_distribution = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# karen_normal = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we choose $\\alpha = 0.05$. \n",
    "What does this mean?\n",
    "\n",
    "\n",
    "How do we know whether or not to reject the null hypothesis?\n",
    "\n",
    "\n",
    "\n",
    "What does it mean to reject the null hypothesis?\n",
    "\n",
    "What is a p-value?\n",
    "\n",
    "Let's say that Karen correctly identifies that it is raining on **31 out of the 100 days**.\n",
    "\n",
    "Do we reject the null hypothesis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "Confidence level of 95% If you get a p-value of greater than 0.05 then you fail to reject the null hypothesis. If it's less than 0.05 you reject the null hypothesis.\n",
    "\n",
    "We also call alpha the false positive rate.  -->\n",
    "<!-- \n",
    "\n",
    "We've seen good enough evidence to accept alternate hypothesis. More technically, the probability of observing the data given the null hypothesis true is less than the threshold we set. \n",
    "\n",
    "If we're trying to make a decision based on the hypothesis, we're going with the alternative hypothesis. \n",
    " -->\n",
    " \n",
    "<!--  # null hypothesis is p=0.3\n",
    "print(1 - karen_distribution.cdf(31))\n",
    "\n",
    "# what's the threshhold at alpha = 0.05\n",
    "print(karen_distribution.ppf(0.95))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get a visual\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(x, karen_normal.pdf(x), linewidth=3)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.fill_between(x, karen_normal.pdf(x), \n",
    "                where=(x >= 31), color=\"red\", alpha=0.5)\n",
    "ax.set_title(\"p-value Region\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Sample Test\n",
    "\n",
    "Cady (it's pronounced like Katie) thinks she's better at identifying the weather than Karen. \n",
    "\n",
    "* $H_0$: \n",
    "* $H_a$: \n",
    "\n",
    "\n",
    "Karen correctly guessed **31 out of 100 days**, and Cady correctly guesses **40 out of 100 days**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "karen_proportion = 0.31\n",
    "cady_proportion = 0.40\n",
    "\n",
    "# sampling distribution\n",
    "# We do this because the sampling mean and the population mean are the same... think back to \n",
    "karen_sample_mean = karen_proportion\n",
    "cady_sample_mean = cady_proportion\n",
    "\n",
    "\n",
    "karen_sd = np.sqrt(karen_proportion * (1-karen_proportion))\n",
    "cady_sd = np.sqrt(cady_proportion * (1-cady_proportion))\n",
    "karen_sample_mean_sd = karen_sd / np.sqrt(n)\n",
    "cady_sample_mean_sd = cady_sd / np.sqrt(n)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "karen = stats.norm(karen_sample_mean, karen_sample_mean_sd)\n",
    "cady = stats.norm(cady_sample_mean, cady_sample_mean_sd)\n",
    "\n",
    "support = np.linspace(karen.ppf(0.0001), cady.ppf(0.9999), 100)\n",
    "karen_pdf = karen.pdf(support)\n",
    "cady_pdf = cady.pdf(support)\n",
    "\n",
    "ax.plot(support, karen_pdf, color='blue', label='Karen')\n",
    "ax.fill(support, karen_pdf, color='blue', alpha=0.5)\n",
    "ax.plot(support, cady_pdf, color='red', label='Cady')\n",
    "ax.fill(support, cady_pdf, color='red', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of Sample Means for Guessing the Weather\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we test this hypothesis?!\n",
    "\n",
    "We can use a t-test. There are two options for us, the Student's t-test and the Welch t-test.\n",
    "\n",
    "For the Student's t-test, the t statistic is defined as:\n",
    "$$t={\\frac {{\\bar {X}}_{1}-{\\bar {X}}_{2}}{\\sqrt {{\\frac {s_{X_{1}}^{2}+s_{X_{2}}^{2}}{n}}}}}$$\n",
    "In this case we're assuming that the sample size is the same for both groups. There are adjustments that can accommodate different sample sizes.\n",
    "\n",
    "For the Welch t-test, the t statistic is defined as:\n",
    "$$t=\\frac{\\overline {X}_{1}-\\overline {X}_{2}}{\\sqrt{{s_{1}^{2} \\over N_{1}} + {s_{2}^{2} \\over N_{2}}}}$$\n",
    "\n",
    "$\\overline{X}$ is a sample mean.\n",
    "\n",
    "$s$ is a sample standard deviation.\n",
    "\n",
    "$N$ is a sample size.\n",
    "\n",
    "The t distribution has one parameter, $\\nu$ for degrees of freedom. \n",
    "\n",
    "$${\\nu \\approx \\frac{{\\left({s_{1}^{2} \\over N_{1}}+{s_{2}^{2} \\over N_{2}}\\right)^{2}}}{{\\quad {s_{1}^{4} \\over N_{1}^{2}\\nu _{1}}\\;+\\;{s_{2}^{4} \\over N_{2}^{2}\\nu _{2}}\\quad }}}$$\n",
    "\n",
    "Buuuuuut.... we don't need to code this out. We'll just use stats.ttest(sample_1, sample_2)\n",
    "\n",
    "The Student's t requires that we assume the two samples have equal variance.\n",
    "The Welch's t requires that we assume the two samples are normally distributed. \n",
    "\n",
    "The Mann Whitney U test instead finds the probability that if you were to randomly sample from each population, the probability that one sample would be larger than the other. \n",
    "\n",
    "### This is one area of stats where it's not very important to know the formulas, and it's ok to just know how to code it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "karen_proportion = 0.31\n",
    "cady_proportion = 0.40\n",
    "\n",
    "# In python when you multiply a list by an integer, it repeats the list n times. \n",
    "# This is different from broadcasting. In numpy arrays, if you multiply an array by an integer\n",
    "# you will have an array of the same length, where each element is multiplied by that integer.\n",
    "# np.repeat is similar to this list multiplication.\n",
    "# We're coding correct guesses as a 1, and incorrect as a 0. \n",
    "karen_correct_guesses = [1]*int(n*karen_proportion)\n",
    "karen_incorrect_guesses = [0]*int(n*(1-karen_proportion))\n",
    "\n",
    "# Here we're making use of how in python adding two lists together concatenates them. \n",
    "karen_sample = np.array(karen_correct_guesses + karen_incorrect_guesses)\n",
    "cady_sample = np.array([1]*int(n*cady_proportion) + [0]*int(n*(1-cady_proportion)))\n",
    "\n",
    "# ttest in scipy is two sided.\n",
    "print(\"Student's t-test:\")\n",
    "print(stats.ttest_ind(karen_sample, cady_sample))\n",
    "print()\n",
    "print(\"Welch's t-test:\")\n",
    "print(stats.ttest_ind(karen_sample, cady_sample, equal_var=False))\n",
    "print()\n",
    "# \n",
    "print(\"Mann-Whitney U test:\")\n",
    "print(stats.mannwhitneyu(cady_sample, karen_sample, alternative = 'two-sided'))\n",
    "\n",
    "# For one sided t test divide pvalue by 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reject if the p-value is lower than alpha. So here we're going to fail to reject. \n",
    "\n",
    "I would suggest using the U test here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Resources\n",
    "\n",
    "## Non-Parametrics: Mann-Whitney Signed Rank Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Mann-Whitney U-test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a modern alternative to the classical Student's and Welch's t-test that makes good use of modern computing power.  It makes **no** distributional assumptions (unlike the t-test, which assumes the populations are normal), and can always be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Sums and the Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our example of Matt and Nick competing to kickflip higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.scatter(matt_heights, np.repeat(0, len(matt_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.scatter(nick_heights, np.repeat(1, len(nick_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels([\"Matt\", \"Nick\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the Mann-Whitney test is to view this as a competition.  We let each of Nick's kickfips compete against all of Matt's kickflips, and see how many times it wins (i.e. how many of Matt's kickflips it beats).  We then add these number of wins up over all of Nick's kickflips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_winning_pairs(sample_1, sample_2):\n",
    "    sample_1, sample_2 = np.array(sample_1), np.array(sample_2)\n",
    "    n_total_wins = 0\n",
    "    for x in sample_1:\n",
    "        n_wins = np.sum(x > sample_2) + 0.5*np.sum(x == sample_2)\n",
    "        n_total_wins += n_wins\n",
    "    return n_total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick_wins = count_winning_pairs(nick_heights, matt_heights)\n",
    "matt_wins = count_winning_pairs(matt_heights, nick_heights)\n",
    "print(\"Number of Nick Wins: {}\".format(nick_wins))\n",
    "print(\"Number of Matt Wins: {}\".format(matt_wins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the total number of wins is just the total number of comparisons between one of Matt's kickflips and one of Nicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of Wins: {}\".format(nick_wins + matt_wins))\n",
    "print(\"Total Number of Comparisons: {}\".format(\n",
    "    len(nick_heights)*len(matt_heights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of wins we calculated above is called the **Mann-Whitney U Statistic**, or the **Rank Sum Statisitic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The U-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the hypothesis that Nick is better than Matt, we need to adopt a Null hypothesis.  The Null for the Mann-Whitney test is directly related to which competitor is better.\n",
    "\n",
    "> $H_0$: Matt's Kickflips are equally likely to be higher than Nicks as the other way around.  I.e. \n",
    "  \n",
    "  $$P(\\text{Height Matt Kickflip} > \\text{Height Nick Kickflip}) = 0.5$$\n",
    "  \n",
    "As is usual, assuming this null hypothesis is true, the rank-sum statistic assumes a known (but complicated) distribution.  This time we can't write down the distribution in any explicit way, but python can calculate p-values using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.mannwhitneyu(nick_heights, matt_heights, alternative=\"greater\")\n",
    "print(\"p-value for Nick > Matt: {:2.3f}\".format(res.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result as with the t-test: Nick is clearly better than Matt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Suppose you don't have access to a library to calculate the distribution of some statistic under the null hypothesis. How would you calculate p-values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Properties of Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of hypothesis testing can seem awkward and silly when first encountered, and the great number of individual hypothesis tests you need to learn to \"speak the language\" can be overwhelming.  Hopefully our examples have made the idea clear and intuitive.\n",
    "\n",
    "Let's now turn to some higher level properties of hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distribution of p-values Under the Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher gave the first definition of **p-value**, his original intention was only to devise some measure of **strength of evidence** for a scientific hypothesis.\n",
    "\n",
    "The idea of a *rejection threshold* came later, from Neyman–Pearson.  Their idea was to **control the rate of false scientific discoveries**.\n",
    "\n",
    "To explain this, we need to study the distribution of p-values under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are studying a question, and the null hypothesis is **actually true**.  We collect some data, and compute the p-value of this data under the null hypothesis.  The question we want to address is **how do the computed p-values behave probabilistically?**\n",
    "\n",
    "That is, drawing a different sample will result in a different p-value, how are these p-values distributed with respect to different samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Let's brainstorm.  Assuming the Null Hypothesis is **actually true** in some situations, what should be the distribution of p-values we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to do this is to construct a simulation, let's write a function that runs a (one-tailed) one sample z-test on data sampled from the null hypothesis distribution, and returns the p-value of the sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_sample_z_test(n_simulations, sample_size=100, mu=0.0, sigma=1.0):\n",
    "    normal = stats.norm(mu, sigma)\n",
    "    sampling_distribution = stats.norm(mu, sigma / np.sqrt(sample_size))\n",
    "    p_values = []\n",
    "    for _ in range(n_simulations):\n",
    "        sample = normal.rvs(sample_size)\n",
    "        sample_mean = np.mean(sample)\n",
    "        #sample_variance = np.var(sample)\n",
    "        p_value = 1 - sampling_distribution.cdf(sample_mean)\n",
    "        p_values.append(p_value)\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run many simulations, and draw a histogram of the p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = simulate_one_sample_z_test(10**5)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_ = ax.hist(p_values, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "ax.set_title(\"Distribution of p-values Under the Null Hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The p-values from a properly done hypothesis test, in the situation that the null hypothesis is true, are uniformly distributed between 0 and 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact above gives some insight into the idea behind the rejection threshold $\\alpha$.\n",
    "\n",
    "Recall that we set a rejection threshold *before* running the experiment, and it is related to the weight of evidence we require before rejecting the null hypothesis.  I.e., we reject the null hypothesis when our computed p-value is less than our rejection threshold.\n",
    "\n",
    "In the situation where the **null hypothesis is actually true**, setting a rejection threshold of, say, $0.05$ ensures that we will **only falsely reject the null hypothesis 5% of the time**.  Falsely rejecting the null hypothesis is called a **false positive**, or a **type one error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_, _, patches = ax.hist(p_values, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "for i in range(6):\n",
    "    patches[i].set_color(\"green\")\n",
    "    \n",
    "ax.set_title(\"False Positive p-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green bars are the tests in our simulation that falsely reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Repeated Experiment Philosophy\n",
    "\n",
    "> If we repeatedly and properly do a hypothesis test with rejection threshold $\\alpha$ in a situation where our research hypothesis is **false**, then we will only **falsely conclude that it is true** at a rate of $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the repeated experiment philosophy is often stated as the basis for hypothesis testing, it is actually **not** how hypothesis testing is used.  A more accurate philosophy is the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst Case Long Term False Positive Rate\n",
    "\n",
    "> In the worst case situation that **all of our scientific hypothesis are false**, scientists using a rejection threshold of $\\alpha$ for their experiments will have, in the long term, a false positive rate of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Why is the second interpretation much more reasonable than the first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Failing to reject the null hypothesis when the scientific hypothesis is **true** is called a **false negative** or a **type two error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the type two error rate takes some new concepts (mainly statistical power), which will be the subject of tomorrow's lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Testing: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I am considering writing a paper for the **American Journal of Skateboarding Research**.  I have collected data about kickflips in various situations, different terrain, different weather, different shoes, different warmup routines.  I have various questions I would like to ask of this data:\n",
    "\n",
    "  - Does using a crisp, new board increase the height of kickflips?\n",
    "  - Do dry, sunny days increase the height of kickflips?\n",
    "  - Do broken in (as opposed to new) shoes increase the height of kickflips?\n",
    "  - Does an aerobic warmup routine increase the height of kickflips?\n",
    "  \n",
    "Suppose that \n",
    "\n",
    "  - All my alternative hypotheses are, in reality, false. \n",
    "  - I plan to test all these hypotheses using my collected data, and write a paper in **AJSR** if I get a positive result for any of them.\n",
    "  \n",
    "**Question:** If I perform each of these four separate tests at a threshold of $0.05$, what is the true rate at which I reject **at least one** of these hypotheses and publish a false paper in **AJSR**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation here is a standard probabilistic argument.\n",
    "\n",
    "  - The rate I falsely reject a single hypothesis is $\\alpha$.\n",
    "  - The rate I do **not** falsely reject a single hypothesis is $1 - \\alpha$.\n",
    "  - The rate I do **not** falsely reject **each and every one** of the hypotheses is $(1 - \\alpha)^4$.\n",
    "  - The rate I **do** falsely reject **at least one** of the hypotheses is $1 - (1 - \\alpha)^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate = 1 - (1 - 0.05)**4\n",
    "\n",
    "print(\"True combined false positive rate: {:2.2f}\".format(combined_false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this another way.  Suppose that the four p-values we get are $p_1, p_2$, $p_3$, and $p_4$.  Then we falsely reject the combined hypothesis whenever at least one of the p-values is less than $0.05$\n",
    "\n",
    "$$ min(p_1, p_2, p_3, p_4) < 0.05 $$\n",
    "\n",
    "To keep the false positive rate under control, we would need this to happen only $5\\%$ of the time.\n",
    "\n",
    "We can re-do our simulation and see how often a composite of three hypotheses is falsely rejected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = simulate_one_sample_z_test(4*10**5)\n",
    "p_values_combined = np.array([min(w, x, y, z) for w, x, y, z in \n",
    "                              zip(p_values[::4], p_values[1::4], p_values[2::4], p_values[3::4])])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_, _, patches = ax.hist(p_values_combined, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "for i in range(6):\n",
    "    patches[i].set_color(\"green\")\n",
    "    \n",
    "ax.set_title(\"False Positive p-values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate = np.sum(p_values_combined <= 0.05) / float(len(p_values_combined))\n",
    "print(\"Combined False Positive Rate: {:2.2f}\".format(combined_false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 18% of the time, we will end up publishing a false paper.  This is **BAD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bonferroni Correction** is a popular way to rectify the over testing issue.\n",
    "\n",
    "Suppose we want to test a combined hypothesis as a threshold of $\\alpha$.  The bonferroni correction procedure then tests each of the individual hypotheses at a threshold of\n",
    "\n",
    "$$ \\alpha_\\text{Bonferroni} = \\frac{\\alpha}{\\text{# of Hypotheses in Combined Hypothesis}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this fixes the issue in our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate_corrected = np.sum(\n",
    "    p_values_combined <= 0.05 / 4) / float(len(p_values_combined))\n",
    "print(\"Combined False Positive Rate: {:2.2f}\".format(combined_false_positive_rate_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutiple Testing Over Time: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're studying the impact of changing the layout of your skateboarding website.  The website is intended to map out the skating spots in cities around the country, user's can add spots, photos, and comments.\n",
    "\n",
    "You've made some changes to the comment system, and are hoping that it will draw more forum comments from users.\n",
    "\n",
    "Your plan to test this is to split users of your site into two groups, one group will always see the new layout, and one will always see the old.  You plan to run the site this way for two months, and in the end test whether the users with the new layouts generated more forum comments than those under the old layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some data under the **truth** that the new website is **slightly worse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)\n",
    "\n",
    "old_forum_comment_rate = 0.2   # Old Website: Better.\n",
    "new_forum_comment_rate = 0.18  # New Website: Slightly worse.\n",
    "commenters_per_day = stats.poisson(2)\n",
    "\n",
    "# Same Number of Commenters for Each\n",
    "commenters_per_day_old = commenters_per_day.rvs(2*31)\n",
    "commenters_per_day_new = commenters_per_day.rvs(2*31)\n",
    "\n",
    "# \n",
    "comments_per_day_old = [\n",
    "    stats.binom(commenters, old_forum_comment_rate).rvs(1)\n",
    "    for commenters in commenters_per_day_old]\n",
    "comments_per_day_new = [\n",
    "    stats.binom(commenters, new_forum_comment_rate).rvs(1)\n",
    "    for commenters in commenters_per_day_new]\n",
    "\n",
    "cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "\n",
    "x = np.arange(1, 2*31 + 1)\n",
    "ax.plot(x, cumlative_comments_per_day_old, label=\"Old Layout\")\n",
    "ax.plot(x, cumlative_comments_per_day_new, label=\"New Layout\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Cumulative Number of Comments\")\n",
    "\n",
    "ax.set_title(\"Cumulative Number of Comments over Two Months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we collected makes it look like the new website is **better**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_of_comments_new_layout = np.sum(comments_per_day_new) / float(np.sum(commenters_per_day_new))\n",
    "rate_of_comments_old_layout = np.sum(comments_per_day_old) / float(np.sum(commenters_per_day_old))\n",
    "\n",
    "print(\"Rate of comments, new layout: {:2.2f}\".format(rate_of_comments_new_layout))\n",
    "print(\"Rate of comments, old layout: {:2.2f}\".format(rate_of_comments_old_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our testing framework tells us the truth, we do not reject the null that the new website is not as good as the old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sample_test_of_population_proportions(commentators_new, comments_new, commentators_old, comments_old):\n",
    "    difference_in_sample_proportions = (comments_new / float(commentators_new)) - (comments_old / float(commentators_old))\n",
    "    overall_proportion = (comments_new + comments_old) / float(commentators_new + commentators_old)\n",
    "    test_varaince = ((comments_new + comments_old) * overall_proportion * (1 - overall_proportion)) / (commentators_new + commentators_old)\n",
    "    test_distribution = stats.norm(0, np.sqrt(test_varaince))\n",
    "    p_value = 1 - test_distribution.cdf(difference_in_sample_proportions)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = two_sample_test_of_population_proportions(\n",
    "    np.sum(commenters_per_day_new), np.sum(comments_per_day_new), \n",
    "    np.sum(commenters_per_day_old), np.sum(comments_per_day_old)\n",
    ")\n",
    "\n",
    "print(\"p-value for full experiment: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we cheated.  We looked at the data every day, and ran a hypothesis test on that days data, stopping if we ever got a significant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the sequence of p-values we would get if we run the test every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)\n",
    "cumlative_commenters_per_day_old = np.cumsum(commenters_per_day_old)\n",
    "cumlative_commenters_per_day_new = np.cumsum(commenters_per_day_new)\n",
    "\n",
    "p_values = [\n",
    "    two_sample_test_of_population_proportions(\n",
    "        cumlative_commenters_per_day_new[i], \n",
    "        cumlative_comments_per_day_new[i],\n",
    "        cumlative_commenters_per_day_old[i], \n",
    "        cumlative_comments_per_day_old[i]\n",
    "    )\n",
    "    for i in range(1, 2*31)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(range(1, 2*31), p_values)\n",
    "ax.set_title(\"P-values Over Time\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"p-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peeked at our data any time between the 10'th and 16'th day, we would have made the wrong conclusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(range(1, 2*31), p_values)\n",
    "ax.axvline(10, linestyle=\"--\")\n",
    "ax.axvline(16, linestyle=\"--\")\n",
    "ax.axvspan(10, 16, alpha=0.25, color='red')\n",
    "ax.set_title(\"P-values Over Time\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"p-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What is the danger in testing every day, and stopping the experiment if a significant result is found?  Everything seemed to go ok here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Chi Squared Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Congruential Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been contracted to create a small game that will run on an [embedded system](https://en.wikipedia.org/wiki/Embedded_system) (think, for example, a [Tamagotchi](https://en.wikipedia.org/wiki/Tamagotchi) ).  The system does not have many resources, so you have to create your own random number generator.\n",
    "\n",
    "After some research, you hit on a lightweight solution, a [linear congruential generator](https://en.wikipedia.org/wiki/Linear_congruential_generator).  To scope things out, you code a simple generator in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCongruentialGenerator:\n",
    "    \n",
    "    def __init__(self, a, c, modulus, seed):\n",
    "        self._a = a\n",
    "        self._c = c\n",
    "        self._modulus = modulus\n",
    "        self._seed = seed\n",
    "        \n",
    "    def next(self):\n",
    "        next_sample = (self._a * self._seed + self._c) % self._modulus\n",
    "        self._seed = next_sample\n",
    "        return next_sample\n",
    "    \n",
    "    def sample(self, n):\n",
    "        L = []\n",
    "        for _ in range(n):\n",
    "            L.append(lcm.next())\n",
    "        return np.array(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lookup some parameters to use online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm = LinearCongruentialGenerator(48271, 0, 2**31 - 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to test if you're code really works, which involves assessing the \"randomness\" of your generator.  \n",
    "\n",
    "**Discussion:** How would you assess the quality of your random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your intention in the app is to use the generator to create random six sided dice rolls, so you plan to test whether the generator is appropriate for that application.\n",
    "\n",
    "You have a strong belief that the die is fair, as this code is based on an industry standard method, so your goal here is to detect whether the random number generator is *unfair.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app will only be used in small bursts of activity, so you decide to run these tests on a smallish sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_rolls = lcm.sample(50) % 6\n",
    "dice_freqs = np.bincount(dice_rolls)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "ax.bar(range(1, 7), dice_freqs, align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chi-Squared Test for Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to arrange our results into a **contingency table**, which compares the expected frequency to the observed frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dice_rolls_contingency_table = pd.DataFrame(\n",
    "    {'expected': np.repeat(8.3, 6) , 'actual': dice_freqs}\n",
    ")\n",
    "print(dice_rolls_contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Chi Squared Test for Distribution**, (also known as the **Chi Squared Test for Goodness of Fit** tests the hypothesis that the **actual** data was not generated from a discrete distribution which has expected frequencies as calculated.\n",
    "\n",
    ">$H_0$: The data was generated from a discrete distribution with the given expected frequencies.\n",
    "\n",
    "> $H_{a}$: The data was *not* generated from a discrete distribution with the given expected frequencies.\n",
    "\n",
    "In out situation, a shorter way to express this is\n",
    "\n",
    ">$H_0$: The data generated from the random number generator is consistent with a fair die.\n",
    "\n",
    "> $H_{a}$: The data generated from the random number generator is not consistent with a fair die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** a point of awkwardness here.  We would like to conclude the die is fair, so really our antagonistic hypothesis should be that the die is unfair.  But assuming the die is unfair does not let us create a probabilistic model for the situation under the null.\n",
    "\n",
    "Unfortunately, this is the common logic of Chi squared tests for distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the Contingency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already done this step.  Good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the Chi Squared test is to compute the following test statistic:\n",
    "\n",
    "$$ T = \\sum_i \\frac{(O_i - E_i)^2}{E_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test_statistic(observeds, expecteds):\n",
    "    numerators = (observeds - expecteds)**2\n",
    "    ratios = numerators / expecteds\n",
    "    return np.sum(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of the Chi Squared test is that this quantity follows a certain distribution, the chi squared distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preform the Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi Squared distribution has one parameter, the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [1, 2, 3, 4, 5]\n",
    "x = np.linspace(0, 3, num=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "for df in dfs:\n",
    "    chisq = stats.chi2(df)\n",
    "    ax.plot(x, chisq.pdf(x), linewidth=2,\n",
    "            label=\"Degree of Freedom: {}\".format(df))\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi squared test provides the degrees of freedom as:\n",
    "    \n",
    "$$ \\text{Number of Cells in the Contingency Table} - \\text{Number of Parameters in Distribution} + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's apply this to our contingency tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dice Rolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our degrees of freedom for the Chi Squared distribution is\n",
    "\n",
    "$$ 6 - 1 + 1 = 6 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test statistic is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = chi_squared_test_statistic(dice_rolls_contingency_table.actual, \n",
    "                               dice_rolls_contingency_table.expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our p-value for the test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distribution = stats.chi2(6)\n",
    "p_value = 1 - test_distribution.cdf(T)\n",
    "print(\"p-value for dice rolls: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this experiment does *not* give us enough evidence to conclude that our random number generator produces fair rolls.  Looks like we should do more research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Do the other test, of consecutive differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Secret Code to Find Random Seed that Rejects In Forum Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_on_day(seed, burn_in=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    old_forum_comment_rate = 0.2   # Old Website: Better.\n",
    "    new_forum_comment_rate = 0.18  # New Website: Slightly worse.\n",
    "    commenters_per_day = stats.poisson(2)\n",
    "\n",
    "    # Same Number of Commenters for Each\n",
    "    commenters_per_day_old = commenters_per_day.rvs(2*31)\n",
    "    commenters_per_day_new = commenters_per_day.rvs(2*31)\n",
    "\n",
    "    # \n",
    "    comments_per_day_old = [\n",
    "        stats.binom(commenters, old_forum_comment_rate).rvs(1)\n",
    "        for commenters in commenters_per_day_old]\n",
    "    comments_per_day_new = [\n",
    "        stats.binom(commenters, new_forum_comment_rate).rvs(1)\n",
    "        for commenters in commenters_per_day_new]\n",
    "\n",
    "    cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "    cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)\n",
    "\n",
    "    p_values = np.array([\n",
    "        two_sample_test_of_population_proportions(\n",
    "            cumlative_commenters_per_day_new[i], cumlative_comments_per_day_new[i],\n",
    "            cumlative_commenters_per_day_old[i], cumlative_comments_per_day_old[i]\n",
    "        )\n",
    "        for i in range(1, 2*31)\n",
    "    ])\n",
    "    \n",
    "    return any(p_values[burn_in:] <= 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1, 50000):\n",
    "    if reject_on_day(seed, burn_in=15):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0, 1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(222)\n",
    "np.random.uniform(0, 1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
