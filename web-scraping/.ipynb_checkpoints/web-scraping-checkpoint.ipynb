{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Requests sends and recieves HTTP requests.\n",
    "import requests\n",
    "\n",
    "# Beautiful Soup parses HTML documents in python.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "import time\n",
    "import copy \n",
    "from bson import ObjectId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "\n",
    "##  Success criteria:\n",
    "\n",
    "I will be successful today if I can...\n",
    "\n",
    "1. Identify some use cases for mongo-db as opposed to traditional RDBMS\n",
    "2. Create, Read, Update and Delete documents using the mongo shell\n",
    "2. Use pymongo and requests to pull a website's html into a collection \n",
    "3. Use beautiful soup to parse html\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. *Review* MongoDB and PyMongo\n",
    "2. *Explain* and *Use* the basic concepts of HTML with regard to fetching data:\n",
    "    * Connecting to web pages from Python\n",
    "    * Parsing HTML in Python\n",
    "3. *Write* code to pull elements from websites using the BeautifulSoup library \n",
    "4. *Describe* a typical pipeline for scraping data from the web and apply this process to scrap monster.com for data science jobs in Austin.\n",
    "5. *Use* public API's to fetch pre-formatted data using BeautifulSoup.\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [w3 schools](http://www.w3schools.com/) : HTML tags and thier attributes.\n",
    "* [BeautifulSoup Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Review MongoDB and PyMongo\n",
    "\n",
    "MongoDB and PyMongo is a NoSQL database program.  Let's first compare SQL to NoSQL. \n",
    "\n",
    "## SQL versus NoSQL \n",
    "\n",
    "### Example: Structure of a Relational Database versus a Document Oriented Database: \n",
    "\n",
    "<center><img src=\"img/sql_vs_mongo_table.png\" style=\"width: 700px\"></center>\n",
    "\n",
    "![](img/rd_mongo.jpg)\n",
    "\n",
    "\n",
    "## MongoDB\n",
    "\n",
    "### What's it about? \n",
    "\n",
    "* MongoDB is a document-oriented database, an alternative to RDBMS, used for storing semi-structured data.\n",
    "* JSON-like objects form the data model, rather than RDBMS tables.\n",
    "* Schema is optional.\n",
    "* Sub-optimal for complicated queries.\n",
    "\n",
    "### Structure of the database.\n",
    "\n",
    "* MongoDB is made up of databases which contain collections (tables).\n",
    "* A collection is made up of documents (analogous to rows or records).\n",
    "* Each document is a JSON object made up of key-value pairs (analogous to columns).\n",
    "\n",
    "So a RDBMS defines columns at the table level, document oriented database defines its fields at a document level.\n",
    "\n",
    "## PyMongo \n",
    "\n",
    "In this lecture, we will create a database and collection.  Then we will insert documents into a collection. Let's see how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the hosted MongoDB instance \n",
    "client = MongoClient('localhost', 27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['reddit'] # create or access already existing database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names() #any collections available? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = db['comments'] # access already existing collection (this would also create a new collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments.find():  # find documents in collection from this morning\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.insert_one(\n",
    "            {'comment_id':'laugher',\n",
    "            'body': 'lol'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for comment in comments.find():  # find documents in collection from this morning\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete an extra item if you ran the above insert_one twice\n",
    "#db.comments.delete_one({'_id': ObjectId('6054e7743794b58c916539eb')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# *Explain* and *Use* the basic concepts of HTML with regard to fetching data\n",
    "\n",
    "* Documents on the web are generally written in <span style=\"text-decoration: underline\">**H**yper**T**ext **M**arkup **L**anguage</span>, HTML, which can be natively viewed by browsers, the tool that we use to browse the web.\n",
    "\n",
    "### HTML\n",
    "\n",
    "**H**yper**T**ext **M**arkup **L**anguage\n",
    "\n",
    "A *markup language* (think markdown) that forms the building blocks of all websites. It specified not just the text of the document but also the organization (into sections and paragraphs and lists and such). It can also control the layout of the document (the font and color and size and such) though that is properly handled with Cascading Style Sheets (CSS)\n",
    "\n",
    "It consists of opening and closing tags enclosed in angle brackets (like `<html>` and `</html>`) often with more HTML in between.\n",
    "\n",
    "A minimal HTML document, unfortuantely, contains a lot of cruft.  Here's one I got from [https://www.sitepoint.com/a-minimal-html-document/](https://www.sitepoint.com/a-minimal-html-document/).\n",
    "\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
    "    \"http://www.w3.org/TR/html4/strict.dtd\">\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "  \n",
    "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">\n",
    "    <title>title</title>\n",
    "    <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
    "    <script type=\"text/javascript\" src=\"script.js\"></script>\n",
    "  </head>\n",
    "  <body>\n",
    "\t\t\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "The key=value pairs inside of a tag are called attributes. The `<link>` and `<script>` tags aren't necessary, but appear in more or less every HTML document.\n",
    "\n",
    "* The `<link>` tag points to a **stylesheet**, which controls how different parts of the document are rendered in the browser.  This makes things pretty.\n",
    "* The `<script>` tag points to a **javascript** program.  This allows programmers to add *dynamic behaviour* to a html document.\n",
    "* The `<body>` tag contains the guts of your document.\n",
    "\n",
    "### Important Tags\n",
    "\n",
    "```html\n",
    "<a href=\"http://www.w3schools.com\">A hyperlink to W3Schools.com!</a>\n",
    "\n",
    "<h1>This is a header!</h1>\n",
    "\n",
    "<p>This is a paragraph!</p>\n",
    "\n",
    "<h2>This is a Subheading!</h2>\n",
    "\n",
    "<table>\n",
    "  This is a table!\n",
    "  <tr>\n",
    "    <th>The header in the first row.</th>\n",
    "    <th>Another header in the first row.</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>An entry in the second row.</td>\n",
    "    <td>Another entry in the second row.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  This is an unordered list!\n",
    "  <li>This is the first thing in the list!</li>\n",
    "  <li>This is the second thing in the list!</li>\n",
    "</ul>\n",
    "<div>Specifies a division of the document, generally with additional attributes specifying layout and behavior.</div>\n",
    "A <span>span is similar</span> but occurs in the middle of a line.\n",
    "\n",
    "```\n",
    "\n",
    "## HTTP Requests\n",
    "\n",
    "To get data from the web, you need to make a HTTP request.  The two most important request types are:\n",
    "\n",
    "* GET (queries data, no data is *sent*)\n",
    "    - used for fetching documents\n",
    "    \n",
    "\n",
    "\n",
    "* POST (updates data, *data must be sent*)\n",
    "    - used for updating data\n",
    "\n",
    "    \n",
    "<br>\n",
    "\n",
    "Usually HTTP requests are sent by browsers (like Chrome or Safari) but `curl` is a command line program for sending HTTP requests.  It's easy to send a `GET` request to a url.\n",
    "\n",
    "## Requests Library\n",
    "\n",
    "* The [requests](http://docs.python-requests.org/en/latest/index.html) library is designed to simplify the process of making http requests within Python.\n",
    "* The interface is mindbogglingly simple:\n",
    "    1. Instantiate a requests object to the request, this will mostly be a `get`, with the URL and optional parameters you'd like passed through the request.\n",
    "    2. That instance makes the results of the request available via attributes/methods, i.e. we now have a python object representation of the website to play with.\n",
    "    \n",
    "Let's do a simple demo where I get the hypertext from deertier.com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "deer_tier_url = 'http://deertier.com/Leaderboard/AnyPercentRealTime'\n",
    "r = requests.get(deer_tier_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status code of `200` means that everything went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the HTML via text attribute.  Below I will use the pprint module to make printed text readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# *Write* code to pull elements from websites using the BeautifulSoup library \n",
    "\n",
    "In order to demo Beautiful Soup, we will first parse a simple webpage in this repo saved to the file name **basic.html**.  First let's read in this file as a string and pass it along to beautiful soup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in html\n",
    "with open('basic.html', 'r') as myfile:\n",
    "    html_str = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass to beautiful soup\n",
    "soup = BeautifulSoup(html_str, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Beautiful Soup: Tools and Finding tags\n",
    "\n",
    "First you can easily find HTML tags in the original document with Beautiful Soup.  One way is to use the tag's name.  For example below I find the title of our document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tag may have attribute(s). You can access these attributes like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.div['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to grab the second table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to grab all the `<table>` tags you can use the find_all() method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can specify the tag and html attributes using find_all(); if no attribute name given it is the class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div','myDiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tables[1].find_all('th')\n",
    "rows = tables[1].find_all('tr')\n",
    "indices,rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "columns = {}\n",
    "for index in indices:\n",
    "    columns[index.text] = None\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "keys = list(columns.keys())\n",
    "for i,row in enumerate(rows):\n",
    "    if i > 0:\n",
    "        new_row = copy.copy(columns)\n",
    "        entries = row.find_all('td')\n",
    "        for j,entry in enumerate(entries):\n",
    "            new_row[keys[j]]= entry.text\n",
    "        all_data.append(new_row)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Describe* a typical pipeline for scraping data from the web.\n",
    "\n",
    "We now have all the tools we need to scrap a webpage! Let's first define the process: \n",
    "\n",
    "![](img/web_scraping_procedure.png)\n",
    "\n",
    "Now let's scape the web! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Scrap Monster.com for Data Science Jobs! \n",
    "\n",
    "## Step1: Inspect the page you plan to scrap\n",
    "\n",
    "https://www.monster.com/jobs/search/?q=Data-Scientist&where=Denver__2C-CO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Request the webpage's raw HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.monster.com/jobs/search/?q=Data-Scientist&where=Denver__2C-CO'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the status code to make sure you were successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint.pprint(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Save the raw HTML into a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = db.data_science_colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages.insert_one({'html': r.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse the hypertext to get data with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all('section','card-content')[1] # get's info for each job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('section','card-content')[1].find('h2','title') # get info on with job title and link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the .text method to eliminate some of the html noise\n",
    "soup.find_all('section','card-content')[1].find('h2','title').text # get job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here again we see using the .a gets us into the the anchor tag \n",
    "# and specifying the href in brackets eliminatees the html noise\n",
    "soup.find_all('section','card-content')[1].find('h2','title').a['href'] # get link to job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('section','card-content')[1].find('div','company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('section','card-content')[1].find('div','company').span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather information from sub-pages \n",
    "Next, I will want to loop through all job postings on the Monster webpage, go to the hyperlink and get more info.\n",
    "\n",
    "First I will look through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = []\n",
    "job_title = []\n",
    "company = []\n",
    "for i,job in enumerate(soup.find_all('section','card-content')):\n",
    "    if job.h2 != None:    # skips the first 'card-content section which does not have job title'\n",
    "        link.append(job.find('h2','title').a['href'])\n",
    "        job_title.append(job.find('h2','title').text.rstrip())   # rstrip will remove things like '\\n' from the job title\n",
    "        company.append(job.find('div','company').span.text.rstrip())\n",
    "\n",
    "\n",
    "[j for j in job_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Repeat! \n",
    "![](img/web_scraping_procedure.png)\n",
    "Next I want to go through the list of hyperlinks, grab the hypertext from each page, throw it in a MongoDB (that will prevent us from having to re-scrape), and parse the code again.  That's what's happening below.  I will get when the job was posted from each of the linked webpages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when_post = []\n",
    "\n",
    "#loop through all of the links available in the href that was already saved to a list\n",
    "for links in link:\n",
    "    # step 1: inspect webpage -- since they have similar structure, I can check out one of the links\n",
    "    # step 2: get the HTML code via requests library\n",
    "    sub_page = requests.get(links)\n",
    "    \n",
    "    #step 3: put HTML into MongoDB\n",
    "    #new collection\n",
    "    pages = db.data_science_links\n",
    "    pages.insert_one({'html': sub_page.content})\n",
    "    \n",
    "    try:\n",
    "        # step4: parse with beautiful soup \n",
    "        sub_soup = BeautifulSoup(sub_page.text, 'html.parser')\n",
    "        # get HTML that list when job was posted\n",
    "        posted = sub_soup.find('div',{'name':'value_posted'}).text\n",
    "\n",
    "        # append this info to a when_post list \n",
    "        when_post.append(posted)\n",
    "\n",
    "        print('When posted:', posted )\n",
    "        time.sleep(2)   # If you request too much code too quickly you can get banned from scrapping the site! \n",
    "        # Add time between request to try to prevent this from happening\n",
    "    except AttributeError:\n",
    "         when_post.append('not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.data_science_links.count_documents({}), db.data_science_colorado.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets close our connection since we should be done inserting info into our db for now \n",
    "#we can query from it later if we want different info\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'job_title': job_title, 'company':company, 'posted': when_post,'link':link})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('monster_ds_job_data.csv') # write out pandas DF to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "* An API is a way for developers to communicate with a certain application against a specific contract\n",
    "* An API is typically defined as a set of Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, which is usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format.\n",
    "\n",
    "* Finally, lets get some COVID data via an API: https://covidtracking.com/data/api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.covidtracking.com/v1/states/az/daily.json'\n",
    "r = requests.get(api_url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! Let's throw the covid data into a pandas library, and see how AZ is doing... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_data = pd.DataFrame.from_dict(r.json())\n",
    "az_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will want to plot hospitalizations, so I will first drop any nulls\n",
    "az_data = az_data.dropna(subset=['hospitalizedCurrently'])\n",
    "az_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(az_data.index.to_numpy()[::-1], az_data['hospitalizedCurrently'].to_numpy())\n",
    "ax.set_xlabel('Days Since April 13th 2020')\n",
    "ax.set_ylabel('# Currently Hospitalized')\n",
    "ax.set_title('AZ Covid Hospitalizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
