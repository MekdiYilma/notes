{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDDs\n",
    "\n",
    "## Success Criteria\n",
    "Today I will be successful if I can...\n",
    "- Define/Explain Big data\n",
    "- Define what an RDD is by its properties and operations\n",
    "- Explain Lazy Evaluation\n",
    "- Explain the difference between Transformations and Actions on an RDD\n",
    "- Implement .map(funct) and .filter(funct) transformations on an RDD\n",
    "- Implement .collect() and .take(n) Actions on an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "\n",
    "## Spark is a tool for parallelized computation\n",
    "- Highly efficient distributed operations\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk.\n",
    "- Spark keeps everything in memory when possible, uses lots of it.\n",
    "\n",
    "<img src=\"images/spark_ecosystem.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing data storage: Resilient Distributed Datasets (RDD)\n",
    "\n",
    "<img src=\"images/rdd_on_cluster.png\" width=\"200\" align=\"right\">\n",
    "\n",
    "[Image Source](http://horicky.blogspot.com/2015/02/big-data-processing-in-spark.html)\n",
    "\n",
    " \n",
    "RDDs are the primary class introduced by Spark. It is a data container that allows for the construction of RDDs.\n",
    "\n",
    "- **immutable** \n",
    "- **lazily evaluated**\n",
    "- **cacheable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing computation: a \"functional programming paradigm\" and DAGs\n",
    "\n",
    "Spark provides many **transformation** functions. By programming these functions, you construct a **Directed Acyclic Graph** (DAG) of steps to execute the transformation.\n",
    "\n",
    "<img src=\"images/dag.png\">\n",
    "\n",
    "When you use them, these functions are passed from the **client** to the **master**(The machine on which the Driver program runs), who then distributes them to workers, who apply them across their partitions of the RDD.\n",
    "\n",
    "\n",
    "<!-- \n",
    "Remember the image from slides:\n",
    "\n",
    "<img src=\"images/spark_components.png\">\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark architecture : from your coding hands to the cluster\n",
    "\n",
    "<img src=\"images/from_rdd_to_cluster.png\">\n",
    "\n",
    "References: \n",
    " - https://trongkhoanguyen.com/spark/understand-rdd-operations-transformations-and-actions/ \n",
    "\n",
    "You construct your sequence of transformations in python.\n",
    "\n",
    "Spark functional programming interface builds up a **DAG**\n",
    "\n",
    "This DAG is sent by the **driver** for execution functional programming interface builds up a DAGto the **cluster manager**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: Spark/RDD in Python\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above).\n",
    "1. create the environment to run spark from python\n",
    "2. extract RDDs from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Initializing a `SparkSession`\n",
    "\n",
    "IPython / IPython notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkSession` that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"images/spark_driver_etc.png\"/>\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"df lecture\") \\\n",
    "            .getOrCreate()\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[2]\") \\\n",
    "            .appName(\"Spark Session Intro\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1616300522023'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '42317'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.app.name', 'Spark Session Intro'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', 'dc66c93859ea'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1616300523226'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[2]'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/jovyan/work/Desktop/dsi/notes/spark-rdds/spark-warehouse')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Creating an RDD\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from a|n external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| [`sc.parallelize(array)`]() | Create an RDD from a python array or list |\n",
    "| [`sc.textFile(path)`]() | Create an RDD from a text file |\n",
    "| [`sc.pickleFile(path)`]() | Create an RDD from a pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Creating RDDs from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.parallelize()` : create an RDD from a python array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[5] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to output the content in python [irl, use with great care]\n",
    "print(rdd.collect())\n",
    "type(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl.csv                  input.txt       sales.json     \u001b[0m\u001b[01;34mtoy_data.pkl\u001b[0m/\n",
      "airline-data-extract.csv  sales2.json.gz  sales.txt      toy_data.txt\n",
      "cookie_data.txt           sales.csv       toy_dataB.txt\n"
     ]
    }
   ],
   "source": [
    "ls spark_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('spark_data/toy_data.txt')\n",
    "print(f.read())\n",
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      " <class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('spark_data/toy_data.txt', 'r') as data:\n",
    "    print(data.read(), type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('spark_data/toy_data.txt')\n",
    "\n",
    "# to output the content in python [irl, use collect() with great care]\n",
    "print(rdd.collect())\n",
    "type(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">`sc.pickleFile()` : from a HDFS pickle file\n",
    "\n",
    "The import will give you an rdd composed of whatever table was stored into that file.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%ls spark_data/toy_data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "ls spark_data/toy_data.pkl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.pickleFile('spark_data/toy_data.pkl')\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Creating RDDs from S3\n",
    "\n",
    "You can read from S3 too, but it takes some configuration to make it work.  See this [post](https://medium.com/@bogdan.cojocar/how-to-read-json-files-from-s3-using-pyspark-and-the-jupyter-notebook-275dcb27e124) as a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to the S3 repository\n",
    "\n",
    "link = 's3a://mortar-example-data/airline-data'\n",
    "# rdd = sc.textFile(link)  # this won't work, but give it a shot\n",
    "\n",
    "rdd = sc.textFile('spark_data/airline-data-extract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform an RDD into another RDD because RDD are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "<h5><center>http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html</center></h5>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42],\n",
    "              ['matthew', 7],\n",
    "              ['jorge', 2],\n",
    "              ['josh', 10],\n",
    "              ['evangeline', 90],\n",
    "              ['emilie', -4],\n",
    "              ['yunjin', 65]]\n",
    "\n",
    "another_rdd = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['matthew', 4],\n",
       "  ['jorge', 8],\n",
       "  ['josh', 15],\n",
       "  ['evangeline', 16],\n",
       "  ['emilie', 23],\n",
       "  ['yunjin', 42]],\n",
       " [['matthew', 7],\n",
       "  ['jorge', 2],\n",
       "  ['josh', 10],\n",
       "  ['evangeline', 90],\n",
       "  ['emilie', -4],\n",
       "  ['yunjin', 65]]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42],\n",
       " ['matthew', 7],\n",
       " ['jorge', 2],\n",
       " ['josh', 10],\n",
       " ['evangeline', 90],\n",
       " ['emilie', -4],\n",
       " ['yunjin', 65]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['evangeline', 16],\n",
       " ['yunjin', 42],\n",
       " ['jorge', 2],\n",
       " ['josh', 10],\n",
       " ['evangeline', 90],\n",
       " ['emilie', -4]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#narrow transformation\n",
    "another_rdd2 = another_rdd.filter(lambda x: x[1]%2==0)\n",
    "another_rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['matthew', 4], ['jorge', 8], ['evangeline', 16], ['yunjin', 42]],\n",
       " [['jorge', 2], ['josh', 10], ['evangeline', 90], ['emilie', -4]]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take another look at the partitions\n",
    "another_rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('josh', 15),\n",
       " ('evangeline', 90),\n",
       " ('matthew', 7),\n",
       " ('jorge', 8),\n",
       " ('emilie', 23),\n",
       " ('yunjin', 65)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wide transformation\n",
    "another_rdd3 = another_rdd.reduceByKey(max)\n",
    "another_rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - | - |\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Merge the values for each key using non-associative operation, like mean. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD, which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "In the sequence below, we will in one sequence:\n",
    "1. read an RDD from a text file\n",
    "2. transform by applying `split`\n",
    "3. transform by filtering\n",
    "4. transform by casting some columns to their corresponding type.\n",
    "5. use an action to output the results\n",
    "\n",
    "Each transformation is a method of an RDD, and returns another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aapl.csv                  input.txt       sales.json     \u001b[0m\u001b[01;34mtoy_data.pkl\u001b[0m/\n",
      "airline-data-extract.csv  sales2.json.gz  sales.txt      toy_data.txt\n",
      "cookie_data.txt           sales.csv       toy_dataB.txt\n"
     ]
    }
   ],
   "source": [
    "ls spark_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID    Date           Store   State  Product    Amount\n",
      "101    11/13/2014     100     WA     331        300.00\n",
      "104    11/18/2014     700     OR     329        450.00\n",
      "102    11/15/2014     203     CA     321        200.00\n",
      "106    11/19/2014     202     CA     331        330.00\n",
      "103    11/17/2014     101     WA     373        750.00\n",
      "105    11/19/2014     202     CA     321        200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('spark_data/sales.txt', 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall: Input functions, reading RDDs from files, are functions of the SparkContext.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads a text file line by line\n",
    "rdd1 = sc.textFile('spark_data/sales.txt')\n",
    "\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount'],\n",
       " ['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies split() to each row\n",
    "rdd2 = rdd1.map(lambda rowstr : rowstr.split())\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'101'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters rows\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    id, date, store, state, product, amount = row\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "# applies casting_function to rows\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "\n",
    "# shows the result\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's see the canonical way to write that in Python...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    id, date, store, state, product, amount = row\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('spark_data/sales.txt')\\\n",
    "        .map(lambda rowstr : rowstr.split())\\\n",
    "        .filter(lambda row: not row[0].startswith('#'))\\\n",
    "        .map(casting_function)  \n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black\">From now on we'll rely on 2 rdds, names and sales</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    id, date, store, state, product, amount = row\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('spark_data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map(func)` : applying a function on every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: [['matthew', 7], ['jorge', 5], ['josh', 4], ['evangeline', 10], ['emilie', 6], ['yunjin', 6]]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd\n",
    "rddout = rdd_names.map(lambda x : [x[0] , len(x[0])])\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.flatMap(func)` : applying a function on every row and flattening the resulting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "after: ['matthew', 7, 'jorge', 5, 'josh', 4, 'evangeline', 10, 'emilie', 6, 'yunjin', 6]\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.flatMap(lambda row : [row[0],len(row[0])])\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: {}\".format(rdd_names.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"after: {}\".format(rddout.collect()))\n",
    "# print()\n",
    "# print(rdd_names.glom().collect())\n",
    "# print(rddout.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Row reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "(101, '11/13/2014', 100, 'WA', 331, 300.0)\n",
      "(104, '11/18/2014', 700, 'OR', 329, 450.0)\n",
      "(102, '11/15/2014', 203, 'CA', 321, 200.0)\n",
      "(106, '11/19/2014', 202, 'CA', 331, 330.0)\n",
      "(103, '11/17/2014', 101, 'WA', 373, 750.0)\n",
      "(105, '11/19/2014', 202, 'CA', 321, 200.0)\n",
      "\n",
      "after: \n",
      "(102, '11/15/2014', 203, 'CA', 321, 200.0)\n",
      "(106, '11/19/2014', 202, 'CA', 331, 330.0)\n",
      "(105, '11/19/2014', 202, 'CA', 321, 200.0)\n"
     ]
    }
   ],
   "source": [
    "# filtering an rdd\n",
    "rddout = rdd_sales.filter(lambda row: (row[3] == 'CA'))\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before:\")\n",
    "for record in rdd_sales.collect():\n",
    "    print(record)\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\nafter: \")\n",
    "for record in rddout.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!  Very useful if you don't want to work with the whole dataset at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "(101, '11/13/2014', 100, 'WA', 331, 300.0)\n",
      "(104, '11/18/2014', 700, 'OR', 329, 450.0)\n",
      "(102, '11/15/2014', 203, 'CA', 321, 200.0)\n",
      "(106, '11/19/2014', 202, 'CA', 331, 330.0)\n",
      "(103, '11/17/2014', 101, 'WA', 373, 750.0)\n",
      "(105, '11/19/2014', 202, 'CA', 321, 200.0)\n",
      "\n",
      "after: \n",
      "(101, '11/13/2014', 100, 'WA', 331, 300.0)\n",
      "(104, '11/18/2014', 700, 'OR', 329, 450.0)\n",
      "(102, '11/15/2014', 203, 'CA', 321, 200.0)\n"
     ]
    }
   ],
   "source": [
    "# sampling an rdd\n",
    "rddout = rdd_sales.sample(False, 0.55, 42)\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before:\")\n",
    "for record in rdd_sales.collect():\n",
    "    print(record)\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\nafter: \")\n",
    "for record in rddout.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "(101, '11/13/2014', 100, 'WA', 331, 300.0)\n",
      "(104, '11/18/2014', 700, 'OR', 329, 450.0)\n",
      "(102, '11/15/2014', 203, 'CA', 321, 200.0)\n",
      "(106, '11/19/2014', 202, 'CA', 331, 330.0)\n",
      "(103, '11/17/2014', 101, 'WA', 373, 750.0)\n",
      "(105, '11/19/2014', 202, 'CA', 321, 200.0)\n",
      "\n",
      "after: \n",
      "CA\n",
      "WA\n",
      "OR\n"
     ]
    }
   ],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "rddout = rdd_sales.map(lambda row: row[3])\\\n",
    "                    .distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before:\")\n",
    "for record in rdd_sales.collect():\n",
    "    print(record)\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\nafter: \")\n",
    "for record in rddout.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Methods with a `<k,v>` paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.values()`: returns the values of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "['matthew', 4]\n",
      "['jorge', 8]\n",
      "['josh', 15]\n",
      "['evangeline', 16]\n",
      "['emilie', 23]\n",
      "['yunjin', 42]\n",
      "\n",
      "after: \n",
      "4\n",
      "8\n",
      "15\n",
      "16\n",
      "23\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.values()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before:\")\n",
    "for record in rdd_names.collect():\n",
    "    print(record)\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\nafter: \")\n",
    "for record in rddout.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of a RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "['matthew', 4]\n",
      "['jorge', 8]\n",
      "['josh', 15]\n",
      "['evangeline', 16]\n",
      "['emilie', 23]\n",
      "['yunjin', 42]\n",
      "\n",
      "after: \n",
      "matthew\n",
      "jorge\n",
      "josh\n",
      "evangeline\n",
      "emilie\n",
      "yunjin\n"
     ]
    }
   ],
   "source": [
    "# applying a lambda function to an rdd (because why not)\n",
    "rddout = rdd_names.keys()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before:\")\n",
    "for record in rdd_names.collect():\n",
    "    print(record)\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\nafter: \")\n",
    "for record in rddout.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data that's not an obvious key-value pair, you can get strange behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of rdd_sales: [101, 104, 102, 106, 103, 105]\n",
      "\n",
      "Values of rdd_sales: ['11/13/2014', '11/18/2014', '11/15/2014', '11/19/2014', '11/17/2014', '11/19/2014']\n",
      "We are missing other columns of data.  To get them, we would need to transform the data into one column.\n"
     ]
    }
   ],
   "source": [
    "rddout_sales = rdd_sales.keys()\n",
    "print(\"Keys of rdd_sales: {}\".format(rddout_sales.collect()))\n",
    "\n",
    "rddout_sales_values = rdd_sales.values()\n",
    "print(\"\\nValues of rdd_sales: {}\".format(rddout_sales_values.collect()))\n",
    "print(\"We are missing other columns of data.  To get them, we would need to transform the data into one column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WA', 300.0), ('OR', 450.0), ('CA', 200.0), ('CA', 330.0), ('WA', 750.0), ('CA', 200.0)]\n"
     ]
    }
   ],
   "source": [
    "rdd_salesperstate = rdd_sales.map(lambda row: (row[3],row[5]))\n",
    "\n",
    "print(rdd_salesperstate.collect())\n",
    "# rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 'matthew'], ['OR', 'jorge'], ['WA', 'yunjin'], ['TX', 'emilie']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list of managers for each state\n",
    "data_array = [['CA', 'matthew'],\n",
    "              ['OR', 'jorge'],\n",
    "              ['WA','yunjin'],\n",
    "              ['TX', 'emilie']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_managers = sc.parallelize(data_array)\n",
    "rdd_managers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', (200.0, 'matthew')),\n",
       " ('CA', (330.0, 'matthew')),\n",
       " ('CA', (200.0, 'matthew')),\n",
       " ('WA', (300.0, 'yunjin')),\n",
       " ('WA', (750.0, 'yunjin')),\n",
       " ('OR', (450.0, 'jorge'))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to output the content in python [irl, use with great care]\n",
    "rdd_salesperstate.join(rdd_managers).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.reduceByKey(func)`: reduce `v`s by their `k` by applying func (what ?)\n",
    "\n",
    "The `func` here needs to be associative and commutative... can you guess why ?  \n",
    "Commutative property: a + b = b + a  \n",
    "Associative property: a + (b + c) = (a + b) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1],\n",
       " ['WA', 1],\n",
       " ['CA', 2],\n",
       " ['OR', 1],\n",
       " ['CA', 5],\n",
       " ['OR', 1],\n",
       " ['CA', 9],\n",
       " ['CA', 14]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1],\n",
    "             ['CA', 9],\n",
    "             ['CA', 14]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 31), ('WA', 1), ('OR', 2)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda v1,v2 : v1+v2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', [[1, 2], [[5, 9], 14]]), ('WA', 1), ('OR', [1, 1])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda v1,v2: [v1,v2]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupByKey(func)`: reduce `v`s by their `k` by applying func (again ?)\n",
    "\n",
    "This can use any function non-commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CA', 1], ['WA', 1], ['CA', 2], ['OR', 1], ['CA', 5], ['OR', 1]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['CA', 1],\n",
    "              ['WA', 1],\n",
    "              ['CA', 2],\n",
    "              ['OR', 1],\n",
    "              ['CA', 5],\n",
    "              ['OR', 1]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)\n",
    "\n",
    "# to output the content in python [irl, use with great care]\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 6.2), ('WA', 1.0), ('OR', 1.0)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(args):\n",
    "    key,iterator = args\n",
    "    total = 0.0; count = 0\n",
    "    for x in iterator:\n",
    "        total += x; count += 1\n",
    "    return key, total / count\n",
    "\n",
    "keys = rdd.keys().distinct().collect()\n",
    "\n",
    "means = rdd.groupByKey().map(mean).collect()\n",
    "\n",
    "# for k, mn in zip(keys, means):\n",
    "#     print(f\"{k}: {mn:0.2f}\")\n",
    "\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Sorting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['jorge', 8],\n",
       " ['matthew', 4],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting by any function (because why not?)\n",
    "rddout = rdd_names.sortBy(lambda row : (13-row[1])**2, ascending=True)\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[['yunjin', 42], ['emilie', 23], ['evangeline', 16], ['josh', 15], ['jorge', 8], ['matthew', 4]]\n"
     ]
    }
   ],
   "source": [
    "# sorting by any function (because why not?)\n",
    "# rddout = rdd_names.sortBy(lambda row : (13-row[1])**2, ascending=True)\n",
    "\n",
    "#sorting by value descending\n",
    "rddout = rdd_names.sortBy(lambda row : row[1], ascending=False)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n",
      "[('yunjin', 42), ('matthew', 4), ('josh', 15), ('jorge', 8), ('evangeline', 16), ('emilie', 23)]\n"
     ]
    }
   ],
   "source": [
    "# sorting k,v pairs by key\n",
    "rddout = rdd_names.sortByKey(ascending=False)\n",
    "\n",
    "# print out the original rdd\n",
    "print(rdd_names.collect())\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(rddout.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (a python object, or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small, as all the data is loaded into the drivers memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in a RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDDs elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDDs elements. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = [['matthew', 4],\n",
    "              ['jorge', 8],\n",
    "              ['josh', 15],\n",
    "              ['evangeline', 16],\n",
    "              ['emilie', 23],\n",
    "              ['yunjin', 42]]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "rdd_names = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : returning the *full* content of an RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic, cause `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd: <class 'pyspark.rdd.RDD'>\n",
      "type of rdd_collected: <class 'list'>\n",
      "[['matthew', 4], ['jorge', 8], ['josh', 15], ['evangeline', 16], ['emilie', 23], ['yunjin', 42]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "collected = rdd_names.collect()\n",
    "\n",
    "# let's check the type of RDD\n",
    "print(\"type of rdd: {}\".format(type(rdd_names)))\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_collected: {}\".format(type(collected)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : returning (any) n lines of an RDD\n",
    "\n",
    "Returns `n` the rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['matthew', 4],\n",
       " ['jorge', 8],\n",
       " ['josh', 15],\n",
       " ['evangeline', 16],\n",
       " ['emilie', 23],\n",
       " ['yunjin', 42]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_names.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of rdd_taken: <class 'list'>\n",
      "[['matthew', 4], ['jorge', 8], ['josh', 15]]\n"
     ]
    }
   ],
   "source": [
    "# to output the content in python\n",
    "taken = rdd_names.take(3)\n",
    "\n",
    "# let's check the type of what's collected\n",
    "print(\"type of rdd_taken: {}\".format(type(taken)))\n",
    "\n",
    "# let's print the collected content\n",
    "print(taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : returning the first line of an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matthew', 4]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Actions that compute some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: you get that right ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.315302134607444\n"
     ]
    }
   ],
   "source": [
    "print(rdd_names.values().stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's design chains of transformations together !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    id, date, store, state, product, amount = row\n",
    "    return((int(id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "rdd_sales = sc.textFile('spark_data/sales.txt')\\\n",
    "        .map(lambda x : x.split())\\\n",
    "        .filter(lambda x: not x[0].startswith('#'))\\\n",
    "        .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain an RDD of the states sorted by their decreasing cumulated sales.\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WA', 300], ['OR', 450], ['CA', 200], ['CA', 330], ['WA', 750], ['CA', 200]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales1 = rdd_sales.map(lambda row: [row[3], int(row[5])])\n",
    "rdd_sales1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 730), ('WA', 1050), ('OR', 450)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales2 = rdd_sales1.reduceByKey(lambda v1,v2: v1+v2)\n",
    "rdd_sales2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OR', 450), ('CA', 730), ('WA', 1050)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_sales2.sortBy(lambda row: row[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 1050), ('CA', 730), ('OR', 450)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one method:\n",
    "    #just pull out the state and amount\n",
    "    #then get amounts summed per state\n",
    "    #sort info by summed amount\n",
    "\n",
    "rddout = rdd_sales.map(lambda row: [row[3], int(row[5])])\\\n",
    "                .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "                .sortBy(lambda row: row[1], ascending=False)\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_sales.map(lambda x: (x[3],x[5]))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Word count\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "another line\n",
      "yet another line\n",
      "yet another another line\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello world', 'another line', 'yet another line', 'yet another another line']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('spark_data/input.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "rdd_text = sc.textFile('spark_data/input.txt')\n",
    "rdd_text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "You want to create a table of unique words and their occurences.\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "\n",
    "If you had to draw a workflow of the transformations to apply ?\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('another', 4), ('line', 3), ('yet', 2), ('world', 1), ('hello', 1)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text.flatMap(lambda row: row.split())\\\n",
    "        .map(lambda word: (word,1))\\\n",
    "        .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "        .sortBy(lambda v: v[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 1), ('line', 3), ('yet', 2), ('hello', 1), ('another', 4)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout = rdd_text.flatMap(lambda str : str.split())\\\n",
    "                    .map(lambda word: (word,1))\\\n",
    "                    .reduceByKey(lambda v1,v2: v1+v2)\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hint: Get to here first\n",
    "# [('hello', 1),\n",
    "#  ('world', 1),\n",
    "#  ('another', 1),\n",
    "#  ('line', 1),\n",
    "#  ('yet', 1),\n",
    "#  ('another', 1),\n",
    "#  ('line', 1),\n",
    "#  ('yet', 1),\n",
    "#  ('another', 1),\n",
    "#  ('another', 1),\n",
    "#  ('line', 1)]\n",
    "\n",
    "\n",
    "\n",
    "rddout = rdd_text # put your transformations here...\n",
    "\n",
    "rddout.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_text.flatMap(lambda str : str.split())\\\n",
    "            .map(lambda word: (word,1))\\\n",
    "            .reduceByKey(lambda v1,v2: v1+v2)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_aapl_raw = sc.textFile('spark_data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(rdd_aapl_raw.count()))\n",
    "\n",
    "rdd_aapl_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_appl = sc.textFile('spark_data/aapl.csv')\n",
    "\n",
    "rdd_appl.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2016-01-26', '99.989998'],\n",
       " ['2016-07-20', '99.959999'],\n",
       " ['2016-01-12', '99.959999'],\n",
       " ['2016-07-19', '99.870003'],\n",
       " ['2016-05-31', '99.860001']]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_appl.filter(lambda row: not row.startswith('D'))\\\n",
    "        .map(lambda row: [row.split(',')[0],row.split(',')[4]])\\\n",
    "        .sortBy(lambda row: row[1], ascending=False)\\\n",
    "        .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(118.25, '2016-10-25'),\n",
       " (117.650002, '2016-10-24'),\n",
       " (116.599998, '2016-10-21'),\n",
       " (117.059998, '2016-10-20'),\n",
       " (117.120003, '2016-10-19')]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddout =  rdd_appl.filter(lambda line: not line.startswith(\"Date\"))\\\n",
    "                        .map(lambda line: line.split(\",\"))\\\n",
    "                        .map(lambda fields: (float(fields[4]),fields[0]))\\\n",
    "                        .sortBy(lambda row: row[1], ascending=False)\n",
    "\n",
    "# apply transformation to rdd_aapl_raw here\n",
    "\n",
    "rddout.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<details>\n",
    "  <summary>Click here to see the solution below</summary>\n",
    "```\n",
    "rddout = rdd_aapl_raw.filter(lambda line: not line.startswith(\"Date\"))\\\n",
    ".map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda fields: (float(fields[4]),fields[0]))\\\n",
    ".sortBy(lambda row: row[1], ascending=False)\n",
    "\n",
    "rddout.collect()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD: one use case ? oh I don't know maybe... **MACHINE LEARNING !!!**\n",
    "\n",
    "## 3.1. Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in range(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.32 ms, sys: 3.67 ms, total: 7.99 ms\n",
      "Wall time: 81.7 ms\n",
      "CPU times: user 5.29 ms, sys: 3.53 ms, total: 8.82 ms\n",
      "Wall time: 105 ms\n",
      "CPU times: user 6.79 ms, sys: 3.75 ms, total: 10.5 ms\n",
      "Wall time: 121 ms\n",
      "CPU times: user 7.34 ms, sys: 1.56 ms, total: 8.9 ms\n",
      "Wall time: 85.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.07 ms, sys: 567 s, total: 5.63 ms\n",
      "Wall time: 76.7 ms\n",
      "CPU times: user 2.85 ms, sys: 3.92 ms, total: 6.77 ms\n",
      "Wall time: 111 ms\n",
      "CPU times: user 8.04 ms, sys: 3.69 ms, total: 11.7 ms\n",
      "Wall time: 105 ms\n",
      "CPU times: user 5.1 ms, sys: 1.36 ms, total: 6.46 ms\n",
      "Wall time: 51.3 ms\n",
      "CPU times: user 4.44 ms, sys: 371 s, total: 4.82 ms\n",
      "Wall time: 57 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow this was a long walkthrough.\n"
     ]
    }
   ],
   "source": [
    "print('Wow this was a long walkthrough.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
