{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "Today I will be successful if I can...\n",
    "\n",
    "1. Identify difference between spark DataFrame and RDD\n",
    "2. Load data into a Spark DataFrame\n",
    "3. Use various new transformations on a DataFrame\n",
    "4. Utilize the SQL interface on a Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation:\n",
    "\n",
    "Why do we want to learn Spark DataFrames?\n",
    "\n",
    "Spark DataFrames are in most cases, a simpler and more natural way to work with distributed datasets than Spark RDDs. Most operations that you'd like to use can be done through the Spark SQL interface or the Pandas-like. If you are working with Big Data then you likely need to work with Spark, and this is a useful skill.\n",
    "\n",
    "Spark questions may come up in interviews, but only for jobs which require the use of Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RDDs versus DataFrames\n",
    "\n",
    "What is a DataFrame?\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "- DataFrames do not replace RDDs but are actually built ontop of RDDs\n",
    "- Think of a DataFrames as **RDDs with schema**.\n",
    "\n",
    "What is a schema?\n",
    "- Schemas are metadata about your data.\n",
    "- Schemas define table names, column names, and column types over your data.\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs, instead of using column positions.\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "- Schemas enable using column names instead of column positions\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "- Schemas make your data more structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operational DataFrames in Python\n",
    "\n",
    "We'll proceed along the usual spark flow.\n",
    "1. create the environment to run Spark / Spark SQL from python\n",
    "2. create DataFrames from RDDs or from files\n",
    "3. run some transformations\n",
    "4. execute actions to obtain values (local objects in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a `SparkContext` and `SqlContext` in Python\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[4]')\n",
    "```\n",
    "\n",
    "will create a *\"local\"* cluster made of the driver using all 4 cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[4]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e2d451c0171a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=lecture>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e2d451c0171a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lecture</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff56c529520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From an RDD (specifying schema)\n",
    "\n",
    "You can create a DataFrame from an existing RDD (whatever source you used to create this one), **if you add a schema.**\n",
    "\n",
    "To build a schema, you will use existing data types provided in the [`pyspark.sql.types`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) module. Here's a list of the most useful ones (subjective criteria).\n",
    "\n",
    "| Types | Python-like type |\n",
    "| - | - |\n",
    "| StringType | string |\n",
    "| IntegerType | int |\n",
    "| FloatType | float |\n",
    "| ArrayType\\* | array or list |\n",
    "| MapType | dict |\n",
    "\n",
    "\\* see later UDFs (user-defined functions) on how to use that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### csv to rdd to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID,Date,Store,State,Product,Amount\n",
      "101,11/13/2014,100,WA,331,300.00\n",
      "104,11/18/2014,700,OR,329,450.00\n",
      "102,11/15/2014,203,CA,321,200.00\n",
      "106,11/19/2014,202,CA,331,330.00\n",
      "103,11/17/2014,101,WA,373,750.00\n",
      "105,11/19/2014,202,CA,321,200.00\n"
     ]
    }
   ],
   "source": [
    "!head data/sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(row):\n",
    "    (id, date, store, state, product, amount) = row\n",
    "    return (int(id), date, int(store), state, int(product), float(amount))\n",
    "\n",
    "rdd_sales = (\n",
    "    sc.textFile('data/sales.csv')\n",
    "        .map(lambda rowstr : rowstr.split(\",\"))\n",
    "        .filter(lambda row: not row[0].startswith('#'))\n",
    "        .map(casting_function)\n",
    "            )\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "| id|      date|store|state|product|amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- product: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the many data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# create a schema of your own\n",
    "schema = StructType( [\n",
    "    StructField('id',     IntegerType(), True),\n",
    "    StructField('date',   StringType(),  True),\n",
    "    StructField('store',  IntegerType(), True),\n",
    "    StructField('state',  StringType(),  True),\n",
    "    StructField('product',IntegerType(), True),\n",
    "    StructField('amount', FloatType(),   True) ] )\n",
    "\n",
    "# feed that into a DataFrame\n",
    "df = spark.createDataFrame(rdd_sales,schema)\n",
    "\n",
    "# show the result\n",
    "df.show()\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- from pyspark.sql.functions import unix_timestamp\n",
    "df_conv=df.withColumn(\"date\",unix_timestamp(\"date\", 'MM/dd/yyyy').cast(TimestampType()))\n",
    "df_conv.printSchema()\n",
    "df_conv.show(5)\n",
    "# df_conv.show() -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Reading from files (inferring schema)\n",
    "\n",
    "Use [`spark.read.csv`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv) to load a CSV into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df = spark.read.csv('data/sales.csv',\n",
    "                    header=True,       # use headers or not\n",
    "                    sep=\",\",           # char for separation\n",
    "                    inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(f'line count: {df.count()}')\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`spark.read.json`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json) to load a JSON file into a DataFrame. You can specify every useful parameter in there. It can infer the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the json file look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
      "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
      "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
      "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
      "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
      "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
     ]
    }
   ],
   "source": [
    "!head data/sales.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- product: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- store: long (nullable = true)\n",
      "\n",
      "line count: 6\n",
      "\n",
      "+------+----------+---+-------+-----+-----+\n",
      "|amount|      date| id|product|state|store|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "| 300.0|11/13/2014|101|    331|   WA|  100|\n",
      "| 450.0|11/18/2014|104|    329|   OR|  700|\n",
      "| 200.0|11/15/2014|102|    321|   CA|  203|\n",
      "| 330.0|11/19/2014|106|    331|   CA|  202|\n",
      "| 750.0|11/17/2014|103|    373|   WA|  101|\n",
      "| 200.0|11/19/2014|105|    321|   CA|  202|\n",
      "+------+----------+---+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read JSON\n",
    "df = spark.read.json('data/sales.json')\n",
    "\n",
    "# prints the schema\n",
    "df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\\n\".format(df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount=300.0, date='11/13/2014', id=101, product=331, state='WA', store=100),\n",
       " Row(amount=450.0, date='11/18/2014', id=104, product=329, state='OR', store=700),\n",
       " Row(amount=200.0, date='11/15/2014', id=102, product=321, state='CA', store=203),\n",
       " Row(amount=330.0, date='11/19/2014', id=106, product=331, state='CA', store=202),\n",
       " Row(amount=750.0, date='11/17/2014', id=103, product=373, state='WA', store=101),\n",
       " Row(amount=200.0, date='11/19/2014', id=105, product=321, state='CA', store=202)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Actions : turning your DataFrame into a local object\n",
    "\n",
    "Some actions just remain the same, you won't have to learn Spark all over again.\n",
    "\n",
    "Some new actions give you the possibility to describe and show the content in a more fashionable manner.\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | DF vs RDD? | Description |\n",
    "| - | - | - |\n",
    "| [`.collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) | identical | Return a list that contains all of the elements as Rows. |\n",
    "| [`.count()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) | identical | Return the number of elements. |\n",
    "| [`.take(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) | identical | Take the first `n` elements. |\n",
    "| [`.top(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.top) | identical | Get the top `n` elements. |\n",
    "| [`.first()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first) | identical | Return the first element. |\n",
    "| [`.show(n)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) | <span style=\"color:green\">new</span> | Show the DataFrame in table format (`n=20` by default) |\n",
    "| [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) | <span style=\"color:green\">new</span> | Convert the DF into a Pandas DF. |\n",
    "| [`.printSchema(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema)\\* | <span style=\"color:green\">new</span> | Display the schema. This is not an action, it doesn't launch the DAG, but it fits better in this category. |\n",
    "| [`.describe(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) | <span style=\"color:green\">new</span> | Compute statistics for this column. |\n",
    "| [`.sum(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.mean(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.mean) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.min(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.min) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n",
    "| [`.max(*cols)`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max) | <span style=\"color:red\">different</span> | Applies on GroupedData only (see transformations). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|amount|\n",
      "+----------+------+\n",
      "|11/13/2014| 300.0|\n",
      "|11/18/2014| 450.0|\n",
      "|11/15/2014| 200.0|\n",
      "|11/19/2014| 330.0|\n",
      "|11/17/2014| 750.0|\n",
      "|11/19/2014| 200.0|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['date','amount']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Store</th>\n",
       "      <th>State</th>\n",
       "      <th>Product</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>11/13/2014</td>\n",
       "      <td>100</td>\n",
       "      <td>WA</td>\n",
       "      <td>331</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>11/18/2014</td>\n",
       "      <td>700</td>\n",
       "      <td>OR</td>\n",
       "      <td>329</td>\n",
       "      <td>450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>11/15/2014</td>\n",
       "      <td>203</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>331</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "      <td>11/17/2014</td>\n",
       "      <td>101</td>\n",
       "      <td>WA</td>\n",
       "      <td>373</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>11/19/2014</td>\n",
       "      <td>202</td>\n",
       "      <td>CA</td>\n",
       "      <td>321</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #ID        Date  Store State  Product  Amount\n",
       "0  101  11/13/2014    100    WA      331   300.0\n",
       "1  104  11/18/2014    700    OR      329   450.0\n",
       "2  102  11/15/2014    203    CA      321   200.0\n",
       "3  106  11/19/2014    202    CA      331   330.0\n",
       "4  103  11/17/2014    101    WA      373   750.0\n",
       "5  105  11/19/2014    202    CA      321   200.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how `.collect()` returns things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(#ID=101, Date='11/13/2014', Store=100, State='WA', Product=331, Amount=300.0),\n",
       " Row(#ID=104, Date='11/18/2014', Store=700, State='OR', Product=329, Amount=450.0),\n",
       " Row(#ID=102, Date='11/15/2014', Store=203, State='CA', Product=321, Amount=200.0),\n",
       " Row(#ID=106, Date='11/19/2014', Store=202, State='CA', Product=331, Amount=330.0),\n",
       " Row(#ID=103, Date='11/17/2014', Store=101, State='WA', Product=373, Amount=750.0),\n",
       " Row(#ID=105, Date='11/19/2014', Store=202, State='CA', Product=321, Amount=200.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- printSchema()\n",
      "root\n",
      " |-- #ID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Product: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n",
      "--- show()\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n",
      "--- describe()\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|summary|               #ID|      Date|             Store|State|           Product|            Amount|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "|  count|                 6|         6|                 6|    6|                 6|                 6|\n",
      "|   mean|             103.5|      null|251.33333333333334| null| 334.3333333333333| 371.6666666666667|\n",
      "| stddev|1.8708286933869716|      null|225.39180700874346| null|19.500427345744672|207.40459654179958|\n",
      "|    min|               101|11/13/2014|               100|   CA|               321|             200.0|\n",
      "|    max|               106|11/19/2014|               700|   WA|               373|             750.0|\n",
      "+-------+------------------+----------+------------------+-----+------------------+------------------+\n",
      "\n",
      "--- describe(Amount)\n",
      "+-------+------------------+\n",
      "|summary|            Amount|\n",
      "+-------+------------------+\n",
      "|  count|                 6|\n",
      "|   mean| 371.6666666666667|\n",
      "| stddev|207.40459654179958|\n",
      "|    min|             200.0|\n",
      "|    max|             750.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema\n",
    "print(\"--- printSchema()\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "# prints the table itself\n",
    "print(\"--- show()\")\n",
    "df_sales.show()\n",
    "\n",
    "# show the statistics of all numerical columns\n",
    "print(\"--- describe()\")\n",
    "df_sales.describe().show()\n",
    "\n",
    "# show the statistics of one specific column\n",
    "print(\"--- describe(Amount)\")\n",
    "df_sales.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformations on DataFrames\n",
    "\n",
    "- They are still **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**\n",
    "- They transform a DataFrame into another because DataFrames are also **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "You got that... DataFrames are just RDDs with a schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. selecting and adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.select(*cols)` : selecting specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|      Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)\n",
    "\n",
    "df_aapl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_aapl.select(\"Open\", \"Close\")\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square-Bracket notation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      Open|     Close|\n",
      "+----------+----------+\n",
      "|117.949997|    118.25|\n",
      "|117.099998|117.650002|\n",
      "|116.809998|116.599998|\n",
      "|116.860001|117.059998|\n",
      "|    117.25|117.120003|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl[[\"Open\",\"Close\"]].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Column\n",
    "\n",
    "### Adding a column of constant values\n",
    "\n",
    "#### `.withColumn(\"label\", func)` : constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|      Open|      High|blabla|\n",
      "+----------+----------+------+\n",
      "|117.949997|118.360001|    34|\n",
      "|117.099998|117.739998|    34|\n",
      "|116.809998|116.910004|    34|\n",
      "|116.860001|117.379997|    34|\n",
      "|    117.25|117.760002|    34|\n",
      "+----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "## Creates a :class:`Column` of literal value.\n",
    "\n",
    "df_out = df_aapl.withColumn(\"blabla\", lit(34))\n",
    "\n",
    "df_out[['Open','High','blabla']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Column of calculated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caculated from other columns\n",
    "\n",
    "#### `.withColumn(\"label\", func)` : column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+\n",
      "|      Date|      High|       Low|              diff|\n",
      "+----------+----------+----------+------------------+\n",
      "|2016-10-25|118.360001|117.309998|1.0500030000000038|\n",
      "|2016-10-24|117.739998|     117.0|0.7399979999999999|\n",
      "|2016-10-21|116.910004|116.279999| 0.630004999999997|\n",
      "|2016-10-20|117.379997|116.330002|1.0499950000000098|\n",
      "|2016-10-19|117.760002|113.800003|3.9599989999999963|\n",
      "+----------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = (df_aapl\n",
    "            .withColumn(\"diff\", \n",
    "                         df_aapl['High'] - df_aapl['Low'])\n",
    "            .select('Date', 'High', 'Low', 'diff')\n",
    "         )\n",
    "\n",
    "df_out.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column from a User Defined Function (udf)\n",
    "\n",
    "#### `.withColumn(\"label\", func)` : user defined function\n",
    "\n",
    "udf is a mechanism to extend the functionality of the database system. \n",
    "\n",
    "`udf()` turns a normal python function into something Spark can *parallelize* across its distributed data. \n",
    "\n",
    "`udf()` requires two arguments: \n",
    "* a function\n",
    "* the data type the function will return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|      High|       Low|      Open|     Close|   Special|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|118.360001|117.309998|117.949997|    118.25|0.77785903|\n",
      "|117.739998|     117.0|117.099998|117.650002|   0.42694|\n",
      "|116.910004|116.279999|116.809998|116.599998|0.77722335|\n",
      "|117.379997|116.330002|116.860001|117.059998|0.85966575|\n",
      "|117.760002|113.800003|    117.25|117.120003| 4.5097456|\n",
      "|118.209999|117.449997|    118.18|117.470001| 1.5458359|\n",
      "|117.839996|116.779999|117.330002|117.550003|0.85066664|\n",
      "|118.169998|117.129997|117.879997|117.629997| 1.3353877|\n",
      "|117.440002|115.720001|116.790001|116.980003| 1.4223677|\n",
      "|117.980003|    116.75|117.349998|117.339996| 1.2423673|\n",
      "+----------+----------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "#step 1: create your function\n",
    "def my_specialfunc(h,l,o,c):\n",
    "    '''\n",
    "    Do some math to four of our columns... \n",
    "    returning a new column of data... probably should be a float\n",
    "    '''\n",
    "    return ((h-l)*(math.exp(o-c)))\n",
    "\n",
    "#step 2: register that function in the spark session\n",
    "my_specialfunc_udf = udf(my_specialfunc, FloatType())\n",
    "\n",
    "df_out = df_aapl.withColumn(\"Special\", my_specialfunc_udf(df_aapl['High'], \n",
    "                                                          df_aapl['Low'], \n",
    "                                                          df_aapl['Open'], \n",
    "                                                          df_aapl['Close']))\n",
    "\n",
    "df_out.select('High', 'Low', 'Open', 'Close', 'Special').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. aggregating and sorting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupBy()`: aggregating in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------------+\n",
      "|State|sum(Amount)|     avg(Product)|\n",
      "+-----+-----------+-----------------+\n",
      "|   OR|      450.0|            329.0|\n",
      "|   CA|      730.0|324.3333333333333|\n",
      "|   WA|     1050.0|            352.0|\n",
      "+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_out = (df_sales\n",
    "          .groupBy(\"State\")\n",
    "          .agg(F.sum(\"Amount\"), \n",
    "               F.avg('Product')\n",
    "              )\n",
    "         )\n",
    "\n",
    "df_out.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   OR|      450.0|\n",
      "|   CA|      730.0|\n",
      "|   WA|     1050.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = (df_sales\n",
    "     .groupBy(\"State\")\n",
    "     .sum(\"Amount\")\n",
    "    )\n",
    "\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.orderBy()` : sorting by a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|sum(Amount)|\n",
      "+-----+-----------+\n",
      "|   WA|     1050.0|\n",
      "|   CA|      730.0|\n",
      "|   OR|      450.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = (df_sales\n",
    "          .groupBy(\"State\")\n",
    "          .agg(F.sum(\"Amount\")\n",
    "          )\n",
    "          .orderBy(\"sum(Amount)\", ascending=False)\n",
    "         )\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's design chains of transformations together !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_sales = spark.read.csv('data/sales.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a DataFrame of the states and their average sales, sorted from highest to lowest on average. \n",
    "\n",
    "What transformations do you need to apply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- (df_out.groupBy('State')\n",
    "       .agg(F.avg('Amount').alias('average'))\n",
    "       .orderBy('average', ascending = False)\n",
    "       .select('State', F.round('average', 2))\n",
    ").show()\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Find the date on which AAPL's stock price was the highest\n",
    "\n",
    "### Input DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|      Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV\n",
    "df_aapl = spark.read.csv('data/aapl.csv',\n",
    "                         header=True,       # use headers or not\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "\n",
    "1. keep only fields for Date and Close \n",
    "4. order by Close in descending order\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|      Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code heree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- df_out = (df_aapl[['Date', 'Close']]\n",
    "          .orderBy('Close', ascending=False)\n",
    "         )\n",
    "\n",
    "df_out.show(5) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are you tired of chaining operations together?\n",
    "\n",
    "## If only there were a simple query languge that let us spell it out in (nearly) plain text\n",
    "\n",
    "\n",
    "# 4. The SQL Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know you missed it. Let's run some SQL queries on these tables!\n",
    "\n",
    "First we tell spark to create \"SQL namespace\" and assign a name to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aapl.createOrReplaceTempView('aapl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write queries using `spark.sql`, and it will have access to any table we have registered like above. The output of the query is another spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|      Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * from aapl LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: In order to refer to `aapl` as if it were a table in a SQL database, we have to use `createOrReplaceTempView`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------------------+\n",
      "|      Date|      Open|     Close|                diff|\n",
      "+----------+----------+----------+--------------------+\n",
      "|2016-10-25|117.949997|    118.25|  0.3000030000000038|\n",
      "|2016-10-24|117.099998|117.650002|  0.5500040000000013|\n",
      "|2016-10-21|116.809998|116.599998|-0.20999999999999375|\n",
      "+----------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql('''SELECT \n",
    "                       Date, \n",
    "                       Open, \n",
    "                       Close, \n",
    "                       Close - Open as diff \n",
    "\n",
    "                   FROM aapl \n",
    "                   \n",
    "                   LIMIT 3''')\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|      Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "|2016-10-18|    118.18|118.209999|117.449997|117.470001|24553500|117.470001|\n",
      "|2016-10-17|117.330002|117.839996|116.779999|117.550003|23624900|117.550003|\n",
      "|2016-10-14|117.879997|118.169998|117.129997|117.629997|35652200|117.629997|\n",
      "|2016-10-13|116.790001|117.440002|115.720001|116.980003|35192400|116.980003|\n",
      "|2016-10-12|117.349998|117.980003|    116.75|117.339996|37586800|117.339996|\n",
      "|2016-10-11|117.699997|118.690002|116.199997|116.300003|64041000|116.300003|\n",
      "|2016-10-10|115.019997|    116.75|114.720001|116.050003|36236000|116.050003|\n",
      "|2016-10-07|114.309998|114.559998|113.510002|114.059998|24358400|114.059998|\n",
      "|2016-10-06|113.699997|114.339996|113.129997|113.889999|28779300|113.889999|\n",
      "|2016-10-05|113.400002|113.660004|112.690002|113.050003|21453100|113.050003|\n",
      "|2016-10-04|113.059998|114.309998|112.629997|     113.0|29736800|     113.0|\n",
      "|2016-10-03|112.709999|113.050003|112.279999|112.519997|21701800|112.519997|\n",
      "|2016-09-30|112.459999|113.370003|111.800003|113.050003|36379100|113.050003|\n",
      "|2016-09-29|113.160004|113.800003|111.800003|    112.18|35887000|    112.18|\n",
      "|2016-09-28|113.690002|114.639999|    113.43|113.949997|29641100|113.949997|\n",
      "+----------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aapl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Now, design a query that would:\n",
    "\n",
    "1. keep only fields for Date and Close \n",
    "4. order by Close in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''' Your code here '''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You want to obtain a DataFrame of the states and their average sales, sorted from highest to lowest on average. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+-----+-------+------+\n",
      "|#ID|      Date|Store|State|Product|Amount|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "|101|11/13/2014|  100|   WA|    331| 300.0|\n",
      "|104|11/18/2014|  700|   OR|    329| 450.0|\n",
      "|102|11/15/2014|  203|   CA|    321| 200.0|\n",
      "|106|11/19/2014|  202|   CA|    331| 330.0|\n",
      "|103|11/17/2014|  101|   WA|    373| 750.0|\n",
      "|105|11/19/2014|  202|   CA|    321| 200.0|\n",
      "+---+----------+-----+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|state| total|\n",
      "+-----+------+\n",
      "|   WA| 525.0|\n",
      "|   OR| 450.0|\n",
      "|   CA|243.33|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''Your Code Here'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- query = '''SELECT \n",
    "                state, \n",
    "                ROUND(AVG(Amount), 2) as total \n",
    "            FROM sales \n",
    "            GROUP BY State \n",
    "            ORDER BY total DESC'''\n",
    "\n",
    "spark.sql(query).show() -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
