{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Based on Anne Barry, Dan Rupp and Jack Bennetto's lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "Today I will be successful if I can... \n",
    "\n",
    " * Describe the three kinds of model error\n",
    " * Identify what types of models have high bias and high variance\n",
    " * Name two different Error Metrics and calculate MSE\n",
    " * Explain k-fold cross validation\n",
    " * Explain the training, validation, testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "In this lesson we will talk about\n",
    "\n",
    "* A simple example\n",
    "* Bias and Variance\n",
    "* Train-test split\n",
    "* K-fold cross validation\n",
    "\n",
    "## Big Questions:\n",
    "\n",
    "What common pitfalls do we fall into when training models?\n",
    "\n",
    "How do we make sure our models will predict on unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/800px-CRISP-DM_Process_Diagram.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "[CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) is the Cross-industry standard process for data mining.  The diagram above illustrates the lifecycle of a data science project.  Today we will focus on evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering Bias and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model</th>\n",
       "      <th>origin</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  acceleration  model  \\\n",
       "0  18.0          8         307.0      130.0  3504.0          12.0     70   \n",
       "1  15.0          8         350.0      165.0  3693.0          11.5     70   \n",
       "2  18.0          8         318.0      150.0  3436.0          11.0     70   \n",
       "3  16.0          8         304.0      150.0  3433.0          12.0     70   \n",
       "4  17.0          8         302.0      140.0  3449.0          10.5     70   \n",
       "\n",
       "   origin                   car_name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/cars.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv', na_values=['?'])\n",
    "cars = cars[cars.horsepower.notnull()]\n",
    "\n",
    "X = cars.horsepower.values\n",
    "y = cars.mpg.values\n",
    "\n",
    "fig, axs = plt.subplots(3,2,figsize=(14, 14))\n",
    "x_tick = np.linspace(50,225,150)\n",
    "k_num = [1,3,5,10,15,50]\n",
    "\n",
    "for k, ax in zip(k_num,axs.flatten()):\n",
    "    model = KNeighborsRegressor(k)\n",
    "    model.fit(X.reshape(-1,1),y.reshape(-1,1))\n",
    "    ax.scatter(X,y,c='blue')\n",
    "    ax.plot(x_tick, model.predict(x_tick.reshape(-1,1)), linewidth=4)\n",
    "    ax.set_xlabel('Horse Power')\n",
    "    ax.set_ylabel('MPG')\n",
    "    ax.set_title('Horse Power vs MPG with K={}'.format(k))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "* What does the red line mean?\n",
    "\n",
    "* What happens to the red line as k increases?\n",
    "  \n",
    "* How would these predictions change if I used a different sample of cars from the same population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_samples_given_k(axs, cars, k):\n",
    "    \n",
    "    x_tick = np.linspace(50,225,150)\n",
    "\n",
    "    for samp, ax in enumerate(axs.flatten()):\n",
    "        #each different sample of 100 values from cars\n",
    "        c = cars.sample(100)\n",
    "        X = c.horsepower.values\n",
    "        y = c.mpg.values\n",
    "        model = KNeighborsRegressor(k)\n",
    "        model.fit(X.reshape(-1,1),y.reshape(-1,1))\n",
    "        ax.scatter(X,y,c='blue')\n",
    "        ax.plot(x_tick, model.predict(x_tick.reshape(-1,1)), linewidth=4)\n",
    "        ax.set_xlabel('Horse Power')\n",
    "        ax.set_ylabel('MPG')\n",
    "        ax.set_title('Sample {} Horse Power vs MPG with K={}'.format(samp,k))\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training various samples When K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(14, 14))\n",
    "plt_samples_given_k(axs, cars, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training various samples When K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(14, 14))\n",
    "plt_samples_given_k(axs, cars, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training various samples When K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(14, 14))\n",
    "plt_samples_given_k(axs, cars, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training various samples When K = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(14, 14))\n",
    "plt_samples_given_k(axs, cars, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for each value of k:\n",
    "* What happens to the predictions as the sample changes?\n",
    "* How well do you think this model predicts on unknown data?\n",
    "* Which model is most sensitive to changes in the sample group?\n",
    "* As k approaches n (the number of observations in the sample), what will the red line look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Details of Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance: \n",
    "\n",
    "A model with **high variance** is **overfit** to the training data and doesn't generalize well to other data. This happens when we have a very complicated model with many independent features, making the model sensitive to small fluctuations in the training set.\n",
    "\n",
    "*Measurement*: \n",
    "\n",
    "\n",
    "the average variance of predicted values from a single model trained on different samples of a training set \n",
    "<!-- (from model to model, using different samples, how much do the predicted values vary from eachother?)  -->\n",
    "\n",
    "Which model demonstrates this? Meaning which k value demonstrates this?\n",
    "\n",
    "\n",
    "### Bias:\n",
    "A model with **high bias** is **underfit** and individual predictions deviate greatly from their actual values. Suppose you had a car and wanted to predict its MPG but also you are lazy so you used the mean MPG from the training set.  That's not great. You're not paying good attention to the data in the training sample.\n",
    "\n",
    "*Measurement*: the average of $|\\hat y- y|$ from a single model trained on different samples of a training set\n",
    "\n",
    "<!-- The distance from the prediction to the actual value averaged over different samples using a single model -->\n",
    "\n",
    "Note: If we predict well on the training set well we will generally have low bias.\n",
    "\n",
    "Which model demonstrates this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three sources of error: Bias, variance, and irreducible error\n",
    "\n",
    "We'll come back to these again and again and again through the course. The mean-squared error of a model can be decomposed into three components. (We will spend more time on  MSE soon) \n",
    "\n",
    "<img src=\"img/bias_var.png\">\n",
    "\n",
    "\n",
    "Note that we don't ever actually know the values of these components, but they are useful in understanding how we make the best possible prediction.\n",
    "\n",
    "1. The **irreducible error/noise** is the error inherent in any value. Even if we had all possible data (the \"population\") and could build the best possible model, we can't predict values exactly.\n",
    "2. The **bias**\n",
    "3. The **variance**\n",
    "\n",
    "Again, if we can't know these values, why do we care?\n",
    "\n",
    "In general, there is a trade-off between bias and variance. A complex model might have very low bias, but will be highly dependent on the sample taken so will have high variance. A simple model might have higher bias, because it underfits, but lower variance, predicting other data nearly as well as the training sample.\n",
    "\n",
    "Some models have **hyperparameters** that can be tuned. Most represent that trade-off: moving them in one direction will lower the bias and raise the variance; moving them in the other will do the opposite.\n",
    "\n",
    "Every time you learn about a hyperparameter you should ask yourself if it represents a bias-variance trade-off, and which direction increases bias and decreases variance, and which direction does the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "You want to build a model to predict peoples heights. \n",
    "\n",
    "- 1st Model: predicts a person's height based on social security number\n",
    "\n",
    "- 2nd Model: Predicts a person's height using the mean height of the training sample. \n",
    "\n",
    "Which model exhibits high variance and which shows high bias? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math\n",
    "$\\DeclareMathOperator{\\E}{\\mathbb{E}}$\n",
    "$\\DeclareMathOperator{\\Bias}{Bias}$\n",
    "$\\DeclareMathOperator{\\Var}{Var}$\n",
    "\n",
    "Suppose there is some underlying true function $f(x)$, such that $y = f(x) + \\epsilon$, where the noise $\\epsilon$ has a mean of zero and standard deviation of $\\sigma$. We've created some function $\\hat f(x)$ to estimate $y$, and want $|y - \\hat f(x)|$ to be as small as possible. We can decompose this into three parts: the  square of the **bias**, the **variance**, and the **irreducible error**.\n",
    "\n",
    "$$\\E\\left[(y - \\hat f(x))^2\\right] = \\left(\\Bias[\\hat f(x)]\\right)^2 + \\Var[\\hat f(x)] + \\sigma^2$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to go through the derivation; you can find in elsewhere (e.g. [wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation)). Instead I'll cover we'll look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bias_var.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance example\n",
    "\n",
    "Let's see what that looks like with some fake data. Our true function $f$ will be a cosine curve; we'll add some noise to give values for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def y(x):\n",
    "    return f(x) + stats.norm(0, sigma).rvs(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = (0, 10)\n",
    "n_pts = 100\n",
    "xpts = np.linspace(*xlim, n_pts)\n",
    "#Generate y values without noise\n",
    "ypts = f(xpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(xpts, ypts, 'b-', label='$f(x)$')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll choose a training sample and fit a model to those data. We'll use a kNN regressor, since that's a simple model that we've already learned. We'll start with `k=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "sample = stats.uniform(xlim[0], xlim[1]-xlim[0]).rvs(sample_size)\n",
    "#generate our y data from our uniform distribution of x values\n",
    "y_sample = y(sample)\n",
    "\n",
    "#Create and fit our x sample data and our y values \n",
    "#y values calculated from the function above including noise\n",
    "model = KNeighborsRegressor(3)\n",
    "model.fit(sample[:, None], y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ax.plot(sample, y_sample, 'ro', label='$y$')\n",
    "ax.plot(xpts, model.predict(xpts[:, None]), 'r-', lw=0.8, label='$\\hat{f}(x)$')\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0].remove()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to make sense of bias and variance, we need to consider a large number of training samples, and build a model for each. In reality we only have our one training sample, but we're doing this to try to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(18,10), sharey='row', sharex='all')\n",
    "\n",
    "n_trials = 100\n",
    "sample_size = 50\n",
    "for i, k in enumerate((1, 3, 11, 25)):\n",
    "    predictions = np.zeros((n_trials, n_pts))\n",
    "    for t in range(n_trials):\n",
    "        sample = stats.uniform(xlim[0], xlim[1]-xlim[0]).rvs(sample_size)\n",
    "        y_sample = y(sample)\n",
    "        model = KNeighborsRegressor(k)\n",
    "        model.fit(sample[:, None], y_sample)\n",
    "        predictions[t, :] = model.predict(xpts[:, None])\n",
    "        ax[0, i].plot(xpts, predictions[t,:], 'r-', lw=0.1)\n",
    "    ax[0, i].plot(xpts, ypts, 'b-', label='$f(x)$')\n",
    "    ax[0, i].plot(xpts, predictions.mean(axis=0), color='#AA0000', lw=3, label=\"$E[\\hat f(x)]$\")\n",
    "\n",
    "    ax[0, i].set_ylim((-2, 2))\n",
    "    ax[0, i].set_title(f\"k = {k}\")\n",
    "    ax[0, i].legend()\n",
    "    #ax[1].plot(xpts, predictions.mean(axis=0) - ypts, 'k', label='bias')\n",
    "    bias = predictions.mean(axis=0) - ypts\n",
    "    variance = predictions.var(axis=0)\n",
    "    ax[1, i].plot(xpts, bias**2, 'k', label='bias^2')\n",
    "    ax[1, i].plot(xpts, variance, 'r', label='variance')\n",
    "    ax[1, i].plot(xpts, bias**2 + variance, 'g', label='total')\n",
    "    ax[1, i].plot(xpts, np.zeros_like(xpts), 'k', lw=0.5)\n",
    "\n",
    "    ax[1, i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so how do we tell which model is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're stuck with the bias-variance trade off\n",
    "\n",
    "We can not directly compute the bias and variance of a real model.  So how is the 'correct' model complexity chosen?   \n",
    "<br/>\n",
    "Model complexity can be the order of the fit, the number of features, interaction of the features, number of splits (decision tree), number of neurons/layers of a neural net, number of layers....  \n",
    "<br/>\n",
    "We can't do anything to reduce the sampling error from the population, but can we find the model complexity that minimizes the sum of the bias and variance?\n",
    "\n",
    "\n",
    "#### An oversimplified way to consider it...\n",
    "\n",
    "<img src=\"img/bias_variance_meme.jpeg\" alt=\"Drawing\" style=\"width: 320px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I can't calculate the total error for a real world model, than how can I determine which model is best? \n",
    "\n",
    "### The solution: CROSS VALIDATION!\n",
    "\n",
    "<img src=\"img/800px-CRISP-DM_Process_Diagram.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Cross validation really has two separate purposes.\n",
    "\n",
    "- First, it's used to **evaluate your model**. Part of the [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) is evaluation; you (usually) need to know how well your model will predict real-world results. There are many ways to measure that, like mean-square-error or mean-absolute-error for regression models, or log loss or brier score or AUC/ROC or F-score some combination of precision and recall or sensitivity and specificity, based on your specific business case, but the key problem is that you can't measure it on your training data.\n",
    "\n",
    "- Second, it is used for **model comparison**. Over the coming week we'll learn a bunch of different models, and we need to evaluate which will do best for our data. In addition, many of these models have hyperparameters, and we need cross validation to choose the appropriate values, i.e., tuning the hyperparameters. \n",
    "\n",
    "\n",
    "Note: You can measure on your training data in some circumstances, either because your statistical measure allows some estimation of the error, or you have an ensemble model where different submodels see different data (out-of-bag error). But those aren't as general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation:\n",
    " - \"Cross-validation: is a **model validation technique** for assessing how the results of a statistical analysis will generalize to an independent data set.\" -Wikipedia   \n",
    "    \n",
    "We use cross-validation when:\n",
    " 1. Attempting to quantify how well a model (of some given complexity) will predict on unseen data\n",
    " 2. Tuning hyperparameters of models to get best predictions\n",
    " 3. Choosing between types of models \n",
    "\n",
    "<img src=\"img/cross_val_illustrated.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The train-test split\n",
    "\n",
    "\n",
    "<img src=\"img/types_cross_val.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "The simplest approach we can use is the train-test split. You shouldn't call this cross validation, just say \"train-test split\" or \"hold-out validation.\"\n",
    "\n",
    "Let's start with the cars dataset that you may have seen before.    \n",
    "    \n",
    "To import train test split: \n",
    "\n",
    "`from sklearn.model_selection import train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few rows without data for horsepower, which is why it shows up as an object. We're just going to throw those away for now without worrying too much if that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower.astype('float128')\n",
    "cars.mpg = cars.mpg.astype('float128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ax.plot(cars.horsepower, cars.mpg, '.')\n",
    "ax.set_title('Horse Power vs MPG')\n",
    "ax.set_xlabel(\"horsepower\")\n",
    "ax.set_ylabel(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create my feature matrix and my targets array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[['horsepower']]\n",
    "y = cars.mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,6))\n",
    "ax.scatter(X_train,y_train, label='Training Data')\n",
    "ax.scatter(X_test,y_test, label='Testing Data')\n",
    "ax.set_title('Horse Power vs MPG split')\n",
    "ax.set_xlabel(\"horsepower\")\n",
    "ax.set_ylabel(\"mpg\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyperparameters and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(20)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "\n",
    "There is a variety of evaluation metrics and depending on your model, weather or not you are doing regression or classification and other complicating factures due to something like class imballance, you will choose different evaluation metrics. \n",
    "\n",
    "Checkout the 3 most common Regression metrics [here](https://towardsdatascience.com/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b)\n",
    "\n",
    "\n",
    "### One Common Regression Evaluation: MSE\n",
    "\n",
    "How can we measure the error? A typical choice is to use mean squared error. **The lower the MSE, the better.** \n",
    "$$\n",
    "MSE := \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2\n",
    "$$\n",
    "\n",
    "** one reason I love MSE is because the name literally tells you exactly how to calculate it. Take the Mean of the Square of the Error. The error for a given data point is the difference between the observed/actual value and the predicted value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_hat, y_test):\n",
    "    '''\n",
    "    y_hat: predicted y values from x_test \n",
    "    y_test: actual y values from x_test\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Common Regression Evaluation: R-Squared\n",
    "\n",
    "Mean squared error is a good error metric, however the result you are looking for is dependent on the data set. For a more general result there is a scaled version called $R^2$. \n",
    "\\begin{align}\n",
    "    R^2 &:= 1 - \\frac{SS_{res}}{SS_{tot}} \\\\\n",
    "    &= 1 - \\frac{\\sum_{i=1}^n (\\hat{y_i} - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "\\end{align}    \n",
    "\n",
    "Where $SS_{res}$ is the sum of the squared residuals and $SS_{tot}$ is the total sum of squares. $R^2$ can be interpreted as the fraction of the variance in the data that is explained by the model.\n",
    "\n",
    "$R^2$ will be between 0 and 1. 0 means that your model explains none of the variance in the data, while 1 means your model explains all of the variance in the data. **The higher $R^2$, the better!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that model.score will automatically score the model for you using R^2 (not all models but this is KNN's .score method)\n",
    "\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))\n",
    "    \n",
    "    \n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Let's also take a look at our MSE for the sake of it\n",
    "MSE(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we did a bit better on the training data, as expected...or did we? Let's try a different random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resplit your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)\n",
    "\n",
    "#again, train on just the \"training\" set\n",
    "model = KNeighborsRegressor(20)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "# Let's also take a look at our MSE for the sake of it\n",
    "MSE(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting a lower $k$ but use the same split from the cell above\n",
    "\n",
    "What should this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"horsepower\")\n",
    "ax.set_ylabel(\"mpg\")\n",
    "ax.plot(X_train, y_train, 'r.')\n",
    "ax.plot(X_test, y_test, 'b.')\n",
    "\n",
    "\n",
    "model = KNeighborsRegressor(15)\n",
    "model.fit(X_train, y_train)\n",
    "xpts = np.linspace(50, 220, 100).reshape(-1, 1)\n",
    "ax.plot(xpts, model.predict(xpts), 'k-')\n",
    "\n",
    "print(\"R^2 on training data: {}\".format(model.score(X_train, y_train)))\n",
    "print(\"R^2 on testing data:  {}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another MSE function that can take a model as a parameter\n",
    "def mean_squared_error(model, X, y):\n",
    "    return np.mean((model.predict(X) - y) **2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a graph you've probably seen showing the error in the training and test sets for different complexities. Let's see if we can reproduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for k in range(1, 11):\n",
    "    model = KNeighborsRegressor(k)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "    test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "    #train_score.append(-model.score(X_train, y_train))\n",
    "    #test_score.append(-model.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(1, 11), train_score, '.-r', label=\"train set\")\n",
    "ax.plot(range(1, 11), test_score, '.-b', label=\"test set\")\n",
    "ax.set_xlabel('complexity (value of k)')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! So we do better with the training set than the test set for any complexity. We have a minimum value of around 7-8. Let's try some many more splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for k in range(1, 11):\n",
    "        model = KNeighborsRegressor(k)\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score.append(mean_squared_error(model, X_train, y_train))\n",
    "        test_score.append(mean_squared_error(model, X_test, y_test))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('complexity (value of k)')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.set_ylim(5, 60)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this isn't all that consistent. We need something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETTER: Use K-fold Cross Validation\n",
    "\n",
    "*Do not confuse the new k variable in k-fold cross validation with the k hyperparameter in KNN.\n",
    "\n",
    "<img src=\"img/types_cross_val.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "With Cross Validation, we randomly partition the data into $k$ groups, $D_1$, $D_2$, ..., $D_k$. For each $i \\in [1..k]$ we:\n",
    "\n",
    " * Build a model using $D_{j \\ne i}$ as a training data\n",
    " * Calculate the error of the model on $D_i$\n",
    " \n",
    "We average all these errors to compute the overall error of the model. We can either compare those  across different models to choose the best model or use that number to report the actual error of our one model.\n",
    "\n",
    "There isn't a clear \"best\" value for $k$(Number of Folds). The extreme version of k-fold cross validation, when $k=n$, is called leave-one-out cross validation. That's generally not so good, and $k=2$ is not so good, but experience has shown that other choices are fine. I like $k=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So suppose we have fifteen data points (labeled a...o) and we're doing 5-fold cross validation. We'll put the 1st three in the first fold, the next three in the next fold, and so on.\n",
    "\n",
    "We would build 5 different models (one for each split column, below) using the \"train\" data points, test each on the \"TEST\" data points, and average the results.\n",
    "\n",
    "data point | fold | 1st split  | 2nd split | 3rd split | 4th split | 5th split\n",
    "---|---|---|---|---|---|---\n",
    " a | 1 | **test** | train | train | train | train | \n",
    " b | 1 | **test** | train | train | train | train | \n",
    " c | 1 | **test** | train | train | train | train | \n",
    " d | 2 | train | **test** | train | train | train | \n",
    " e | 2 | train | **test** | train | train | train | \n",
    " f | 2 | train | **test** | train | train | train | \n",
    " g | 3 | train | train | **test** | train | train | \n",
    " h | 3 | train | train | **test** | train | train | \n",
    " i | 3 | train | train | **test** | train | train | \n",
    " j | 4 | train | train | train | **test** | train | \n",
    " k | 4 | train | train | train | **test** | train | \n",
    " l | 4 | train | train | train | **test** | train | \n",
    " m | 5 | train | train | train | train | **test** | \n",
    " n | 5 | train | train | train | train | **test** | \n",
    " o | 5 | train | train | train | train | **test** | \n",
    "\n",
    "**Question:** Generally we shuffle the data points first. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could write code to do this, but `sklearn` has already done that. One confusing thing about the `KFold` object in `sklearn` is that it returns indices, not the data themselves. Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#use the sklearn KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)  # almost always use shuffle=True\n",
    "#Create a score accumulator\n",
    "fold_scores = []\n",
    "\n",
    "#Iterate through the 5 splits\n",
    "for train_indices, test_indices in kf.split(X_train):\n",
    "    model = KNeighborsRegressor(10)\n",
    "    model.fit(X_train.values[train_indices], y_train.values[train_indices])\n",
    "    fold_scores.append(model.score(X_train.values[test_indices], y_train.values[test_indices]))\n",
    "\n",
    "print(f\"R^2 score against each test set: {[round(score, 2) for score in fold_scores]}\")\n",
    "print(np.mean(fold_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we get an average score of 0.69, that means our Hyperparameter ${k}$ = 10 should return an average R^2 score of this value. Now we won't have so much variety in our error metric when we compare different Hyperparameters/models. \n",
    "\n",
    "Let's do the same error-vs-complexity graph using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in range(50):\n",
    "    #Do KFold 50 times to look at more accurate errors scores when comparing differen n_neighbors(k)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    scores = []\n",
    "\n",
    "    train_score = []\n",
    "    test_score = []\n",
    "\n",
    "    for k in range(1, 11):\n",
    "        train_fold_scores = []\n",
    "        test_fold_scores = []\n",
    "\n",
    "        for train_indices, test_indices in kf.split(X):\n",
    "            model = KNeighborsRegressor(k)\n",
    "            model.fit(X.values[train_indices], y.values[train_indices])\n",
    "            train_fold_scores.append(mean_squared_error(model, X.values[train_indices], y.values[train_indices]))\n",
    "            test_fold_scores.append(mean_squared_error(model, X.values[test_indices], y.values[test_indices]))\n",
    "\n",
    "        train_score.append(np.mean(train_fold_scores))\n",
    "        test_score.append(np.mean(test_fold_scores))\n",
    "        #train_score.append(-model.score(X_train, y_train))\n",
    "        #test_score.append(-model.score(X_test, y_test))\n",
    "    if t == 0:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', label=\"train set\", alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', label=\"test set\", alpha=0.1)\n",
    "    else:\n",
    "        ax.plot(range(1, 11), train_score, 'r.-', alpha=0.1)\n",
    "        ax.plot(range(1, 11), test_score, 'b.-', alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel('complexity (value of k)')\n",
    "ax.set_ylabel('mean squared error')\n",
    "ax.set_ylim(5, 60)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple other notes.\n",
    "\n",
    "Many `sklearn` models include \"CV\" versions that use cross validation to calculate hyperparameters automatically.\n",
    "\n",
    "**Stratified cross validation** is a variation in which the partitions are chosen to have similar distributions of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the testing data\n",
    "\n",
    "There's a problem with all this. Because the model and hyperparameters are chosen based on the training and testing data, the errors of the model aren't an accurate representation of how it would behave on outside data. If we want to know how it will behave in general, we need to hold out additional data. In this case we have\n",
    "\n",
    " * **Training data** are used to fit the model.\n",
    " * **Validation data** are used to choose the model and hyperparameters. Once these are determined, these are combined with the training data to re-fit the model.\n",
    " * **Hold-out data** are used to evaluate the final accuracy of the model. \n",
    " \n",
    "<img src=\"img/cross_val_illustrated.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "People use different terms for the splits.\n",
    "\n",
    "* All data -> Train, Test, Hold-out (This is what I typically say)\n",
    "\n",
    "* All data -> Train, Validate, Test\n",
    "\n",
    "* All data -> Train, Validate, Hold-out (in visual above) (maybe most common)\n",
    "\n",
    "* All the same idea.\n",
    " \n",
    "For each of these we can use either simple hold-out validation or k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this to:   \n",
    " * Compare different models \n",
    "     - Example: Compaere KNN to Linear Regression to Logistic Regression...\n",
    " * Compare models with different parameters \n",
    "     - Example: Different n_neighbors(k) or different distance metrics\n",
    " * Compare models with different features \n",
    "     - ompare different models\n",
    "\n",
    "    Example: Compaere KNN to Linear Regression to Logistic Regression...\n",
    "\n",
    "Compare models with different parameters\n",
    "\n",
    "    Example: Different n_neighbors(k) or different distance metrics\n",
    "\n",
    "Compare models with different features\n",
    "\n",
    "    Example: Maybe take out the engine size or include color and make of car...\n",
    "\n",
    "Example: Maybe take out the engine size or include color and make of car..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do if your model is overfitting\n",
    "\n",
    "Pretty common.  If you are starting with 5-10 features that can already be pretty complex.  \n",
    "\n",
    "1. Get more data...  (not usually possible/practical but almost always the easy answer)\n",
    "2. Subset Selection: keep only a subset of your predictors (ie, dimensions)\n",
    "3. Regularization: restrict your models parameter space (Next Week)\n",
    "4. Dimensionality Reduction: project the data into a lower dimensional space (later in course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection:\n",
    "\n",
    "The features you select to put into your model will dictate how well you model preforms.  As this is a important idea, what ways can we try different features?   \n",
    "\n",
    "   \n",
    "What if we try every combination of features and find the best score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best subset:\n",
    "- Try every combination of `p` predictors\n",
    " * Computationally intensive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise:\n",
    "- Iteratively pick predictors to be in/out of the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Stepwise Selection   \n",
    "    \n",
    " 1. Create a model for every feature on its own.  Choose the feature with the best score.   \n",
    " 2. Run model with 2 features, best feature and every other feature.  Choose model features that have best score.   \n",
    " 3. Continue adding features until increase of score becomes not worth extra features or you run out of features.   \n",
    " \n",
    "   \n",
    "Is this better then just trying all combinations?   \n",
    "   \n",
    "Why may this not be good?\n",
    " <img src=\"img/step_forward.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Success Criteria\n",
    "\n",
    "Today I will be successful if I can... \n",
    "\n",
    " * Describe the three kinds of model error\n",
    " * Identify what types of models have high bias and high variance\n",
    " * Name two different Error Metrics and calculate MSE\n",
    " * Explain k-fold cross validation\n",
    " * Explain the training, validation, testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/target.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.model_selection import cross_validate\n",
    "## Let's build a model, cross-validate it and get the test score\n",
    "\n",
    "data = cars.copy()\n",
    "\n",
    "#Using all but one of our features for now... \n",
    "y = data.pop('mpg')\n",
    "\n",
    "X = X.iloc[:, :-1]\n",
    "\n",
    "#Here X_test/y_test is our holdout set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "# Cross validate against X_train and y_train... this will create \n",
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 17, 20, 25, 35, 40, 80]\n",
    "\n",
    "\n",
    "cv_scores = []\n",
    "test_scores = []\n",
    "\n",
    "## building several models with different values of k\n",
    "for k in k_vals:\n",
    "    model = KNeighborsRegressor(k)\n",
    "    #Note in cross_validate, it has specifically KFolds cross validation as an option, there are other ways to CV but this is most popular\n",
    "    cv = cross_validate(model, X_train, y_train, cv = KFold(n_splits = 5, shuffle=True))['test_score'].mean()\n",
    "\n",
    "    # store the average score in accumulators to compare later\n",
    "    cv_scores.append(cv)\n",
    "    # lets see how we compare against the holdout data\n",
    "    model.fit(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(k_val, cv_scores, label = 'cross-val scores')\n",
    "ax.plot(k_val, test_scores, label = 'test scores')\n",
    "ax.set_xlabel('K Hyperparameter')\n",
    "ax.set_ylabel('R Squared Score')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
