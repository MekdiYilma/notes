{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "Today you will be successful if you can...\n",
    "\n",
    " * State the purpose of Lasso and Ridge regression\n",
    " * Choose the regularization hyperparameter with cross validation\n",
    " * Note the differences between Lasso and Ridge regression\n",
    " * Perform standardization and normalization, and explain why it is needed with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't normally do this. This is to suppress warnings which happen during \n",
    "# demonstration of elasticnet\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent days we've talked about linear regression, in which we find the coefficients $\\beta_0$, $\\beta_1$, ..., $\\beta_p$ to minimize\n",
    "\n",
    "$$RSS = \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Ridge Regession** we find the values to minimize\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right) \\right)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Effectively we've penalizing extreme values of $\\beta$ (note that we aren't including $\\beta_0$). The value $\\alpha$ is a hyperparameter of the model specifying how large the penalty should be.\n",
    "\n",
    "**Question:** What does this mean if $\\alpha = 0$?\n",
    "<!-- The ridge regression line will be equivalent to the OLS line. (No regularization here) -->\n",
    "\n",
    "**Question:** What does this mean if $\\alpha \\to \\infty$\n",
    "<!-- We increase more and more bias. The larger we make alpha, our predictions become less and less senstitive to the features.  -->\n",
    "\n",
    "**Question:** How should we decide the appropriate value for $\\alpha$?\n",
    "<!-- Cross Validation? yay -->\n",
    "\n",
    "N.B.: sometimes to coefficient is called $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **LASSO Regession** (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) we find the values to minimize\n",
    "\n",
    "$$ \\sum_{i=1}^n \\left( y_i - \\left(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} \\right)\\right)^2 + \\alpha \\sum_{j=1}^{p} | \\beta_j |$$\n",
    "\n",
    "In many ways this is similar to Ridge:\n",
    "  * We're penalizing large values of $\\beta$.\n",
    "  * We aren't including $\\beta_0$.\n",
    "  * We have a hyperparameter $\\alpha$.\n",
    "\n",
    "The difference is the exponent. Ridge is sometimes known as **L2 regularization**, while LASSO is **L1 regularization**. We'll talk more about this in a bit.\n",
    "\n",
    "\n",
    "**Question:** how should we decide the appropriate value for $\\alpha$?\n",
    "\n",
    "**Question:** How does this relate to the bias-variance trade-off? If $\\alpha$ increases, what happens to the bias? What happens to the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example\n",
    "\n",
    "Let's use the cars dataset to investigate, predicting the mpg from the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throwing away some bad data. Don't do this at home.\n",
    "cars = cars[cars.horsepower != '?']\n",
    "cars.horsepower = cars.horsepower.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model']\n",
    "X = cars[columns].copy()\n",
    "y = cars['mpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the how the coefficients depend on various values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = -3\n",
    "max_alpha_exp = 1.5\n",
    "nfeatures = 6\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    #model = Pipeline([('standardize', StandardScaler()),\n",
    "    #                  ('lasso', Lasso(alpha=alpha))])\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X, y)\n",
    "    #coefs[i] = model.steps[1][1].coef_\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{{{}}}$\".format(columns[feature]))\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for LASSO regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nalphas = 50\n",
    "min_alpha_exp = 0\n",
    "max_alpha_exp = 6\n",
    "coefs = np.zeros((nalphas, nfeatures))\n",
    "alphas = np.logspace(min_alpha_exp, max_alpha_exp, nalphas)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X, y)\n",
    "    coefs[i] = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for feature, color in zip(range(nfeatures),\n",
    "                          ['r','g','b','c','m','k']):\n",
    "    plt.plot(alphas, coefs[:, feature],\n",
    "             color=color,\n",
    "             label=\"$\\\\beta_{{{}}}$\".format(columns[feature]))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"$\\\\beta$ as a function of $\\\\alpha$ for Ridge regression\")\n",
    "ax.set_xlabel(\"$\\\\alpha$\")\n",
    "ax.set_ylabel(\"$\\\\beta$\")\n",
    "ax.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: what's going on? How does this differ from LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(ax, radius, color, x=0, y=0):\n",
    "    artist = plt.Circle((x, y),\n",
    "                             radius,\n",
    "                             color=color,\n",
    "                             fill=False)\n",
    "    ax.add_artist(artist)\n",
    "    return artist\n",
    "def draw_diamond(ax, radius, color, x=0, y=0):\n",
    "    artist = plt.Polygon([(x, radius+y),\n",
    "                               (radius+x, y),\n",
    "                               (x, -radius+y),\n",
    "                               (-radius+x, y)],\n",
    "                              color=color,\n",
    "                              fill=False)\n",
    "    ax.add_artist(artist)\n",
    "    return artist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help understand this, consider the contour plots of the loss functions for the RMSE, and for the regularization penalty term. Imagine the center of each is the minimum, and we're looking for the minimum of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 6))\n",
    "for ax in axs:\n",
    "    ax.axhline(0, color='k')\n",
    "    ax.axvline(0, color='k')\n",
    "    ax.set_xlabel(r\"$\\beta_1$\")\n",
    "    ax.set_ylabel(r\"$\\beta_2$\")\n",
    "\n",
    "axs[0].set_title(r\"Ridge with $\\beta_1 \\ne 0$\")\n",
    "for r in [1.0, 2.0, 2.5, 2.8, 3.0]:\n",
    "    artist_loss = draw_circle(axs[0], r, 'b', 3, 4)\n",
    "for r in [1.0, 1.4, 1.77, 2.0]:\n",
    "    artist_regularization = draw_circle(axs[0], r, 'g')\n",
    "axs[0].legend([artist_loss, artist_regularization], ['RMSE loss', 'regularization penalty'])\n",
    "\n",
    "axs[1].set_title(r\"LASSO with $\\beta_1 \\ne 0$\")\n",
    "for r in [1.0, 2.0, 2.5, 2.8, 3.0]:\n",
    "    artist_loss = draw_circle(axs[1], r, 'b', 3, 4)\n",
    "for r in [0.45, 0.9, 1.35, 1.8, 2.25, 2.7]:\n",
    "    artist_regularization = draw_diamond(axs[1], r, 'g')\n",
    "axs[1].legend([artist_loss, artist_regularization], ['RMSE loss', 'regularization penalty'])\n",
    "\n",
    "# Example with LASSO at zero\n",
    "axs[2].set_title(r\"LASSO with $\\beta_1 = 0$\")\n",
    "for r in [1.0, 2.0, 2.5, 2.8, 3.0]:\n",
    "    artist_loss = draw_circle(axs[2], r, 'b', 1.3, 4)\n",
    "for r in [0.433, 0.866, 1.3]:\n",
    "    artist_regularization = draw_diamond(axs[2], r, 'g')\n",
    "axs[2].legend([artist_loss, artist_regularization], ['RMSE loss', 'regularization penalty'])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-3.5, 6.5)\n",
    "    ax.set_ylim(-3, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum of a sum of functions occurs when the gradient of the sum of the functions is zero. This occurs when the gradients of the two functions are equal and opposite, which occurs when the contour lines are parallel.\n",
    "\n",
    "In the above plots, imagine the minimum solutions are at the points when the outer contours lines are tangent. For Ridge regression, a minimum at $\\beta_i = 0$ is very unlikely, but quite possible for LASSO regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and regularization\n",
    "\n",
    "Over the coming weeks we'll talk about a variety of predictive models, and various ways in which they are different. One of those is whether it is necessary to standardize/normalize the features before fitting. First, some definitions:\n",
    "\n",
    "**Standardization** (in this context) is the process of subtracting the mean from each feature, and then dividing by the standard deviation, so each feature has a mean of 0 and standard deviation of 1.\n",
    "\n",
    "**Normalization** (again, in this context) is the process of subtracting the minimum value from each feature, and then dividing by the difference of the minimum and maximum, so each feature ranges from 0 to 1.\n",
    "\n",
    "Both accomplish the same purpose, of having all features on the same scale.\n",
    "\n",
    "So for some models you need to standardize (or normalize) the features before fitting the model, at least if they have significantly different ranges. It's not that hard to write code to do that, but the transformers in `sklearn` make that a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#for standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#for minimization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Ridge())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline model will take care of the transformations in fitting and prediction automatically. You can normalize using `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "In linear regression without regularization, scaling **does not matter**. If you change the scale of a feature (multiplying each value by a constant), it will change the corresponding coefficient **but the predictions will be exactly the same**.\n",
    "\n",
    "This is not true when we add regularization. Since we include a term that is proportional to the $\\beta$, the actual predictions will change it we rescale the values.\n",
    "\n",
    "As a rule of thumb, if changing the units of the features will change the predictions of a model, you need to standardize (or normalize) the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['regressor'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['regressor'].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do one more example using cross validation to determine our best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X.values,y.values, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X_train, y_train, model, k = 5):\n",
    "    ''' Returns error for k-fold cross validation. '''\n",
    "    kf = KFold(n_splits=k)\n",
    "    error = np.empty(k)\n",
    "    index = 0\n",
    "    for train, test in kf.split(X_train):\n",
    "        model.fit(X_train[train], y_train[train])\n",
    "        pred = model.predict(X_train[test])\n",
    "        error[index] = mean_squared_error(y_train[test], pred, squared = False)\n",
    "        index += 1\n",
    "    return np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Ridge())])\n",
    "cross_val(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "alphas = np.logspace(-2, 1.5, 20)\n",
    "for a in alphas:\n",
    "    cv_score = cross_val(X_train, y_train, Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Ridge(alpha = a))]))\n",
    "    cv_scores.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = np.argmin(cv_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(alphas, cv_scores)\n",
    "ax.scatter(alphas[min_score], cv_scores[min_score], ls = '--', c = 'r', \\\n",
    "           label = f'alpha: {round(alphas[min_score],2)} \\nRMSE: {round(cv_scores[min_score], 2)}')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_xscale('log')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our training error is 3.47 using RMSE, a perfectly acceptable loss/cost function, leads us to an alpha around 1.62. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('standardize', StandardScaler()),\n",
    "                   ('regressor', Ridge(alpha = alphas[min_score]))])\n",
    "model.fit(X_train, y_train)\n",
    "final_preds = model.predict(X_holdout)\n",
    "print(f'Train RMSE: {mean_squared_error(model.predict(X_train), y_train, squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(y_holdout, final_preds, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch \n",
    "Woudln't it be nice to do cross validation to determine the best alphas without having to code all of these for loops? \n",
    "\n",
    "Sklearn's GridsearchCV does just that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "search_space = [{'regressor__alpha': (np.logspace(-2, 1.5, 20))}]\n",
    "\n",
    "rmse = make_scorer(mean_squared_error,greater_is_better=False, squared = False)\n",
    "\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Lasso())])\n",
    "\n",
    "enet_cv = GridSearchCV(pipe, \n",
    "                      search_space, \n",
    "                      cv=KFold(10, shuffle=True), \n",
    "                      scoring=rmse,\n",
    "                      return_train_score = True)\n",
    "\n",
    "enet_cv.fit(X_train,y_train)\n",
    "\n",
    "enet_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might want to know how our model does using ElasticNet, seeing how it is the best of both worlds and can balance L1 and L2! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_space = [{'regressor__alpha': (np.logspace(-2, 2, 30)), \\\n",
    "                'regressor__l1_ratio': (np.linspace(0,1,11))}]\n",
    "\n",
    "rmse = make_scorer(mean_squared_error,greater_is_better=False, squared = False)\n",
    "\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('regressor', ElasticNet())])\n",
    "\n",
    "enet_cv = GridSearchCV(pipe, \n",
    "                      search_space, \n",
    "                      cv=KFold(5, shuffle=True), \n",
    "                      scoring='neg_mean_squared_error',\n",
    "                      return_train_score = True)\n",
    "\n",
    "enet_cv.fit(X_train, y_train)\n",
    "\n",
    "enet_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
