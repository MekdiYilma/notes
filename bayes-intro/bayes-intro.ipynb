{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics\n",
    "### Example via Jack Bennetto (thanks Jack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "I will feel successful today if I can...\n",
    "\n",
    " * Describe the difference between Frequentist and Bayesian statistics\n",
    " * ID different components of Bayes Theorem\n",
    " * Use Bayes' theorem to calculate posterior probabilities \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' theorem.\n",
    "\n",
    "This Theorem is usually written using variables A and B... \n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$\n",
    "\n",
    "Each term has a name.\n",
    "\n",
    "* $P(A)$ is the *prior probability*\n",
    "* $P(B|A)$ is the *likelihood*.\n",
    "* $P(B)$ is the *marglinal likelihood* sometimes called *normalization constant*\n",
    "* $P(A|B)$ is the *posterior probability*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Suppose we're considering some hypothesis $H$ and we've collected some data $\\mathbf{X}$.\n",
    "$$ P(H|\\mathbf{X}) = \\frac{P(\\mathbf{X}|H) P(H)}{P(\\mathbf{X})} $$\n",
    "\n",
    "\n",
    "\n",
    "If there are a bunch of hypotheses $H_1, H_2, ... H_n$, we could write this as\n",
    "\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we see the normalizing constant is the likelihood times the prior summed over all possible hypotheses (using the law of total probability).\n",
    "\n",
    "In other words, it's the constant (independent of hypothesis) needed to be multiplied by all the numerators so that they all add up to one.\n",
    "\n",
    "Let's run through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian statistics to find a mean\n",
    "Let's assume you have a bunch of points drawn from a normal distribution. To make things easy, let's say you happen to know that the standard deviation is 3, and the mean $\\mu \\in \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}$. We're going to determine the probability that any of those are the correct mean based on data.\n",
    "\n",
    "Humans are pretty bad at choosing random numbers, so someone will need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mu = stats.randint(0,10).rvs()\n",
    "sd = 3\n",
    "\n",
    "# Now you need to choose a number from the distribution. What is it?\n",
    "first_X = stats.norm(mu, sd).rvs()\n",
    "first_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now using that number, I'm going to figure out the likelihood you would have gotten that from any of the possible hypotheses, by looking at the **pdf** of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = first_X\n",
    "likelihoods = []\n",
    "for i in range(0,10):\n",
    "    likelihoods.append(stats.norm(i, sd).pdf(datum))\n",
    "    print(\"The likelihood of N({0}, {1}) generating {2:5.2f} is {3:6.4f}\"\n",
    "           .format(i, sd, datum, likelihoods[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), likelihoods)\n",
    "ax.bar(mu, likelihoods[mu], color = 'r', label = r'True ${mu}$')\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('likelihood')\n",
    "ax.legend()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do these bars all add to one?\n",
    "\n",
    "Which of these hypotheses has the maximum likelihood of producing the data?\n",
    "\n",
    "If we were a Frequentist, we'd go with that, and then we'd construct a confidence interval, giving a range that (had we sampled from the data many times) has a certain probability (maybe 95%) of including the actual value.\n",
    "\n",
    "But today we're all going to be Bayesians, which means we're going to assign probabilities of each hypothesis being true.\n",
    "\n",
    "The tough part of being a Bayesian is we need to start out with a prior probabilities. For this, we'll assume that all the probabilities are equal. You chose them that way using the computer, so that works out, but if you'd picked a number from your head, and you liked some numbers more than others, that might not be best.\n",
    "\n",
    "The arbitrary choice of priors is probably **the largest criticism** of Bayesian statistics. But if you have enough data it doesn't matter that much.\n",
    "\n",
    "Dont Forget... Here is our Bayesian formula\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.ones(10)/10\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to multiple each of these by the likelihood..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    probs[i] *= stats.norm(i, sd).pdf(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then divide normalize them by dividing them each by the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs /= probs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, what we've done is multiplied each of the prior probabilities by the likelihood of each hypothesis of generating the observed data, and divided these all by the normalizing constant, to get the posterior probabilities.\n",
    "$$\\begin{align}\n",
    "P(H_i|\\mathbf{X}) & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{P(\\mathbf{X})}\\\\\n",
    "         & = \\frac{P(\\mathbf{X}|H_i) P(H_i)}{\\sum_{j=0}^{n} P(\\mathbf{X}|H_j) P(H_j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got for the **probabilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {2:6.4f}\"\n",
    "           .format(i, sd, probs[i]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.bar(mu, probs[mu], color = 'r', label = r'True ${mu}$')\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do these add up to one?\n",
    "\n",
    "Okay, that was great, but maybe we should get some more data. Generate another number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_X = stats.norm(mu, sd).rvs()\n",
    "another_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we\n",
    " * calculate the likelihoods,\n",
    " * multiply these **by our old posterior probabilities** (which are the new priors),\n",
    " * normalize (divide the sum of the prior times likelihood, so they add to one), and\n",
    " * look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = another_X\n",
    "\n",
    "# calculate the likelihoods\n",
    "for i in range(10):\n",
    "#     multiply these by our old posterior probabilities (which are the new priors),\n",
    "    probs[i] *= stats.norm(i, sd).pdf(datum)\n",
    "# normalize (divide the sum of the prior times likelihood, so they add to one)\n",
    "probs /= probs.sum()\n",
    "\n",
    "# look at the output\n",
    "for i in range(0,10):\n",
    "    print(\"The probability of N({0}, {1}) being correct is {3:10.8f}\"\n",
    "           .format(i, sd, datum, probs[i]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.bar(mu, probs[mu], color = 'r', label = r'True ${mu}$')\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and again... our \"posteriors\" replace our priors and we update our situation over and over and over...\n",
    "for _ in range(10):\n",
    "    datum = stats.norm(mu, sd).rvs()\n",
    "\n",
    "    # calculate the likelihoods\n",
    "    for i in range(10):\n",
    "    #     multiply these by our old posterior probabilities (which are the new priors),\n",
    "        probs[i] *= stats.norm(i, sd).pdf(datum)\n",
    "    # normalize (divide the sum of the prior times likelihood, so they add to one)\n",
    "    probs /= probs.sum()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(10), probs)\n",
    "ax.bar(mu, probs[mu], color = 'r', label = r'True ${mu}$')\n",
    "ax.set_xlabel('hypothesized mean')\n",
    "ax.set_ylabel('posterior probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing this iteratively, repeatedly getting another data point and updating our prior, but we could have done this all at once, calculating the likelihood of seeing the whole dataset.\n",
    "\n",
    "In a real problem we'd have *many* more possible hypotheses. In the case above, we might not know the number came from a discrete distribution so we'd need to consider every possible value. And we probably wouldn't know the standard deviation, so we'd need to consider every combination of a mean and standard deviation. We could follow the same approach, calculating the likelihood of seeing our data for each possible hypothesis and updating the posterior probabilities. Later we'll talk about how to solve this practically..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A Tangible Example \n",
    "\n",
    "##### Example via RFT\n",
    "\n",
    "#### Fair or Unfair Coin\n",
    "\n",
    "Let's say we picked one of two coins.  One isn't fair (pHeads = 0.40) while the other one is (pHeads = 0.5).  After a certain number of flips, what is our degree of belief that the results came from each of the coins?\n",
    "\n",
    "Bayes rule for this example:\n",
    "\n",
    "Your Hypotheses available are only... \n",
    "\n",
    "$H_i$ = **Fair OR Unfair**\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(H_i | Flips)   = \\frac{p(Flips | H_i) \\times p(H_i)}{p(Flips)}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "At the start, there is an equal probability of picking either coin:\n",
    "\n",
    "$$ p(Fair) = p(Unfair) = 0.5 $$\n",
    "\n",
    "\n",
    "$p(Flips)$ is calculated using the Law of Total Probability: \n",
    "\n",
    "$$p(Flips) = p(Flips|Fair)\\times p(Fair) + p(Flips|Unfair)\\times p(Unfair) $$\n",
    "\n",
    "If we have just one flip (a tails):\n",
    "\n",
    "$$p(tails) = p(tails|Fair)\\times p(Fair) + p(tails|Unfair)\\times p(Unfair)$$\n",
    "\n",
    "$$p(tails) = 0.5\\times 0.5 + 0.6\\times 0.5 = 0.55$$\n",
    "\n",
    "If we have just one flip (a heads):\n",
    "\n",
    "$$p(heads) = p(heads|Fair)\\times p(Fair) + p(heads|Unfair)\\times p(Unfair)$$\n",
    "\n",
    "$$p(heads) = 0.5\\times 0.5 + 0.4\\times 0.5 = 0.45$$\n",
    "\n",
    "If we have zero flips, $P(flips)=1$ as default.\n",
    "\n",
    "\n",
    "We'll keep track of the probabilities in a list:  \n",
    "$$ [p(Fair), p(Unfair)] $$ \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37) # try to make it so that we all get the same result\n",
    "\n",
    "# make the coins and select one\n",
    "p_fair = 0.5 # don't change\n",
    "\n",
    "p_not_fair = 0.4 \n",
    "\n",
    "# Pick one of these two values to be the p of the coin we chose\n",
    "p = np.random.choice([p_fair, p_not_fair])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicate_coin_picked(p):\n",
    "    if p == .5:\n",
    "        print(\"It's the fair coin (p = 0.5).\")\n",
    "    else:\n",
    "        print(f\"It's the unfair coin (p = {p}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we figure out, from the flips, whether it's a fair coin or not?  And if so, how soon can we know it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 1\n",
    "\n",
    "def flip_the_coin(p, flips_lst):\n",
    "    '''Flips the coin with probability of success p\n",
    "       and appends to the flips_lst'''\n",
    "    flips_lst.append(1*(np.random.random()<=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it out - initialize flips_lst\n",
    "flips_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some flips (trials) - keep executing this cell\n",
    "flip_the_coin(p, flips_lst)\n",
    "print(flips_lst, round(np.mean(flips_lst),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we\n",
    " * calculate the likelihoods,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate our likelihoods\n",
    "\n",
    "def calculate_likelihood(flips_lst):\n",
    "    '''Likelihood of flips in flips_lst given fair, not fair coin'''\n",
    "    likelihood_fair = stats.bernoulli.pmf(flips_lst[-1], p_fair)\n",
    "    likelihood_not_fair = stats.bernoulli.pmf(flips_lst[-1], p_not_fair)\n",
    "    return [likelihood_fair, likelihood_not_fair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's double check that function\n",
    "likelihoods = calculate_likelihood(flips_lst)\n",
    "print(np.around(likelihoods,3))\n",
    "print(\"\\nLikelihood fair: {0:0.3f}\".format(likelihoods[0]))\n",
    "print(\"Likelihood not fair: {0:0.3f}\".format(likelihoods[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "p(H_i | Flips)   = \\frac{p(Flips | H_i) \\times p(H_i)}{p(Flips)}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "posterior   = \\frac{likelihood \\times prior}{marginal}\n",
    "\\end{equation*}\n",
    "\n",
    "*remember marginal likelihood is aka normalization constant*\n",
    "\n",
    " * multiply these **by our old posterior probabilities** (which are the new priors),\n",
    " * normalize (divide the sum of the prior times likelihood, so they add to one), and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function 3\n",
    "marginal = 1  # placeholder, If we have zero flips, P(flips)=1 as default.\n",
    "\n",
    "def calculate_posterior(likelihoods_lst, prior_lst):\n",
    "    '''Calculates the posterior given the likelihoods and prior\n",
    "    '''\n",
    "    posterior_unnormalized = []\n",
    "    for likelihood, prior in zip(likelihoods_lst, prior_lst):\n",
    "        #multiply likelihoods by our old posterior probabilities (marinal/constant is one for now)\n",
    "        posterior_unnormalized.append(likelihood * prior/ marginal)\n",
    "        \n",
    "    # normalize so that the total probability in posterior is 1\n",
    "    posterior_un_total = sum(posterior_unnormalized)\n",
    "    posterior_lst = []\n",
    "    for posterior in posterior_unnormalized:\n",
    "        posterior_lst.append(posterior/posterior_un_total)\n",
    "    return posterior_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - plot the probability of the fair coin with increased flips\n",
    "\n",
    "def plot_pfair_prob(num_flips, p_fair_arr, data):\n",
    "    fig, axs = plt.subplots(2,1,figsize=(10,10))\n",
    "    flip_num = np.arange(1, num_flips + 1)\n",
    "    axs[0].scatter(np.arange(1,num_flips+1), data+np.random.random(len(data))/10, marker='.')\n",
    "    axs[0].plot(np.arange(1,num_flips+1),np.cumsum(data)/np.arange(1,num_flips+1))\n",
    "    axs[0].axhline(.5, color = 'green', linestyle = '--')\n",
    "    axs[0].axhline(p, color = 'green', linestyle = '--')\n",
    "    axs[0].set_title('Actual Flips and Running Average')\n",
    "    axs[1].plot(flip_num, p_fair_arr)\n",
    "    axs[1].set_ylim([-0.1, 1.1])\n",
    "    axs[1].set_title('Probability of fair coin as a function of flip number')\n",
    "    axs[1].set_ylabel('Probability p_fair')\n",
    "    axs[1].set_xlabel('Flip number')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do a simulation0\n",
    "# np.random.seed(2) # try 2 and 3\n",
    "\n",
    "indicate_coin_picked(p)\n",
    "\n",
    "# initialize\n",
    "priors = [0.5, 0.5]\n",
    "flips_lst = []\n",
    "\n",
    "# set the number of flips\n",
    "num_flips = 2000\n",
    "p_fair_arr = np.zeros(num_flips)\n",
    "\n",
    "for i in range(num_flips):\n",
    "    flip_the_coin(p, flips_lst)\n",
    "    likelihoods = calculate_likelihood(flips_lst)\n",
    "    posteriors = calculate_posterior(likelihoods, priors)\n",
    "    p_fair_arr[i] = posteriors[0]\n",
    "    priors = posteriors\n",
    "\n",
    "print(\"\\nPosteriors after {0} trials\".format(num_flips))\n",
    "print(\"Probability Fair {0:0.3f}, Not fair {1:0.3f}\".format(posteriors[0], posteriors[1]))\n",
    "\n",
    "plot_pfair_prob(num_flips, p_fair_arr, flips_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems very sensitive to updates.  Let's add a tuning dial (learning rate) that affects how much each update can affect the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal = 1  # placeholder, p(Flips) is same for both fair and unfair \n",
    "\n",
    "def normalize(lst):\n",
    "    total = sum(lst)\n",
    "    return [val/total for val in lst]\n",
    "\n",
    "def calculate_posterior_with_learning_rate(likelihoods_lst, prior_lst, learning_rate):\n",
    "    '''Calculates the posterior given the likelihoods and prior'''\n",
    "    posterior_unnormalized = []\n",
    "    for likelihood, prior in zip(likelihoods_lst, prior_lst):\n",
    "        posterior_unnormalized.append(likelihood * prior / marginal)\n",
    "    \n",
    "    # now need to normalize so that the total probability in posterior is 1\n",
    "    posterior_lst = normalize(posterior_unnormalized)\n",
    "    \n",
    "    # now weight returned posterior by new posterior and old posterior\n",
    "    posterior_weighted_unnorm = []\n",
    "    for posterior, prior in zip(posterior_lst, prior_lst):\n",
    "        posterior_weighted_unnorm.append(learning_rate * posterior + \n",
    "                                         (1 - learning_rate) * prior)\n",
    "    posterior_weighted = normalize(posterior_weighted_unnorm)\n",
    "    return posterior_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicate_coin_picked(p)\n",
    "\n",
    "# initialize\n",
    "\n",
    "priors = [0.5, 0.5]\n",
    "\n",
    "learning_rate = 1.5\n",
    "\n",
    "flips_lst = []\n",
    "\n",
    "# set the number of flips\n",
    "num_flips = 2000\n",
    "p_fair_arr = np.zeros(num_flips)\n",
    "\n",
    "\n",
    "for i in range(num_flips):\n",
    "    flip_the_coin(p, flips_lst)\n",
    "    likelihoods = calculate_likelihood(flips_lst)\n",
    "    posteriors = calculate_posterior_with_learning_rate(likelihoods, priors, learning_rate)\n",
    "    p_fair_arr[i] = posteriors[0]\n",
    "    priors = posteriors\n",
    "\n",
    "\n",
    "print(\"\\nPosteriors after {0} trials\".format(num_flips))\n",
    "print(\"Probability Fair {0:0.3f}, Not fair {1:0.3f}\".format(posteriors[0], posteriors[1]))\n",
    "\n",
    "plot_pfair_prob(num_flips, p_fair_arr, flips_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
