{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Always make it pretty.\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Large Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the simplest and possibly the most natural statistic: the *mean* of a random sample.  To be clear:\n",
    "\n",
    "  - The **mean**, or **expectation** of a random variable or population is the theoretical quantity computed by the following integral:\n",
    "  \n",
    "$$ E[X] = \\int_{- \\infty}^{\\infty} t \\\n",
    "f_{X}(t) dt $$\n",
    "\n",
    "The expectation is a property of a random variable, and you *must* know the distribution of the variable *completely* to compute it.  **This *never* happens in real life**, the application of computing a mean of a random variable in this way is when we hypothesize *models* of our data, we'll get to that.\n",
    "\n",
    "  - The **sample mean** of a set of *data* (which is often postulated to result from an i.i.d sample from a random variable) is the *practical* quantity computed as the following sum:\n",
    "  \n",
    "$$\\frac{1}{n} \\sum_i X_i$$\n",
    "\n",
    "When both concepts are relevant, we often qualify the first from the second by using the term **population mean**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While the names betray a connection, it's important to realize that these two concepts are just *definitions*, subject to our whim.\n",
    "\n",
    "The fundamental connection between the two concepts is given by the *law of large numbers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Law of Large Numbers.\n",
    "\n",
    "Suppose that $X_1, X_2, \\ldots$ are i.i.d copies of a random variable $X$, that is, a random sample from a population.  \n",
    "\n",
    "Then, for $n$ sufficiently large, the sample mean computed from a sample of size $n$ is approximately equal to the population mean:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_i X_i \\approx E[X] $$\n",
    "\n",
    "**Technical Note:** The precise mathematical statement of the law of large numbers needs some more technology, namely **[convergence in probability](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability)**.  It essentially says that, as $n$ get large, the probability of seeing a sample mean that is different from the population mean goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Binomial Draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $X$ is a simple Bernoulli random variable, for simplicitly\n",
    "\n",
    "$$ X \\sim \\text{Bernoulli}(p = 0.5) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the plots below, we draw samples of increasing size from $X$, and compute the corresponding sample mean.  According to the law of large numbers, these paths of sample means should all limit towards the population expectation of $p = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumlative_sample_means(sample):\n",
    "    cumlative_sums = np.cumsum(sample)\n",
    "    sample_sizes = np.arange(1, len(cumlative_sums) + 1)\n",
    "    sample_means = cumlative_sums / sample_sizes\n",
    "    return sample_sizes, sample_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "axs[0].set_title(\"Proportion of Heads as Sample Size Increases\")\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    sample = bernoulli.rvs(100).astype(np.float64)\n",
    "    sample_sizes, sample_means = compute_cumlative_sample_means(sample)\n",
    "    ax.plot(sample_sizes, np.repeat(0.5, len(sample_sizes)), linewidth=2, c='grey', alpha=0.5)\n",
    "    ax.plot(sample_sizes, sample_means)\n",
    "    if idx == 3:\n",
    "        ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax.set_title(\"Proportion of Heads as Sample Size Increases\")\n",
    "for i in range(100):\n",
    "    sample = bernoulli.rvs(1000).astype(np.float64)\n",
    "    sample_sizes, sample_means = compute_cumlative_sample_means(sample)\n",
    "    ax.plot(sample_sizes, sample_means)\n",
    "    ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Thing that is NOT True\n",
    "\n",
    "The law of large numbers does **not** say that the number of $0$'s sampled becomes equal to the number of $1$'s sampled as $n$ grows, **only that the differences between them are swamped out as the denominator gets large**.\n",
    "\n",
    "If you flip a coin an infinite number of times, you should *not* expect to get \"the same number of heads as tails\".  Statistics does *not* say they cancel out.  Only that the differences become less noticeable over time with respect to the magnitude of numbers involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumlative_sample_differences(sample):\n",
    "    ones_and_negative_ones = sample + (sample - 1)\n",
    "    cumlative_sums = np.cumsum(ones_and_negative_ones)\n",
    "    sample_sizes = np.arange(1, len(cumlative_sums) + 1)\n",
    "    return sample_sizes, cumlative_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=(16, 8), sharex=True, sharey=True)\n",
    "\n",
    "axs[0].set_title(\"Difference Between Number of Heads and Tails as Sample Size Increases.\")\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    sample = bernoulli.rvs(1000).astype(np.float64)\n",
    "    sample_sizes, sample_differences = compute_cumlative_sample_differences(sample)\n",
    "    ax.plot(sample_sizes, np.repeat(0, len(sample_sizes)), linewidth=2, c='grey', alpha=0.5)\n",
    "    ax.plot(sample_sizes, sample_differences)\n",
    "    if idx == 3:\n",
    "        ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, if you plot many of these curves on one graph, you can see that in aggregate, they spread out over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax.set_title(\"Difference Between Number of Heads and Tails as Sample Size Increases.\")\n",
    "for i in range(100):\n",
    "    sample = bernoulli.rvs(1000).astype(np.float64)\n",
    "    sample_sizes, sample_differences = compute_cumlative_sample_differences(sample)\n",
    "    #ax.plot(sample_sizes, np.repeat(0, len(sample_sizes)), linewidth=2, c='grey', alpha=0.5)\n",
    "    ax.plot(sample_sizes, sample_differences)\n",
    "ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to interpret the law of large numbers is that the difference in the number of heads and tails are spreading out *slower* than that rate of increase of sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variance of the Sample Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The law of large numbers tells us that the sample mean is an estimate of the population mean.  A follow up question now arises: **how accurate of an estimate of the population mean is the sample mean?**\n",
    "\n",
    "This is essentially asking: how tight is the bundle of curves here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax.set_title(\"Proportion of Heads as Sample Size Increases\")\n",
    "for i in range(100):\n",
    "    sample = bernoulli.rvs(1000).astype(np.float64)\n",
    "    sample_sizes, sample_means = compute_cumlative_sample_means(sample)\n",
    "    ax.plot(sample_sizes, sample_means)\n",
    "ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bundle has a characteristic shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = stats.binom(n=1, p=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax.set_title(\"Proportion of Heads as Sample Size Increases\")\n",
    "for i in range(100):\n",
    "    sample = bernoulli.rvs(1000).astype(np.float64)\n",
    "    sample_sizes, sample_means = compute_cumlative_sample_means(sample)\n",
    "    ax.plot(sample_sizes, sample_means, alpha=0.25) \n",
    "    \n",
    "x = np.arange(1, 1000)\n",
    "ax.plot(x, 0.5 + 0.25/np.sqrt(x), color=\"black\", linewidth=3)\n",
    "ax.plot(x, 0.5 - 0.25/np.sqrt(x), color=\"black\", linewidth=3)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_xlabel(\"Sample Size\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our observations that \n",
    "  - A sample mean has less variance than the individual draws in the samples themselves.\n",
    "  - The variance of a sample mean gets smaller as the size of the sample increases.\n",
    "  \n",
    "It's actually possible to quantify how much less variance.\n",
    "\n",
    "This calcualtion is a classic, and is something every statistician should be able to repeat.\n",
    "\n",
    "$$ var \\left( \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\right) = \\frac{1}{n^2} var (X_1 + X_2 + \\cdots + X_n) = \\frac{n}{n^2} var(X) = \\frac{1}{n} var (X) $$\n",
    "\n",
    "$$ sd \\left( \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\right) = \\frac{1}{\\sqrt{n}} sd (X) $$\n",
    "\n",
    "\n",
    "The final quantity here is called the **standard error**, it is shorthand for the more unwieldy phrase **standard deviation of the sample mean**.\n",
    "\n",
    "**Question:** There are two assumptions needed for this calculation to work out:\n",
    "\n",
    "1. The draws are all identically distributed.\n",
    "2. The draws are all independent.\n",
    "\n",
    "Where in the calculation above have we used these facts?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
