{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Based on Jack Bennetto and Ryan Kasichainula Lecture Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scs\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rc('font', size=16)\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "Today I will be successful if I can...\n",
    "\n",
    "1. Explain the difference between inferential regression and predictive. \n",
    "2. Explain how our scipystats.OLS determine the correct betas for our Regression line?\n",
    "3. Note the popular error metric for linear regression and what the value means. \n",
    "4. Interpret the beta coefficients. \n",
    "5. Explain the 5 Assumptions of good inferential linaer regression\n",
    "6. Define the null hypothesis associated with the beta coefficients in OLS summary. \n",
    "7. Note one way to handle categorical features within your data. \n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Start with basics of linear regression\n",
    "2. Work in Inferential Regression focus\n",
    "2. Introduce OLS\n",
    "3. Move ipynb to focus on Assumptions\n",
    "4. Move to final \"predictive\" Linear Regression\n",
    "*Note: These success criteria are for both lecture topics today, Predictive and Inferential linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `cars` data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars_multivariate.csv')\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(cars, figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "What is it? We are hypothesizing a **linear relationship** between a *target* and some *features*.\n",
    "\n",
    "In the case of a **single** feature, we are looking to quantify the relationship of the form\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Let's select `mpg` as our target and `weight` as our predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "X = cars['weight']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X,y, color='k', s=10)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of lines we could draw... how do we pick a \"best\" line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "xx = np.linspace(1500,5500)\n",
    "line0 = 0*xx + 25\n",
    "line1 = (-1/200)*(xx - 1500) + 30\n",
    "line2 = (-1/100)*(xx - 1500) + 39\n",
    "\n",
    "ax.scatter(X,y, color='k', s=20)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg')\n",
    "ax.plot(xx, line0, color='b', lw=3)\n",
    "ax.plot(xx, line1, color='r', lw=3)\n",
    "ax.plot(xx, line2, color='brown', lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call our line $\\hat{y}$. For any point $x_i$, we have our observed value $y_i$ and our value predicted from our line $$\\large \\hat{y}_i = \\beta_0 + \\beta_1 x_i$$\n",
    "\n",
    "\n",
    "Sometimes this is written as the following. Consider how the example changes as we add more features. \n",
    "\n",
    "$$\\large y = \\beta_0 + \\beta_1X_1 + ... \\beta_nX_n + \\epsilon$$\n",
    "\n",
    "\n",
    "The *residual* is the distance between our predicted value and the actual value\n",
    "$$\\large r_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Let's find the line that minimizes the total **sum of squared residuals** (SSR)\n",
    "$$\\large SSR = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.scatter(X,y, color='k', s=20)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg')\n",
    "\n",
    "b0 = (1500/200) + 30\n",
    "b1 = (-1/250)\n",
    "\n",
    "line1 = b0 + b1*xx\n",
    "ax.plot(xx, line1, color='r', lw=2)\n",
    "\n",
    "for x_i, y_i in zip(X,y):\n",
    "    ax.plot([x_i, x_i], [y_i, b1*x_i+b0],\n",
    "            color='gray',\n",
    "            linestyle='dashed')\n",
    "    \n",
    "ax.set_title(\"A line we made up\")    \n",
    "ax.set_ylim(0,50)\n",
    "\n",
    "resids = y - (b0 + b1 * X)\n",
    "print(\"SSR for this line: {}\".format((resids**2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the 'Right' $\\beta$\n",
    "\n",
    "We *could* treat this as a minimization problem and experiment with various values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best beta values to 0 and RSS to something really really high. \n",
    "best_b_0 = 0\n",
    "best_b_1 = 0\n",
    "SSR = 15000\n",
    "\n",
    "# Loop through potential values for b_0 ad b_1\n",
    "# If the RSS is better than the best we've seen so far,\n",
    "# update the best values and the best mse. \n",
    "for b_0 in np.linspace(10,100, 20):\n",
    "    for b_1 in np.linspace(0, -1, 200):\n",
    "        y_pred = b_0 + b_1 * X\n",
    "        resids = y - y_pred\n",
    "        new_SSR = (resids**2).sum()\n",
    "        if new_SSR < SSR:\n",
    "            SSR, best_b_0, best_b_1 = new_SSR, b_0, b_1\n",
    "            \n",
    "print(\"Coefficients: b0={}, b1={} \\n\".format(best_b_0, best_b_1))\n",
    "print(\"SSR: {}\".format(SSR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.scatter(X,y, color='k', s=20)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg')\n",
    "\n",
    "line1 = best_b_0 + best_b_1*xx\n",
    "ax.plot(xx, line1, color='r', lw=2)\n",
    "\n",
    "for x_i, y_i in zip(X,y):\n",
    "    ax.plot([x_i, x_i], [y_i, best_b_1*x_i+best_b_0],\n",
    "            color='gray',\n",
    "            linestyle='dashed')\n",
    "    \n",
    "ax.set_title(\"A Better Line\")    \n",
    "ax.set_ylim(0,50)\n",
    "\n",
    "\n",
    "resids = y - (best_b_0 + best_b_1 * X)\n",
    "print(\"SSR for this line: {}\".format((resids**2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Analytical Solution: OLS\n",
    "\n",
    "Searching for the best coefficients becomes expensive very quickly when we have have more and more predictors. (What computational complexity?)\n",
    "\n",
    "<!-- $O(2^n)$ where n is number of predictors -->\n",
    "\n",
    "Thankfully, we don't have to do that. \n",
    "\n",
    "We can find the values for $\\beta$ which minimize the sum squared errors with linear algebra:\n",
    "\n",
    "$$\\large \\hat{\\beta} = (X^TX)^{-1}(X^Ty)$$\n",
    "\n",
    "The above formula corresponds to the method called **Ordinary Least Squares**. This is a type of linear least squares method to estimate the unknown paramters/coefficients in our linaer regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cars[['weight']]\n",
    "y = cars[['mpg']]\n",
    "\n",
    "X_mat = X.to_numpy()\n",
    "y_vec = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = np.hstack([np.ones((len(X_mat), 1)), X_mat])\n",
    "X_mat[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.linalg.inv(X_mat.T @ X_mat) @ (X_mat.T @ y_vec)\n",
    "\n",
    "print(f'Beta0: {coefs[0][0]} \\nBeta1: {coefs[1][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.scatter(X,y, color='k', s=20)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg')\n",
    "y = cars['mpg']\n",
    "X = cars['weight']\n",
    "\n",
    "best_b_0 = coefs[0][0]\n",
    "best_b_1 = coefs[1][0]\n",
    "\n",
    "line1 = best_b_0 + best_b_1*xx\n",
    "ax.plot(xx, line1, color='r', lw=2)\n",
    "\n",
    "for x_i, y_i in zip(X,y):\n",
    "    ax.plot([x_i, x_i], [y_i, best_b_1*x_i+best_b_0],\n",
    "            color='gray',\n",
    "            linestyle='dashed')\n",
    "    \n",
    "ax.set_title(\"A Best Line\")    \n",
    "ax.set_ylim(0,50)\n",
    "\n",
    "\n",
    "resids = y - (best_b_0 + best_b_1 * X)\n",
    "print(\"SSR for this line: {}\".format((resids**2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I guess we can just use Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` package finds the best-fit line through the origin, i.e., with an intercept of zero.\n",
    "\n",
    " **Question:** So what should I do first to ensure that we can deviate from an intercept at zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "X = sm.add_constant(cars['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using statsmodels libary because of the awesome descriptive statistics (sklearn will be used later for LR)\n",
    "\n",
    "#build our Ordinary Least Squares model with y then X (weird)\n",
    "simple_model = sm.OLS(y, X)\n",
    "\n",
    "#Here our fit is happening differently than a typical sklearn model \n",
    "simple_results = simple_model.fit()\n",
    "\n",
    "#Most beneficial aspect of using statsmodels!\n",
    "simple_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `statsmodels` (unlike `sklearn`) fitting a model returns a different kind of object, called a results object or a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(simple_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results object can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_results.predict([1,3400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_results.predict(sm.add_constant(cars['weight'])).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are just the same values as we fit the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_results.fittedvalues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(1500,5200)\n",
    "best_line = simple_results.params['const'] + simple_results.params['weight']*xx\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X['weight'],y, color='k', s=20)\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('mpg')\n",
    "ax.plot(xx, best_line, color='b', lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember our parameters... \n",
    "print(f'Constant: {simple_results.params.values[0]} \\nWeight:{simple_results.params.values[1]}  \\ntarget: {y.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we can say... \n",
    "\n",
    "HOLDING ALL ELSE CONSTANT: \n",
    "\n",
    "- A car with 0 weight gets 46.3 mpg. \n",
    "- For every increase of 1 weight unit (pound?), mpg worsens by 0.00767."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about multiple features?\n",
    "\n",
    "Assume we have $n$ features.\n",
    "\n",
    "Then the linear relationship we are assuming has the form\n",
    "$$\\large y = 1\\beta_0 + \\beta_1X_1 + ... \\beta_nX_n $$\n",
    "\n",
    "\n",
    "Again, we can calculate these parameters using our Matrix Equation which is the underlying formula that stats models uses to calculate our parameters as well. \n",
    "\n",
    "$$\\large \\hat{\\beta} = (X^TX)^{-1}(X^Ty)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is regression in more than one dimension?\n",
    "\n",
    "Let's plot `mpg` against `weight` and `acceleration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(cars['weight'], cars['acceleration'], cars['mpg'])\n",
    "ax.set_zlabel('mpg')\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('acceleration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression then finds the **plane** that minimizes SSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "X = sm.add_constant(cars[['weight','acceleration']])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using statsmodels again but we could calculate this agian using the matrix equation for OLS\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(cars['weight'], cars['acceleration'], cars['mpg']);\n",
    "ax.set_zlabel('mpg')\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('acceleration');\n",
    "\n",
    "xx1 = np.linspace(cars['weight'].min(), cars['weight'].max(), 20)\n",
    "xx2 = np.linspace(cars['acceleration'].min(), cars['acceleration'].max(), 20)\n",
    "xx1, xx2 = np.meshgrid(xx1, xx2)\n",
    "best_plane = (results.params['const'] +\n",
    "              results.params['weight']*xx1 +\n",
    "              results.params['acceleration']*xx2)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(xx1, xx2, best_plane, color='k', alpha=.6, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was all that other stuff in the summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ : The \"proportion of variance explained\"\n",
    "\n",
    "![](images/lin_alg_acc_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's for a moment refer to the variance of $y$ as \"the total sum of squares\"\n",
    "$$ SST = \\sum_i^N (y_i - \\bar{y})^2 $$\n",
    "\n",
    "Then we define $R^2$ as the percentage of that variance that has been \"captured\" by the regression line\n",
    "$$\n",
    "\\begin{align}\n",
    "    R^2 &:= 1 - \\frac{SSR}{SST} \\\\\n",
    "    &= 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "\\end{align}    \n",
    "$$\n",
    "\n",
    "All in allm the R-squared score measures the strength of the relationship between your model and the features. \n",
    "\n",
    "So would you want a higher ${R}^2 $ or a lower score when investigating your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values? Confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out we don't just want to solve an optimization problem.\n",
    "\n",
    "We want to make **statistical claims** about this linear relationship. In order to do that, we need to assume a distribution, so here we go.\n",
    "\n",
    "$$ y = \\beta X + \\epsilon $$\n",
    "where\n",
    "$$ \\epsilon \\sim Normal(0, \\sigma^2) $$\n",
    "\n",
    "Equivalently, we can write\n",
    "$$ y \\sim Normal(\\beta X, \\sigma^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some work, you can show the following:\n",
    " - Given a set of $N$ observations $\\{(x_i, y_i)\\}$, where $x_i$ is a $p$-dimensional vector, the maximum likelihood estimate for $\\beta$ is the same as the least-squares estimate: $$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    " so our model is $\\hat{y} = \\hat{\\beta}X$\n",
    " - The sampling distribution of $\\beta$ is: $$ \\hat{\\beta} \\sim Normal(\\beta, (X^T X)^{-1}\\sigma^2)$$\n",
    " - The unbiased estimate of $\\sigma^2$ is $$\\hat{\\sigma}^2 = \\frac{SSR}{N-p} = \\frac{\\sum_i^N(y_i - \\hat{y}_i)^2}{N-p}$$\n",
    "   - using this estimate, we can construct confidence bounds on our parameters using the student's t distribution: $$ \\hat{\\beta}_j \\, \\pm t_{N-p} \\sqrt{(X^T X)^{-1}_{jj}\\frac{SSR}{N-p}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all our null hypotheses have the form \"Does $\\beta_i = 0$ ?\"\n",
    "\n",
    "###### References:\n",
    "- [ISLR 3.1.2](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)\n",
    "- [here](http://home.cc.umanitoba.ca/~godwinrt/4042/material/part3.pdf)\n",
    "- [here](http://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "#add another feature to see how our p_values look\n",
    "X = sm.add_constant(cars[['weight','acceleration', 'displacement']])\n",
    "\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move into Assumptions IPYNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What assumptions must hold in order to make these statistical statements?\n",
    "Test your assumptions by **plotting your residuals**: $y_{actual} - y_{predicted}$\n",
    "\n",
    "It is useful to plot your residuals against various features as well as plotting residuals vs predicted y-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity: violated when you see nonlinear trends in your data /  residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "axs[0].scatter(X['weight'],y, color='k', s=20)\n",
    "axs[0].set_xlabel('weight')\n",
    "axs[0].set_ylabel('mpg')\n",
    "\n",
    "axs[1].scatter(X['acceleration'],y, color='k', s=20)\n",
    "axs[1].set_xlabel('acceleration')\n",
    "axs[1].set_ylabel('mpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1, figsize=(8,20))\n",
    "\n",
    "axs[0].scatter(X['weight'], results.resid)\n",
    "axs[0].hlines(0,\n",
    "              X['weight'].min(), \n",
    "              X['weight'].max(), \n",
    "              'k', linestyle='dashed')\n",
    "axs[0].set_xlabel('weight')\n",
    "axs[0].set_ylabel('residuals');\n",
    "\n",
    "axs[1].scatter(X['acceleration'], results.resid)\n",
    "axs[1].hlines(0,\n",
    "              X['acceleration'].min(), \n",
    "              X['acceleration'].max(), \n",
    "              'k', linestyle='dashed')\n",
    "axs[1].set_xlabel('acceleration')\n",
    "axs[1].set_ylabel('residuals');\n",
    "\n",
    "axs[2].scatter(results.fittedvalues, results.resid)\n",
    "axs[2].hlines(0,\n",
    "              results.fittedvalues.min(), \n",
    "              results.fittedvalues.max(),\n",
    "              'k', linestyle='dashed')\n",
    "axs[2].set_xlabel('predicted mpg')\n",
    "axs[2].set_ylabel('residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time we want to work with the studentized residuals: divide the residual by the estimate of the standard deviation of the residuals (which turns out to depend on $X$, even though $\\epsilon$ above does not depend on $X$. More details [here](https://en.wikipedia.org/wiki/Studentized_residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stud_resids = results.outlier_test()['student_resid']\n",
    "\n",
    "fig, axs = plt.subplots(3,1, figsize=(8,20))\n",
    "\n",
    "axs[0].scatter(X['weight'], stud_resids)\n",
    "axs[0].hlines(0,\n",
    "              X['weight'].min(), \n",
    "              X['weight'].max(), \n",
    "              'k', linestyle='dashed')\n",
    "axs[0].set_xlabel('weight')\n",
    "axs[0].set_ylabel('residuals');\n",
    "\n",
    "axs[1].scatter(X['acceleration'], stud_resids)\n",
    "axs[1].hlines(0,\n",
    "              X['acceleration'].min(), \n",
    "              X['acceleration'].max(), \n",
    "              'k', linestyle='dashed')\n",
    "axs[1].set_xlabel('acceleration')\n",
    "axs[1].set_ylabel('residuals');\n",
    "\n",
    "axs[2].scatter(results.fittedvalues, stud_resids)\n",
    "axs[2].hlines(0,\n",
    "              results.fittedvalues.min(), \n",
    "              results.fittedvalues.max(),\n",
    "              'k', linestyle='dashed')\n",
    "axs[2].set_xlabel('predicted mpg')\n",
    "axs[2].set_ylabel('residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity: violated when the variance of your residuals isn't constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches here. First, looking at the residuals about, we need the variance to be fairly constant.\n",
    "\n",
    "Second, there's a hypothesis test for this, the Goldfeld-Quandt test. It divides your data into two subsets and returns the p-value under the null hypothesis of homoscedasticity.\n",
    "\n",
    "How you split the data in half is arbitrary. Usually, you pick a feature and sort the data by that feature, then split on the median value of that feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in `statsmodels.stats.diagnostic.het_goldfeldquandt`, specify the column to use for sorting & splitting data with the parameter `idx`.\n",
    "  - if you leave it blank, the data will be split just by the order it appears in your data set, which is WAY TOO ARBITRARY\n",
    "- the default alternative hypothesis is \"increasing\", meaning it does a one-sided test. pick `alternative='two-sided'` to do a two-tailed test, unless you have a very good reason for assuming the direction of variation change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value, _ = sm.stats.diagnostic.het_goldfeldquandt(y, X, idx=1, alternative='two-sided')\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty small p-val. Reject the null. Houston, we have heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality: violated when the residuals are not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools for this are the [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) and the [Jarque-Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test) (the null hypothesis for the JB test is \"the residuals come from a distribution with zero skewness and zero excess kurtosis\", which are both properties of the normal distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QQ plot puts the *empirical sample quantiles* of the residuals on the y-axis and the *theoretical normal distribution quantiles* on the x-axis. \n",
    "\n",
    "For example, say one of the studentized residuals has the value $-1.54$. And perhaps we found that $3.51\\%$ of the studentized residuals had a value of $-1.54$ or lower. However, if they had been drawn from a normal distribution, we would expect $3.51\\%$ of the residuals to fall at or below the value $-1.81$ instead, so this would show up on the QQ plot as the point $(-1.81,-1.54)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sm.graphics.qqplot(stud_resids, line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity: strictly violated when one feature is a linear combination of others, loosely violated when one feature is highly correlated with others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.plot.scatter('weight','displacement');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "X = sm.add_constant(cars[['weight','acceleration']])\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cars['mpg']\n",
    "X = sm.add_constant(cars[['weight','acceleration', 'displacement']])\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One measure of multicollinearity: the Variance Inflation Factor. Regress feature $X_k$ on all the rest of the features and get the $R^2$ value for that fit.\n",
    "\n",
    "$$VIF_k = \\frac{1}{1 - R_k^2}$$\n",
    "\n",
    "Rule of thumb: collinearity is high if VIF > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variance_inflation_factor(X.values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close call."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
