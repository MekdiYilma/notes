{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Assumptions and Inference\n",
    "\n",
    "Skyler English, Land Belanky's lecture materials\n",
    "\n",
    "\n",
    "In this lecture we think about linear regression as a **statistical model**.\n",
    "\n",
    "Let's see what additional assumptions and checks we must make to use linear regression in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lecture you'll need to install the [py-glm](https://github.com/madrury/py-glm) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/madrury/py-glm.git\n",
      "  Cloning https://github.com/madrury/py-glm.git to /private/var/folders/np/ql4f4__90hq9xmh760b2mx100000gp/T/pip-req-build-bjb455fw\n",
      "  Running command git clone -q https://github.com/madrury/py-glm.git /private/var/folders/np/ql4f4__90hq9xmh760b2mx100000gp/T/pip-req-build-bjb455fw\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from py-glm==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from py-glm==0.0.1) (1.6.1)\n",
      "Requirement already satisfied: patsy in /opt/anaconda3/lib/python3.7/site-packages (from py-glm==0.0.1) (0.5.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from patsy->py-glm==0.0.1) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/madrury/py-glm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from glm.glm import GLM\n",
    "from glm.families import Gaussian\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Regression Assumptions\n",
    "\n",
    "Linear regression is often presented along with a list of assumptions.\n",
    "\n",
    "#### The Linear Regression Assumptions:\n",
    "  - Linearity\n",
    "  - Linear Independence of Features\n",
    "  - Independence of Data\n",
    "  - Homoscedasticity\n",
    "  - Normal Distribution of Errors\n",
    "  \n",
    "These assumptions are not all needed, nor must they all be checked, for all applications of linear regression.  Indeed, when used as a **predictive** model, only the first is really needed (though some of the others bear consequences when blatantly violated).\n",
    "\n",
    "We will take these assumptions in turn, discussing what the assumption means and what can happen when it is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linearity Assumption\n",
    "\n",
    "Recall that the \"linear\" in the name \"linear regression\" does **not** mean that the model is attempting to fit a linear function of the **raw features**.  Instead, it means that the model is a linear function of the **transformed features**.\n",
    "\n",
    "So, the **linearity assumption** in regression says that the **model fits the data well**.  I.e., that $y$ is a **linear function of the (possibly transformed) features in the model**.  Of course, $y$ cannot be an exact linear function, because there is some randomness involved, but this should be the **only reason** that $y$ is not a linear function of the features.\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 f_1 + \\beta_2 f_2 + \\cdots + \\beta_k f_k + \\text{noise} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals vs. feature plots we looked at in the linear regression day can be used to assess this asumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stats.uniform(-2, 6).rvs(500)\n",
    "y = 1 + x - 0.5 * x * x + stats.norm(0, 1).rvs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.scatter(x, y, color=\"grey\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this data looks like it fits some kind of relationship like:\n",
    "    \n",
    "$$ y \\mid x = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\text{noise} $$\n",
    "\n",
    "We just don't know what the numbers $\\beta_0, \\beta_1, \\beta_2$ are.  This is a **description of how the data was generated**, and so meets our definition of a statistical model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(ax, x, y, y_hat, n_bins=50):\n",
    "    residuals = y - y_hat\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "    ax.scatter(x, residuals, color=\"grey\", alpha=0.5)\n",
    "    ax.set_ylabel(\"Residuals ($y - \\hat y$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a quadratic transformation of the predictor, we can fit this model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((len(x), 2))\n",
    "X[:, 0] = x\n",
    "X[:, 1] = x**2\n",
    "\n",
    "quadratic_model = LinearRegression()\n",
    "quadratic_model.fit(X, y)\n",
    "\n",
    "y_hat = quadratic_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.scatter(x, y, color=\"grey\", label='y_true')\n",
    "ax.scatter(x, y_hat, color=\"k\", label='y_hat')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plots show no obvious pattern, which is a hint that we specified our statistical model correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "residual_plot(ax, x, y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had instead used the model\n",
    "\n",
    "$$ y \\mid x = \\beta_0 + \\beta_1 x + \\text{error} $$\n",
    "\n",
    "we would end up with a residual plot containing an obvious patter, which is a hint that we have **misspecified** our statistical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "y_hat = linear_model.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.scatter(x, y, color=\"grey\", label='y_true')\n",
    "ax.scatter(x, y_hat, color=\"k\", label='y_hat')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "residual_plot(ax, x, y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly do not have a band evenly spaced around the center line. This type of plot indicates that something is missing from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linearity assumption for linear regression is essentially the desire that **the model we fit to our data is well specified, i.e. it contains the true (or close to the true) process that generated the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Independence and Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a unique solution, the features must be **linearly independent** from one another (i.e., not multicollinear).\n",
    "\n",
    "If this is not true, then there are an infinite number of possible solutions. This doesn't prevent choosing a solution and using it for prediction, but it is problematic when defining an inferential model.\n",
    "\n",
    "In real life, this won't happen unless the features are derived from a common source, or there are few data points. More commonly, one feature is strongly correlated with a linear combinations of other features. In this case there will be a single solution, but the values for coefficients $\\beta$ will be have a high variance base on the individual sample.\n",
    "\n",
    "Check for this using the [variance_inflation_factor.](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv')\n",
    "pd.plotting.scatter_matrix(cars, figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Independence Assumption and Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consistency of linear regression addresses the effect of gathering more **independent** data.  We would like the regression to become more and more accurate as we feed it more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Accurate with reference to what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is this:  if our data were **truly** generated from a random process like this:\n",
    "\n",
    "$$ y \\mid X \\sim \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\text{noise} $$\n",
    "\n",
    "Then the coefficients we get from fitting a linear regression can be considered as **estimates of the true parameters**:\n",
    "\n",
    "$$ \\underbrace{\\hat \\beta_k}_{\\text{The Coefficients from the Fit Regression}} \\underbrace{\\approx}_{\\text{Are approximations of}} \\underbrace{\\beta_k}_{\\text{The True Parameters}} $$\n",
    "\n",
    "**Consistency** is the statement that this approximation gets **better** as we collect more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regressions(ax, n, num=50):\n",
    "    for k in range(num):\n",
    "        x = stats.uniform(-1, 2).rvs(n)\n",
    "        y = 2*x + stats.norm(0.0, 1.0).rvs(n)\n",
    "        if k == 0:\n",
    "            ax.scatter(x, y, color=\"grey\")\n",
    "        model = LinearRegression()\n",
    "        model.fit(x.reshape(-1, 1), y)\n",
    "        t = np.linspace(-1, 1, num=250)\n",
    "        ax.plot(t, model.predict(t.reshape(-1, 1)), color=\"grey\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [5, 10, 25, 50, 100]\n",
    "\n",
    "fig, axs = plt.subplots(len(ns), figsize=(12, 10), sharey=True)\n",
    "for ax, n in zip(axs, ns):\n",
    "    plot_regressions(ax, n)\n",
    "    ax.plot([-1, 1], [-2, 2], color=\"black\", linewidth=3, label=\"True Line\")\n",
    "    ax.set_title(f\"Linear Regressions: N = {n}\")\n",
    "    ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is consistent as long as the **data** are independent.  That is, each sampled data point $(X, Y)$ must be independent from all other data points.  When data are independent, each additional data point gives us **more information**, which allows us to better estimate the regression line.\n",
    "\n",
    "As an example of how this assumption can be violated, consider data that is generated from an iterative procedure like:\n",
    "\n",
    "$$ y_k = y_{k-1} + \\text{error} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walks = 4\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(12, 6))\n",
    "\n",
    "x = np.arange(250)\n",
    "for ax in axs.flatten():\n",
    "    y = np.random.normal(size=250)\n",
    "    walk = np.cumsum(y)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), walk)\n",
    "    t = np.linspace(0, 250, num=250)\n",
    "    ax.scatter(x, walk)\n",
    "    ax.plot(t, model.predict(t.reshape(-1, 1)), color=\"k\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of data is often called **time series** data, and it is *not* independently sampled. We'll talk more about time-series data later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality of Errors: Distribution of Parameter Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **most information we could possibly have** about the probabilistic nature of linear regression is to know the **distribution of the parameter estimates / coefficients**.\n",
    "\n",
    "### What This Means\n",
    "\n",
    "It's important to understand what this means.\n",
    "\n",
    "The parameter estimates / coefficients from a fit linear regression are **random quantities**.  If we have different sample of data from our population, fitting a linear regression will give us slightly different results. So...\n",
    "\n",
    "$$ \\hat \\beta \\text{ is a random variable!} $$\n",
    "\n",
    "**Note:** We have already observed that when the data are independent, more data will cause these different results to converge to the same answer.  This is consistency!\n",
    "\n",
    "The most we could ask for is a complete description of the distribution of the coefficients.  This is available in some circumstances, but it **requires very stringent assumptions**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_regression(n, noise=stats.norm(0, 1)):\n",
    "    x1 = stats.uniform(-1, 2).rvs(n)\n",
    "    x2 = stats.uniform(-0.5, 1.0).rvs(n)\n",
    "    x3 = stats.uniform(-2, 4).rvs(n)\n",
    "    y = 1 + x1 - 2*x2 + noise.rvs(n)\n",
    "    X = np.empty(shape=(n, 3))\n",
    "    X[:, 0] = x1; X[:, 1] = x2; X[:, 2] = x3\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return X, y, model.coef_\n",
    "\n",
    "def simulate_many_coeffs(n, n_sim=5000, noise=stats.norm(0, 1)):\n",
    "    coeffs = np.empty(shape=(n_sim, 3))\n",
    "    for i in range(n_sim):\n",
    "        _, _, coeffs[i, :] = run_one_regression(n, noise=noise)\n",
    "    return coeffs\n",
    "\n",
    "def plot_paramter_estimate_histograms(axs, coeffs):\n",
    "    axs[0].hist(coeffs[:, 0], bins=100, color=\"grey\", alpha=0.5, \n",
    "                density=True, label=\"Parameter Estimates: Observed\")\n",
    "    axs[0].set_title(r\"Distribution of $\\hat \\beta_1$\")\n",
    "    axs[0].axvline(1, color=\"black\", label=r\"True Parameter $\\beta_1$\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].hist(coeffs[:, 1], bins=100, color=\"grey\", alpha=0.5,\n",
    "                density=True, label=\"Parameter Estimates: Observed\")\n",
    "    axs[1].set_title(r\"Distribution of $\\hat \\beta_2$\")\n",
    "    axs[1].axvline(-2, color=\"black\", label=r\"True Parameter $\\beta_2$\")\n",
    "    axs[1].legend()\n",
    "\n",
    "\n",
    "    axs[2].hist(coeffs[:, 2], bins=100, color=\"grey\", alpha=0.5,\n",
    "                density=True, label=\"Parameter Estimates: Observed\")\n",
    "    axs[2].set_title(r\"Distribution of $\\hat \\beta_3$\")\n",
    "    axs[2].axvline(0, color=\"black\", label=r\"True Parameter $\\beta_3$\")\n",
    "    axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = run_one_regression(100, noise=stats.norm(0, 1))\n",
    "coeffs = simulate_many_coeffs(100, noise=stats.norm(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(12, 6), sharex=True)\n",
    "plot_paramter_estimate_histograms(axs, coeffs)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual assumption / consequence is this:\n",
    "\n",
    "**When the noise is assumed to be normally distributed (centered at zero, variance does not depend on x), then the parameter estimates are ALSO normally distributed**.\n",
    "\n",
    "We can write that assumption in rigorous notation as:\n",
    "\n",
    "$$ y \\mid x \\sim \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\text{Normal}(0, \\sigma) $$\n",
    "\n",
    "Sigma here must **not depend on $x$**.  This specific property is called **homoscedasticity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-homoscedastic (i.e., heteroscedastic)** data has a conditional variance that changes as the feature changes.  For example, if we generate data from a process like\n",
    "\n",
    "$$ y \\mid x \\sim \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\text{Normal}(0, x) $$\n",
    "\n",
    "\n",
    "![](images/homo_hetro.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=1.0, high=5.0, size=500)\n",
    "y2 = 1 + 5*x + np.random.normal(scale=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.scatter(x, y2, color=\"grey\", alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic of **Generalized Linear Models** addresses statistical inference  when data is heteroscedastic.  \n",
    "\n",
    "If you see this behavior - usually some form of tranformation (like a [log transformation](http://onlinestatbook.com/2/transformations/log.html)) of the feature or data can be used to make it more homoscedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example: Applications to Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common application of all these ideas is in the hypothesis testing of regression coefficuents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our model of insect wing sizes to answer an inferential question: \n",
    "\n",
    "> Does continent have any effect on wing size?\n",
    "\n",
    "To accomplish this, we need to pre-specify a regression model relating the wing size of insects to all other relevant measurement.  We've already done this work, so I'll just remind everyone of our final model:\n",
    "\n",
    "$$ \\text{Wing Span} \\approx a + b \\times \\text{Latitude} + c \\times \\text{Latitude}^2 +  d \\times \\text{Sex} + e \\times \\text{Continent} $$\n",
    "\n",
    "Then the new number $e$ captures the effect of the insect being found on the continent labeled $1$.  **If $e$ is zero, or close to zero, then continent has no effect on wing span**.\n",
    "\n",
    "Let's reload the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insects = pd.read_csv('./data/insects.csv', sep='\\t')\n",
    "insects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insects_model_quad_with_continent = GLM(family=Gaussian())\n",
    "insects_model_quad_with_continent.fit(\n",
    "    insects,\n",
    "    formula='wingsize ~ latitude + I(latitude**2) + sex + continent')\n",
    "insects_model_quad_with_continent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those `standard errors` are what we computed with our $\\sigma (X^t X)^{-1}$ calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression p-values\n",
    "\n",
    "Recall that a **p-value** describes the amount of surprise that we feel when observing data, given that we are taking a skeptical stance on how that data is generated.\n",
    "\n",
    "$$ P(\\text{Observing A Statistic Equal or More Extreme than Actual} \\mid H_0) $$\n",
    "\n",
    "So to make sense of the p-values here, we need to describe the **null hypothesis** and the **statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **null hypothesis** in this case is that each value of $y$ was generated by sampling from a relationship like:\n",
    "\n",
    "$$ y \\mid x \\sim N(a + b \\times \\text{Latitude} + c \\times \\text{Latitude}^2 +  d \\times \\text{Sex} + 0 \\times \\text{Continent}, \\sigma) $$\n",
    "\n",
    "Notice that in the null hypothesis, we are assuming **the effect of continent is zero, so we would be surprised to find it very non-zero**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this null hypothesis, the **p-value** reported in linear regression is\n",
    "\n",
    "$$ P(\\text{Observe a Parameter Estimate for Continent More Extreme Than the Fit Model} \\mid H_0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the p-values from our fit regression model with the `_p_value` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = insects_model_quad_with_continent.X_names\n",
    "p_values = insects_model_quad_with_continent.p_values_\n",
    "\n",
    "print(\"Predictor            p-value\")\n",
    "print(\"-\"*30)\n",
    "for name, p in zip(names, p_values):\n",
    "    print(f\"{name :<20} {p:2.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value for continent was $0.25$, so it's quite likely that we would observe a parameter estimate equally or more extreme than we actually did, even if the true effect was zero.  This is consistent with what we saw in our picture, we do **not** believe that `continent` has an effect much different than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Final Word on Assumptions\n",
    "\n",
    "There is a subtlety to our hypothesis test above.\n",
    "\n",
    "The Null Hypothesis can be **false in more than one way**:\n",
    "\n",
    "  - It could be, as we discussed already, that the true effect of `continent` is *not-zero*.  This is the situation we are trying to detect.\n",
    "  - It could be that the value of $y$ was **not created by sampling from a normal distribution centered at the predicted value**.\n",
    "  \n",
    "For example, it could be that the value of $y$ was created by sampling from some other distribution:\n",
    "\n",
    "$$ y \\mid x \\sim \\text{Exponential}(a + b \\times \\text{Latitude} + c \\times \\text{Latitude}^2 +  d \\times \\text{Sex} + e \\times \\text{Continent}) $$\n",
    "\n",
    "In this case we **cannot deduce that $e \\neq 0$, even if we reject the null hypothesis**.\n",
    "\n",
    "**The calculation of p-values in a linear regression depends on ALL THE ASSUMPTIONS ABOVE being true**.  If **any** of them are false, the calculation of the p-value is **not valid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
