{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "### Success Criteria\n",
    "Today I will be successful if I can ...\n",
    "\n",
    "1. Describe the difference between likelihood and probability. \n",
    "2. Define Maximum Likelihood Estimation\n",
    "3. Calculate the MLE for a poisson distribution and a normal distribution. \n",
    "4. Be patient with myself because this stuff ain't easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "secret_number = np.random.uniform(0,10)\n",
    "\n",
    "\n",
    "# For presentation purposes with dark jupyter\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import matplotlib as mpl\n",
    "COLOR = 'blue'\n",
    "mpl.rcParams['text.color'] = COLOR\n",
    "mpl.rcParams['axes.labelcolor'] = COLOR\n",
    "mpl.rcParams['xtick.color'] = COLOR\n",
    "mpl.rcParams['ytick.color'] = COLOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability v. Likelihood\n",
    "\n",
    "### Probabability\n",
    "\n",
    "- Know something about underlying reality or parameter distributions. \n",
    "\n",
    "When this is true, we can ask ourselves, \"What is the chance of observing this particular data or sample given a specific model or population?\" \n",
    "\n",
    "For example, If we know the mean of female heights are normally distributed with a mean of 63.7 and a standard deviation of 3 then **what is the chance of observing a woman with a height bewtween 63 and 65 inches?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm(ax, mean, std, **kwargs):\n",
    "    dist = stats.norm(mean, std)\n",
    "    x = np.linspace(mean - std*3, mean + std*3, 251)\n",
    "    ax.plot(x, dist.pdf(x), **kwargs)\n",
    "    return x, dist\n",
    "   \n",
    "def plot_dist_with_datapoint(ax, mean, std, data_val, **kwargs):\n",
    "    plot_norm(ax, mean, std)\n",
    "    ax.scatter(data_val, 0, color = 'Purple', s = 100, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 63.7\n",
    "std = 3\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "x, dist = plot_norm(ax, mean, std)\n",
    "\n",
    "ax.fill_between(x, dist.pdf(x), where=((x>=63)&(x<=65)), color = 'r', alpha = 0.5, \\\n",
    "                label = f'Probability = {round(dist.cdf(65)-dist.cdf(63),2)}')\n",
    "ax.set_title('Chance of Height being >= 63 and <=65')\n",
    "ax.set_xlabel('Heights')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we would say or write this is... \n",
    "\n",
    "$$P(63<=H<=65 \\mid (\\mu = 63.7 \\space and \\space \\sigma = 3))  = 0.26$$\n",
    "\n",
    "The probability that a female's height is between 63 and 65 inches given our mean is 63.7 and our standard deviation is 3 is 0.26. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With probability we can continuously **change the first half of this sentence** depending on what we are interested in. \n",
    "\n",
    "Like:\n",
    "\n",
    "$$P(H > 67 \\mid (\\mu = 63.7 \\space and \\space \\sigma = 3))$$\n",
    "\n",
    "What is the probability that a female's height is greater than 67 inches given our mean is 63.7 and our standard deviation is 3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,5))\n",
    "\n",
    "x, dist = plot_norm(ax, mean, std)\n",
    "\n",
    "ax.fill_between(x, dist.pdf(x), where=(x>=67), color = 'r', alpha = 0.5, \\\n",
    "                label = f'Probability = {round(1 - dist.cdf(67),2)}')\n",
    "ax.set_title('Chance of Height being >= 67 ')\n",
    "ax.set_xlabel('Heights')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "- Don't know much about underlying reality or parameter distributions. \n",
    "- We DO know our data\n",
    "\n",
    "\n",
    "Likelihood is kinda like the reverse of probability. \n",
    "\n",
    "#### What is the chance or likelihood that a given reality or model is true given our observation data? \n",
    "\n",
    "$$L({Distribution \\space Parameter} \\mid {Data})$$\n",
    "\n",
    "\n",
    "\n",
    "The normal distribution has probability density function:\n",
    "\n",
    "$$ \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\big(\\frac{x - \\mu}{\\sigma}\\big)^2} $$\n",
    "\n",
    "Evaluating this function gives us a *likelihood* not the probability. \n",
    "\n",
    "<!-- Considering that the exact evaluation of this function gives us a value other than zero tells us that it is not a probability  \n",
    " -->\n",
    " \n",
    " #### Example:\n",
    "\n",
    "We poll a rondom Female who is 67 inches tall.\n",
    " \n",
    "\n",
    "With Likelihood, it assumes you already have information about your heights available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this visual, we have a distribution to start with... \n",
    "\n",
    "fig,ax = plt.subplots(figsize = (8,4))\n",
    "plot_dist_with_datapoint(ax, mean, std, 67, label = '67 Inches')\n",
    "ax.set_title('Height of One Female')\n",
    "ax.set_xlabel('Heights')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of measuring a woman this tall is the point on the curve where the datapoint lies, this is the evaluation of our PDF function mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,4))\n",
    "plot_dist_with_datapoint(ax, mean, std, 67)\n",
    "ax.axvline(67, ls = '--', c = 'purple', ymax = 0.5)\n",
    "ax.axhline(dist.pdf(67), ls = '--', c = 'purple', xmax = 0.66)\n",
    "ax.scatter(67, dist.pdf(67), marker = 'X', color = 'purple', s = 200, \n",
    "          label = f'Likelihood is {round(dist.pdf(67),3)}')\n",
    "ax.set_title('Height of One Female')\n",
    "ax.set_xlabel('Heights')\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood is said in this way\n",
    "\n",
    "$$L((\\mu = 63.7 \\space and \\space \\sigma = 3) \\mid H = 67) $$\n",
    "\n",
    "The likelihood of having a meaen of 63.7 and standard deviation of 3 given that we have a 67 in tall female is 0.073. \n",
    "\n",
    "If we shift the distribution over, so that the mean was 67 grams, we see we have increased our likelihood from 0.073 to 0.133. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,1, figsize = (8,8))\n",
    "ax = axs[0]\n",
    "ax1 = axs[1]\n",
    "\n",
    "plot_dist_with_datapoint(ax, mean, std, 67)\n",
    "ax.set_xlim(55, 72.55)\n",
    "ax.axvline(67, ls = '--', c = 'purple', ymax = 0.5)\n",
    "ax.axhline(dist.pdf(67), ls = '--', c = 'purple', xmax = 0.66)\n",
    "ax.scatter(67, dist.pdf(67), marker = 'X', color = 'purple', s = 200, \n",
    "          label = f'Likelihood is {round(dist.pdf(67),3)}')\n",
    "ax.legend(loc = 'upper left')\n",
    "\n",
    "plot_dist_with_datapoint(ax1, 67, std, 67)\n",
    "ax1.set_xlim(55, 72.55)\n",
    "ax1.axvline(67, ls = '--', c = 'purple', ymax = 0.95)\n",
    "new_dist = stats.norm(67, 3)\n",
    "ax1.axhline(new_dist.pdf(67), ls = '--', c = 'purple', xmax = 0.7)\n",
    "ax1.scatter(67, new_dist.pdf(67), marker = 'X', color = 'purple', s = 200, \n",
    "          label = f'Likelihood: {round(new_dist.pdf(67),3)}')\n",
    "ax1.legend(loc ='center left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we typically have more than one data point, that is where the MLE will come into play, for now, make sure you understand the differences between Likelihood and Probability.  \n",
    "\n",
    "#### In Summary\n",
    "\n",
    "Probabilities are areas under a fixed distribution\n",
    "\n",
    "$$P(data \\mid distribution) $$\n",
    "\n",
    "\n",
    "Likelihoods are the y axis values for fixed data points with distributions that can be moved...\n",
    "\n",
    "$$L(distribution \\mid data) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "The **maximum likelihood method** is the gold standard method for fitting statistical models to data.\n",
    "\n",
    "Our goal is to make the following philosophy of model fitting precise\n",
    "\n",
    "> The model should be the random variable that is *most likely* to generate the data.\n",
    "\n",
    "### Poisson Distribution MLE Example\n",
    "\n",
    "Lets first review the parameters involved with a poisson distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(1,6)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(0, 10)\n",
    "_ = [ax.scatter(x, stats.poisson(mu=l).pmf(x),\n",
    "           s=50, zorder=2) for l in lambdas]\n",
    "_ = [ax.plot(x, stats.poisson(mu=l).pmf(x),\n",
    "           zorder=2, label=f\"lambda: {l}\") for l in lambdas]\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples might be the best way to learn MLE... Let's dive in. \n",
    "\n",
    "I am interested in the number of people every week who view my LinkedIn profile. (this should follow a poisson distribution) \n",
    "\n",
    "I go out and get some data, we assume it is iid and random.  \n",
    "\n",
    "$$ x_1 ... x_{50} $$ \n",
    "\n",
    "\n",
    "Lambda $\\lambda$ represents the mean that we would be looking for in this case and I know that each $X_i \\sim Pois(\\lambda)$\n",
    "\n",
    "\n",
    "The Poisson distribution has pmf: \n",
    "$$\\large{\\frac {\\lambda ^{x}e^{-\\lambda }}{x!}}$$\n",
    "\n",
    "\n",
    "Just like probabilities, we can multiply likelihoods together to get the joint likelihood if we assume all of the data points are independent. So, since we have many data points $x_0, x_1, x_2, \\dots, x_{50}$, then the joint likelihood is:\n",
    "\n",
    "$$ \\large L(\\lambda \\mid x_1 ... x_{50}) = \\prod_{i=1}^{50} {\\frac{\\lambda ^{x_i}e^{-\\lambda }}{x_i!}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pause here and make sure we are comfortable so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take a look at those data points over a specific lambda\n",
    "\n",
    "#Generate fake data using our secret lambda\n",
    "data = stats.poisson(mu=secret_number).rvs(50)\n",
    "\n",
    "# create a list of likelihood of each of those outcomes, right now we are guessing a lambda\n",
    "likelihoods = stats.poisson(mu = 7).pmf(data)\n",
    "\n",
    "# print out product of the likelihoods (joint probability assuming independence)\n",
    "print(f'The likelihood that our data came from a Poisson Distribution with a lambda of 7: \\n{np.prod(likelihoods)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at joint likelihoods over varying lambdas... It looks like this product thing works.\n",
    "lambdas = np.linspace(0.1, 10, 100)\n",
    "\n",
    "likelihoods = [stats.poisson(mu = lam).pmf(data) for lam in lambdas]\n",
    "join_likes = [np.prod(like) for like in likelihoods]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(lambdas, join_likes)\n",
    "ax.set_ylabel('Joint Likelihoods')\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_title('MLE of lambda parameter: {}'.format(round(lambdas[np.argmax(join_likes)],2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy right? We see where our Joint likelihood is the highest and we call it a day?\n",
    "\n",
    "Not quite...\n",
    "\n",
    "So in theorey... we could stop here and move on, except, when we are searching for maximum likelihood, it is rarely this simple and in various areas we are searching for multiple parameters. \n",
    "\n",
    "So mathematically, we can actually consider taking the derivative of our joint probability function and set that to equal zero. This will give us our maximum likelihood. \n",
    "\n",
    "Why not take the derivative of a large product over many different parameters?\n",
    "\n",
    "*note: we wont worry about the derivatives today but I want you to see motivation for log likelihood*\n",
    "\n",
    "#### Log Likelihood\n",
    "\n",
    "It turns out that it's typically easier to work with likelihoods if we use the log likelihood instead. Our goal is to **maximize** the likelihood. We want to find the easiest thing possible to maximize, so can we instead maximize the log likelihood?\n",
    "\n",
    "$$\\large \\ln ( \\prod_{i=1}^{50} {\\frac{\\lambda ^{x_i}e^{-\\lambda }}{x_i!}} ) $$\n",
    "\n",
    "What this does is change our multiplication to addition because taking the log of a product gives us the sum of the log of the indiviudals. \n",
    "\n",
    "We are using a log rule. $\\ln(m * n) = \\ln m + \\ln n$.\n",
    "\n",
    "$\\ln x = y$ is equivalent to $ x = e^y $\n",
    "\n",
    "So now we get this...\n",
    "\n",
    "$$\\large\\sum_{i=1}^{50} \\ln\\left({\\frac{\\lambda ^{x_i}e^{-\\lambda }}{x_i!}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "# Still have a distribution \n",
    "poisson = stats.poisson(4)\n",
    "\n",
    "# find the likelihoods of all data points using the pmf (numpy ndarray)\n",
    "likelihoods = [poisson.pmf(datum) for datum in data]\n",
    "\n",
    "# we are finding the logs of the likelihoods\n",
    "log_likelihoods = np.log(likelihoods)\n",
    "\n",
    "# summing up the log likelihoods\n",
    "log_likelihood_sum = np.sum(log_likelihoods)\n",
    "\n",
    "print('The log likelihood that  our data came from a Poisson Distribution with a lambda of 4:')\n",
    "print(log_likelihood_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's functionize this \n",
    "\n",
    "def log_likelihood_poisson(lam, data):\n",
    "    '''\n",
    "    Calculates the log likelihood of the data given that the data is distributed as Poisson(lambda = scale)\n",
    "    \n",
    "    Parameters:\n",
    "    scale: a scale parameter corresponding to 'lambda' in the pdf definition.\n",
    "    data: the observed data\n",
    "    \n",
    "    Returns:\n",
    "    float: The log likelihood of the data given the distribution.\n",
    "    '''\n",
    "    # instantiate poisson as a poisson distriution with shape supplied in the function call.\n",
    "    poisson = stats.poisson(lam)\n",
    "    \n",
    "    # find the likelihoods of all data points using the pmf (numpy ndarray)\n",
    "    likelihoods = [poisson.pmf(datum) for datum in data] #poisson.pmf(data) will also work but this comprehension is more visual code\n",
    "    \n",
    "    # we are finding the logs of the likelihoods\n",
    "    log_likelihoods = np.log(likelihoods)\n",
    "    \n",
    "    # summing up the log likelihoods\n",
    "    log_likelihood_sum = np.sum(log_likelihoods)\n",
    "    return log_likelihood_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the monotonic increase\n",
    "\n",
    "# Create an empty array to hold the log likelihoods\n",
    "poisson_log_likelihood = []\n",
    "\n",
    "# create a numpy ndarray from 0.1 to 10 with 100 points along the line.\n",
    "search_space = np.linspace(0.1, 20, 100)\n",
    "\n",
    "# loop through our search space and append the sum of log likelihoods of the data given model \n",
    "# Poisson(lambda) to the log_likelihood array \n",
    "for lam in search_space:\n",
    "    poisson_log_likelihood.append(log_likelihood_poisson(lam, data))\n",
    "\n",
    "# get the index of the maximum log_likelihood value\n",
    "max_idx = np.argmax(poisson_log_likelihood)    \n",
    "\n",
    "# create a variable equal to the search space corresponding to the maximum log likelihood value.\n",
    "mle = search_space[max_idx]\n",
    "highest_log_pois = poisson_log_likelihood[max_idx]\n",
    "# create the plot\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(search_space, poisson_log_likelihood)\n",
    "ax.axhline(highest_log_pois, ls = '--',c='r', label = 'Derivative = 0')\n",
    "ax.set_title('MLE of shape parameter: {}'.format(round(search_space[max_idx],2)))\n",
    "ax.set_xlabel(\"Lambda\")\n",
    "ax.set_ylabel(\"log likelihood\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#had data\n",
    "#guess the best mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In practice, we use an algorithm like **gradient descent** to find a good approximation to the maximal parameters, this process is called optimization.\n",
    "\n",
    "For now, searching is good engough across various parameters and simply finding the max index is good engough. You do have the ability to calulate a parameter by setting your derivative to zero and solving but we don't need to do this now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So lets review steps we did above.\n",
    "\n",
    "1. Id a model that might fit... (normal, poisson, binomial...)\n",
    "2. Write down the density/ or mass functions of all the random variables in the model. (pdf or pmf)\n",
    "3. Write code to compute the log likelihood function of the model given the data\n",
    "    - pdf of each data point\n",
    "    - log of each pdf\n",
    "    - sum the logs\n",
    "\n",
    "\n",
    "4. Create a large engough searchspace to find a good estimate for parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution Example:\n",
    "\n",
    "The normal Distribution has two parameters we need to consider... For now, we will just see about estimating one and assuming the second. \n",
    "\n",
    "I have been considering this female height example for a while and I am fairly certain that the population mean is 63.7. What I am not sure of, and why I have been guessing is the standard deviation. \n",
    "\n",
    "Let us use MLE to find our Female Heights Estimated Standard Deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "#another secret value to help us generate data. \n",
    "secret_std = np.random.uniform(0,5)\n",
    "\n",
    "height_data = stats.norm(63.7, secret_std).rvs(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Id a model that might fit... (normal, poisson, binomial...)\n",
    "\n",
    "Normal and Mean of 63.7\n",
    "\n",
    "### Step 2. Write down the density/ or mass functions of all the random variables in the model. (pdf or pmf)\n",
    "\n",
    "Remember that our likelihoods can be calculated as the evaluation of any y value on our pdf or pmf. \n",
    "\n",
    "The normal distribution has probability density function:\n",
    "\n",
    "$$ \\large\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\big(\\frac{x - \\mu}{\\sigma}\\big)^2} $$\n",
    "\n",
    "We know the product of the likelihoods can convert to the sum of the logs of the likelihoods so we dont need to do joint probability any more instead move into the log likelihood\n",
    "\n",
    "$$\\large\\sum_{i=0}^{100} \\ln\\left(\\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\big(\\frac{x_i - \\mu}{\\sigma}\\big)^2}\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step3. Write code to compute the log likelihood function of the model given the data\n",
    "    - pdf of each data point\n",
    "    - log of each pdf\n",
    "    - sum the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_normal(mean, std, data):\n",
    "    '''\n",
    "    Calculates the log likelihood of the data given that the data is distributed as Normal(mean = 63.7, sigma = std)\n",
    "    \n",
    "    Parameters:\n",
    "    std: 'standard deviation' parameter corresponding to 'scale' in the pdf definition.\n",
    "    data: the observed data\n",
    "    \n",
    "    Returns:\n",
    "    float: The log likelihood of the data given the distribution.\n",
    "    '''\n",
    "    normal = stats.norm(mean, std)\n",
    "    #pdf of each datapoint \n",
    "    likelihoods = normal.pdf(data) # just use pdf for likelihood! (list comprehension is unnecessary but can be clearer visually)\n",
    "    #log of each pdf... You can use another comprehenison here if you like but again, it is unnecssary \n",
    "    logs = np.log(likelihoods)\n",
    "    #return the sum of those logs\n",
    "    return np.sum(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Create a large engough searchspace to find a good estimate for parameter. \n",
    "\n",
    "Let's find that max log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like_plot(ax, x, log_likelihood, max_log, mle, parameter = 'standard deviation'):\n",
    "    # create the plot\n",
    "    ax.plot(x, log_likelihood)\n",
    "    ax.axhline(max_log, ls = '--',c='r', label = 'Derivative = 0')\n",
    "    ax.set_title(f'MLE of {parameter}: {round(mle,2)}')\n",
    "    ax.set_xlabel(f\"{parameter}\")\n",
    "    ax.set_ylabel(\"log likelihood\")\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array to hold the log likelihoods\n",
    "log_likelihood = []\n",
    "\n",
    "# create a numpy ndarray from 0.1 to 10 with 100 points along the line.\n",
    "search_space = np.linspace(0.2, 4, 100)\n",
    "\n",
    "# loop through our search space and append the sum of log likelihoods of the data given model \n",
    "# Poisson(lambda) to the log_likelihood array \n",
    "for std in search_space:\n",
    "    log_likelihood.append(log_likelihood_normal(63.7, std, height_data))\n",
    "\n",
    "\n",
    "# get the index of the maximum log_likelihood value\n",
    "max_idx = np.argmax(log_likelihood)    \n",
    "# create a variable equal to the search space corresponding to the maximum log likelihood value.\n",
    "mle = search_space[max_idx]\n",
    "\n",
    "#plot it for fun\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "log_like_plot(ax, search_space, log_likelihood, log_likelihood[max_idx], mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you try... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a few minutes to write this    \n",
    "def log_likelihood_binomial(n, p, data):\n",
    "    '''\n",
    "    Calculates the log likelihood of the data given that the data is distributed as Binomial(n, p)\n",
    "    \n",
    "    Parameters:\n",
    "    n: the number of bernoulli trials for the binomial distribution\n",
    "    p: the probability of success for each bernoulli trial\n",
    "    \n",
    "    Returns:\n",
    "    float: The log likelihood of the data given the distribution.\n",
    "    '''\n",
    "    binomial = stats.binom(n,p)\n",
    "    \n",
    "    likelihoods = binomial.pmf(data)\n",
    "    \n",
    "    sum_logs = np.sum(np.log(likelihoods))\n",
    "    \n",
    "    return sum_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--     binomial = stats.binom(n, p)\n",
    "    \n",
    "    likelihoods = binomial.pmf(data)\n",
    "    \n",
    "    log_likelihoods = np.log(likelihoods)\n",
    "    \n",
    "    log_likelihood_sum = np.sum(log_likelihoods)\n",
    "\n",
    "    return(log_likelihood_sum) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to know two parameters? Like n and p?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty numpy array to hold the log likelihoods.\n",
    "# We want it to be 100x100 because we'll have 100 candidate values for each parameter.\n",
    "log_likelihood = np.zeros([100, 100])\n",
    "\n",
    "# create a numpy ndarray from 1 to 100 as candidate values for n\n",
    "#total number of trials \n",
    "# we need to set min_value to the max of our data, because you can't get a result greater than 'n'\n",
    "# from a binomial distribution.\n",
    "min_value = np.max(data)\n",
    "n_search_space = np.arange(min_value, min_value + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array from 0.01 to 0.99, \n",
    "# Avoiding 0 and 1 because you couldn't get any values other than 0 with p=0 or n if p=100.\n",
    "p_search_space = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# loop through our search space and append the sum of log likelihoods of the data given model \n",
    "# binomial(n, p) to the log_likelihood array \n",
    "# enumerate is a useful function for for loops that allows you to capture both the index and the value.\n",
    "for n_idx, n in enumerate(n_search_space):\n",
    "    for p_idx, p in enumerate(p_search_space):\n",
    "        log_likelihood[n_idx, p_idx] = log_likelihood_binomial(n=n, p=p, data=data)\n",
    "\n",
    "# get the index of the maximum value        \n",
    "unraveled_idx_of_max = np.argmax(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert the single number index into a row and column useful for getting the original parameters. \n",
    "idx_n, idx_p = np.unravel_index(unraveled_idx_of_max, log_likelihood.shape)\n",
    "\n",
    "print(\"Maximum log likelihood: {}\".format(np.max(log_likelihood)))\n",
    "print(\"n: {}, p: {}\".format(n_search_space[idx_n], p_search_space[idx_p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import meshgrid\n",
    "\n",
    "\n",
    "X, Y = meshgrid(n_search_space, p_search_space)\n",
    "Z = np.array([\n",
    "        [log_likelihood_binomial(n,p, data)] \n",
    "         for n,p in zip(X.flatten(), Y.flatten())]).reshape(X.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Law of Small Numbers\n",
    "\n",
    "The poisson distribution is a good approximation to the binomial distribution, where $\\lambda = np$. It shouldn't be surprising that the max log likelihoods are similar across the two distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Poisson Distribution log_like:  {highest_log_pois} \\nBinomial Distribution log_like: {np.max(log_likelihood)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this negative number useful?\n",
    "\n",
    "Every data point has a likelihood given some model. That likelihood is between 0 and 1. The log of a number between 0 and 1 is always negative. Multiple independent events means that the joint likelihood is the product of the likelihoods, and the log of the joint likelihood is the sum of log likelihoods. We're taking the log because log is a monotonically increasing function, so maximizing it is the same as maximizing the original function.  Sum of negative numbers gets more and more negative. \n",
    "\n",
    "But if you have the same number of data points, then the relative log likelihood can be compared for goodness of fit.\n",
    "\n",
    "Best fit is the largest number. (So smallest absolute value). Number closest to 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does monotonically increasing mean?\n",
    "# If a function f is monotonically increasing, maximizing f(x) is equivalent to maximizing(x)\n",
    "\n",
    "original = np.linspace(0.0001, 1, 1000)\n",
    "logged = np.log(original)\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "ax.plot(original, logged)\n",
    "ax.set_xlabel('original values')\n",
    "ax.set_ylabel('log transformed values')\n",
    "ax.set_title('log(x) is a monotically increasing function');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a product of many small numbers computes to zero. \n",
    "\n",
    "np.prod([0.01]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to do operations on many small numbers, you should instead sum the logs. \n",
    "# Using the log is also useful when trying to take a derivative for the purposes of \n",
    "# using the derivative for optimization (max/min).\n",
    "\n",
    "np.sum([np.log(0.01)]*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 2\n",
    "stats.expon.rvs(0, 0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
